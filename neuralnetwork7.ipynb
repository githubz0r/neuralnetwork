{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train=np.loadtxt('sincTrain25.dt')\n",
    "trainx=train[:,0]\n",
    "trainy=train[:,1]\n",
    "valid=np.loadtxt('sincValidate10.dt')\n",
    "valx=valid[:,0]\n",
    "valy=valid[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training data and validation data (they are generated from the function sinc(x)=sin(x)/x)so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.subplots_adjust(hspace=0.7)\n",
    "ax[0].scatter(trainx, trainy, s=2)\n",
    "ax[0].set_title('Training data')\n",
    "ax[0].set_ylabel('Sinc(x)')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[1].scatter(valx, valy, s=2)\n",
    "ax[1].set_title('Validation data')\n",
    "ax[1].set_ylabel('Sinc(x)')\n",
    "ax[1].set_xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network function and gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x, w, activation_function, has_ones = False):\n",
    "    w0 = w[0]\n",
    "    w1 = w[1]\n",
    "    if len(x.shape)<1:\n",
    "        x = x.reshape(1,1)\n",
    "    N = x.shape[0]\n",
    "    if not has_ones:\n",
    "        x = np.c_[np.ones(N), x]\n",
    "    a = x@w0 # dimenson is d*M where M is number of columns in w0, i.e. number of neurons\n",
    "    if activation_function == 'relu':\n",
    "        z = relu(a)\n",
    "    elif activation_function == 'softsign':\n",
    "        z = a/(1+np.abs(a))\n",
    "    z = np.c_[np.ones(z.shape[0]), z] # ones for bias\n",
    "    out = z@w1\n",
    "    return(dict(first_mult = a, first_mult_nonlin = z, second_mult = out))\n",
    "\n",
    "def relu(x):\n",
    "    zeroes = np.zeros(x.shape)\n",
    "    zeroes[x>0] = x[x>0]\n",
    "    return(zeroes)\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = (x > 0)*1\n",
    "    return(grad)\n",
    "\n",
    "def nn_grad(x, y, a, z, out, w, activation_function):\n",
    "    '''y must be an array of dimension at least 2'''\n",
    "    w_out = w[1]\n",
    "    N = x.shape[0]\n",
    "    delta_outs = out-y # N*1\n",
    "    qq = np.zeros(21)\n",
    "    delta_outs_repeated = np.repeat((out-y), w_out.shape[0], axis=1) # N*(n_hidden+1)\n",
    "    output_grad = np.sum(np.multiply(delta_outs, z), axis=0)/N # gradient of output unit\n",
    "    if len(output_grad.shape)<2:\n",
    "        output_grad = output_grad.reshape(output_grad.shape[0], 1)\n",
    "    if activation_function == 'relu':\n",
    "        hidden_activation_deriv = relu_grad(a)\n",
    "    elif activation_function == 'softsign':\n",
    "        hidden_activation_deriv = 1/((1+np.abs(a))**2)\n",
    "    delta_hidden_sum_parts = delta_outs@w_out.T # this must be a sum with more than 1 output neuron\n",
    "    delta_hidden_sum_parts = delta_hidden_sum_parts[:,1:] # removing bias column\n",
    "    delta_hidden = hidden_activation_deriv*delta_hidden_sum_parts\n",
    "    hidden_grad = 0\n",
    "    for i in range(N):\n",
    "        vector_of_deltas = delta_hidden[i,:]\n",
    "        vector_of_deltas = vector_of_deltas.reshape(a.shape[1],1)\n",
    "        grad_element = x[i,:]*vector_of_deltas\n",
    "        hidden_grad += grad_element/N\n",
    "    return([hidden_grad.T, output_grad])\n",
    "\n",
    "def nn_gradient_descent(x_train, y_train, x_val, y_val, n_hidden, rate, iterations, patience,\n",
    "                       verbose, weights, initialization_factors, activation_function):\n",
    "    if len(y_train.shape)==1:\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "    if len(y_val.shape)==1:\n",
    "        y_val = y_val.reshape(y_val.shape[0], 1)\n",
    "    if len(x_train.shape)==1:\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "    K = y_train.shape[1]\n",
    "    D = x_train.shape[1]\n",
    "    N = x_train.shape[0]\n",
    "    x_train = np.c_[np.ones(N), x_train, ]\n",
    "    # initialize weights\n",
    "    if initialization_factors == None:\n",
    "        initialization_factors = [np.sqrt(2/(D+1 + n_hidden)), np.sqrt(2/(n_hidden+1 + K))]\n",
    "    if weights == None:\n",
    "        w_hidden0 = np.random.normal(0, 1, (D+1, n_hidden)) * initialization_factors[0]\n",
    "        w_out0 = np.random.normal(0, 1, (n_hidden+1, K)) * initialization_factors[1]\n",
    "    w_hidden1 = None\n",
    "    w_out1 = None\n",
    "    w_best = None\n",
    "    \n",
    "    patience_counter = patience\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    gradient_norm = []\n",
    "    max_iter = iterations\n",
    "    while iterations > 0 and patience_counter >0:\n",
    "        w=[w_hidden0, w_out0]\n",
    "        nn_outs_train = neural_network(x_train, w, activation_function, has_ones=True)\n",
    "        a = nn_outs_train['first_mult']\n",
    "        z = nn_outs_train['first_mult_nonlin']\n",
    "        out = nn_outs_train['second_mult']\n",
    "        train_error = 0.5*np.sum((y_train - out)**2)/N\n",
    "        if len(out.shape)<2:\n",
    "            out=out.reshape(y_train.shape[0], 1) # reshaping to get broadcasting to work later\n",
    "        train_loss.append(train_error)\n",
    "        delta_outs = (out-y_train) # N*1\n",
    "        grads = nn_grad(x=x_train, y=y_train, a=a, z=z, out=out, w=w, activation_function=activation_function)\n",
    "        hidden_grad = grads[0]\n",
    "        output_grad = grads[1]\n",
    "        w_hidden1 = w_hidden0 - rate*hidden_grad #/N\n",
    "        w_out1 = w_out0 - rate*output_grad #/N\n",
    "        val_out = neural_network(x_val, w, activation_function=activation_function)['second_mult']\n",
    "        val_error = 0.5*np.sum((y_val - val_out)**2)/x_val.shape[0]\n",
    "        iterations -= 1\n",
    "        if verbose: \n",
    "            print('iterations: ', max_iter-iterations)\n",
    "            print('Training loss: {}, Validation loss: {}'.format(train_error, val_error))\n",
    "        if len(val_loss)>1:\n",
    "            if val_error < min(val_loss):\n",
    "                if verbose: \n",
    "                    print('new best w')\n",
    "                w_best = w\n",
    "                patience_counter = patience\n",
    "            elif val_error >= val_loss[-1]:\n",
    "                patience_counter -= 1\n",
    "        val_loss.append(val_error)\n",
    "        w_out0 = w_out1\n",
    "        w_hidden0 = w_hidden1\n",
    "        gradient_norm_i = np.sqrt(np.sum(np.concatenate([hidden_grad.flatten(), output_grad.flatten()])**2))\n",
    "        gradient_norm.append(gradient_norm_i)\n",
    "    if w_best == None:\n",
    "        w_best = [w_hidden0, w_out0]\n",
    "    return(dict(weights=w_best, train_loss=train_loss, val_loss=val_loss, gradient_norm=gradient_norm,\n",
    "               iterations=max_iter-iterations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the things into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNregressor_onelayer:\n",
    "    def __init__(self, activation_function, weights=None):\n",
    "        self.weights = weights\n",
    "        self.activation_function = activation_function\n",
    "    def estimate_weights(self, trainx, trainy, valx, valy, n_hidden, rate, iterations, patience, verbose,\n",
    "                        weight_initialization_factors):\n",
    "        training_results = nn_gradient_descent(trainx, trainy, valx, valy, n_hidden, \n",
    "                                           rate, iterations, patience, verbose,\n",
    "                                           self.weights, weight_initialization_factors,\n",
    "                                               activation_function = self.activation_function)\n",
    "        self.weights = training_results['weights']\n",
    "        self.training_loss = training_results['train_loss']\n",
    "        self.validation_loss = training_results['val_loss']\n",
    "        self.gradient_norm = training_results['gradient_norm']\n",
    "        self.iterations = training_results['iterations']\n",
    "    def predict(self, x):\n",
    "        predictions = neural_network(x, self.weights, activation_function = self.activation_function)\n",
    "        return(predictions['second_mult'].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to validate the gradient. We make up some random weights and calculate some outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173.37861170911933"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(84)\n",
    "wtest1=np.random.normal(0,1,(2,20))\n",
    "wtest2=np.random.normal(0,1,(21,1))\n",
    "test_out=neural_network(trainx,[wtest1,wtest2], 'relu')\n",
    "test_a = test_out['first_mult']\n",
    "test_z = test_out['first_mult_nonlin']\n",
    "test_pred = test_out['second_mult']\n",
    "test_error = 0.5*np.sum((test_pred - trainy.reshape(trainy.shape[0], 1))**2)/25\n",
    "test_grad = nn_grad(np.c_[np.ones(25), trainx.reshape(25,1)], trainy.reshape(25,1), \n",
    "                    test_a, test_z, test_pred, [wtest1, wtest2], 'relu')\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a bunch of for loops to check the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_grad_out_all = np.zeros((21,1))\n",
    "error_grad_hidden_all = np.zeros((2, 20))\n",
    "for n in range(25):\n",
    "    eps_val = np.sqrt(np.finfo(float).eps)\n",
    "    error_grad_out = []\n",
    "    for i in range(wtest2.shape[0]):\n",
    "        for j in range(wtest2.shape[1]):\n",
    "            wtest2_2 = np.copy(wtest2)\n",
    "            wtest2_2[i,j] = wtest2_2[i, j] + eps_val\n",
    "            pred_noneps = neural_network(trainx[n], [wtest1, wtest2], 'relu')['second_mult'].reshape(1,1)\n",
    "            pred_eps = neural_network(trainx[n], [wtest1, wtest2_2], 'relu')['second_mult'].reshape(1,1)\n",
    "            error_noneps = 0.5*(pred_noneps-trainy[n])**2\n",
    "            error_eps = 0.5*(pred_eps-trainy[n])**2\n",
    "            error_grad_ij = (error_eps-error_noneps)/eps_val\n",
    "            error_grad_out.append(error_grad_ij)\n",
    "    error_grad_hidden = []\n",
    "    for i in range(wtest1.shape[0]):\n",
    "        for j in range(wtest1.shape[1]):\n",
    "            wtest1_2 = np.copy(wtest1)\n",
    "            wtest1_2[i,j] = wtest1_2[i, j] + eps_val\n",
    "            pred_noneps = neural_network(trainx[n], [wtest1, wtest2], 'relu')['second_mult'].reshape(1,1)\n",
    "            pred_eps = neural_network(trainx[n], [wtest1_2, wtest2], 'relu')['second_mult'].reshape(1,1)\n",
    "            error_noneps = 0.5*(pred_noneps-trainy[n])**2\n",
    "            error_eps = 0.5*(pred_eps-trainy[n])**2\n",
    "            error_grad_ij = (error_eps-error_noneps)/eps_val\n",
    "            error_grad_hidden.append(error_grad_ij)\n",
    "    error_grad_hidden_all+=(np.array(error_grad_hidden).reshape(2,20))/25\n",
    "    error_grad_out_all+=np.array(error_grad_out).reshape(21,1)/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000021, 1.00000008, 1.00000001, 1.00000011, 1.        ,\n",
       "        0.9999999 , 0.99999983, 0.99999999, 1.00000001, 1.00000515,\n",
       "        0.99999986, 1.        , 0.99999937, 0.9999999 , 1.00000002,\n",
       "        0.99999977, 0.99999993, 0.99999998, 1.        , 1.00000002],\n",
       "       [0.99999993, 0.99999997, 1.        , 0.99999998, 1.        ,\n",
       "        1.00000001, 1.00000001, 1.00000001, 0.99999999, 0.99999943,\n",
       "        1.00000002, 0.99999998, 1.00000014, 1.00000002, 1.        ,\n",
       "        1.00000006, 0.99999999, 1.00000001, 1.        , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_grad[0]+np.finfo(float).eps)/(error_grad_hidden_all+np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999999, 1.00000001, 1.00000003, 0.99999999, 1.00000008,\n",
       "        1.        , 1.        , 1.00000001, 1.00000003, 0.99999999,\n",
       "        1.00000002, 1.00000002, 1.00000001, 1.00000001, 1.00000001,\n",
       "        0.99999999, 0.99999997, 1.        , 0.99999999, 1.00000001,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((test_grad[1]+np.finfo(float).eps)/(error_grad_out_all+np.finfo(float).eps)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checks out. Let's make a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1\n",
      "Training loss: 0.25176355295841085, Validation loss: 0.41457514397854417\n",
      "iterations:  2\n",
      "Training loss: 0.05653002505530123, Validation loss: 0.08464248457031819\n",
      "iterations:  3\n",
      "Training loss: 0.04933016115527964, Validation loss: 0.08003735887409569\n",
      "new best w\n",
      "iterations:  4\n",
      "Training loss: 0.0482489087182119, Validation loss: 0.08023973286464806\n",
      "iterations:  5\n",
      "Training loss: 0.047423515264889395, Validation loss: 0.07950560338878052\n",
      "new best w\n",
      "iterations:  6\n",
      "Training loss: 0.04663582761184635, Validation loss: 0.07852158390668655\n",
      "new best w\n",
      "iterations:  7\n",
      "Training loss: 0.04588235998720167, Validation loss: 0.07753128085896902\n",
      "new best w\n",
      "iterations:  8\n",
      "Training loss: 0.04515647310944995, Validation loss: 0.07655318342099807\n",
      "new best w\n",
      "iterations:  9\n",
      "Training loss: 0.044437048006147825, Validation loss: 0.07555298050087357\n",
      "new best w\n",
      "iterations:  10\n",
      "Training loss: 0.043724897235305565, Validation loss: 0.07458554501960796\n",
      "new best w\n",
      "iterations:  11\n",
      "Training loss: 0.04303060319593599, Validation loss: 0.07364115735134605\n",
      "new best w\n",
      "iterations:  12\n",
      "Training loss: 0.04236783966709965, Validation loss: 0.07272251322605373\n",
      "new best w\n",
      "iterations:  13\n",
      "Training loss: 0.04175207029878493, Validation loss: 0.07184590319682334\n",
      "new best w\n",
      "iterations:  14\n",
      "Training loss: 0.04116220733755137, Validation loss: 0.07102248042902948\n",
      "new best w\n",
      "iterations:  15\n",
      "Training loss: 0.0405928690532531, Validation loss: 0.07021696248098298\n",
      "new best w\n",
      "iterations:  16\n",
      "Training loss: 0.04005175830455274, Validation loss: 0.06944160257235207\n",
      "new best w\n",
      "iterations:  17\n",
      "Training loss: 0.03953493288735624, Validation loss: 0.0686986005360683\n",
      "new best w\n",
      "iterations:  18\n",
      "Training loss: 0.039037727654751386, Validation loss: 0.06798485923119038\n",
      "new best w\n",
      "iterations:  19\n",
      "Training loss: 0.03856129107813701, Validation loss: 0.06730518037299474\n",
      "new best w\n",
      "iterations:  20\n",
      "Training loss: 0.03810829183257222, Validation loss: 0.06664057274906456\n",
      "new best w\n",
      "iterations:  21\n",
      "Training loss: 0.037664703805812376, Validation loss: 0.0659990111885703\n",
      "new best w\n",
      "iterations:  22\n",
      "Training loss: 0.037228627548044464, Validation loss: 0.06533654590574561\n",
      "new best w\n",
      "iterations:  23\n",
      "Training loss: 0.0368111108514918, Validation loss: 0.06471574323847226\n",
      "new best w\n",
      "iterations:  24\n",
      "Training loss: 0.03641239437318775, Validation loss: 0.06412099226327936\n",
      "new best w\n",
      "iterations:  25\n",
      "Training loss: 0.03603184568383816, Validation loss: 0.06356042925598196\n",
      "new best w\n",
      "iterations:  26\n",
      "Training loss: 0.035663178754398075, Validation loss: 0.06301517480609158\n",
      "new best w\n",
      "iterations:  27\n",
      "Training loss: 0.035306625895849375, Validation loss: 0.062484773955318026\n",
      "new best w\n",
      "iterations:  28\n",
      "Training loss: 0.03496132573610087, Validation loss: 0.061962320622812025\n",
      "new best w\n",
      "iterations:  29\n",
      "Training loss: 0.03462598715543957, Validation loss: 0.06145038963557766\n",
      "new best w\n",
      "iterations:  30\n",
      "Training loss: 0.0343027615599957, Validation loss: 0.06095791257608961\n",
      "new best w\n",
      "iterations:  31\n",
      "Training loss: 0.03399585187885885, Validation loss: 0.06049686105287576\n",
      "new best w\n",
      "iterations:  32\n",
      "Training loss: 0.03370544666419324, Validation loss: 0.060044620452250855\n",
      "new best w\n",
      "iterations:  33\n",
      "Training loss: 0.0334262350896469, Validation loss: 0.05963115103076849\n",
      "new best w\n",
      "iterations:  34\n",
      "Training loss: 0.03315810175554537, Validation loss: 0.059208904748889055\n",
      "new best w\n",
      "iterations:  35\n",
      "Training loss: 0.03290129333726312, Validation loss: 0.05881326297754458\n",
      "new best w\n",
      "iterations:  36\n",
      "Training loss: 0.03265225156564953, Validation loss: 0.05841557959606941\n",
      "new best w\n",
      "iterations:  37\n",
      "Training loss: 0.03241084503097229, Validation loss: 0.05803274416104698\n",
      "new best w\n",
      "iterations:  38\n",
      "Training loss: 0.03217657890066857, Validation loss: 0.057649543872731225\n",
      "new best w\n",
      "iterations:  39\n",
      "Training loss: 0.03194891453148988, Validation loss: 0.05728303728293919\n",
      "new best w\n",
      "iterations:  40\n",
      "Training loss: 0.03172871949835049, Validation loss: 0.05693005501469826\n",
      "new best w\n",
      "iterations:  41\n",
      "Training loss: 0.03151888875725016, Validation loss: 0.056589703153436324\n",
      "new best w\n",
      "iterations:  42\n",
      "Training loss: 0.03131524954950336, Validation loss: 0.0562512034487573\n",
      "new best w\n",
      "iterations:  43\n",
      "Training loss: 0.03111822531957954, Validation loss: 0.05592665013659204\n",
      "new best w\n",
      "iterations:  44\n",
      "Training loss: 0.030927356409170197, Validation loss: 0.05561458256506947\n",
      "new best w\n",
      "iterations:  45\n",
      "Training loss: 0.03074260223020467, Validation loss: 0.05532110527830281\n",
      "new best w\n",
      "iterations:  46\n",
      "Training loss: 0.030561725093217976, Validation loss: 0.05499471277091138\n",
      "new best w\n",
      "iterations:  47\n",
      "Training loss: 0.03038372816925215, Validation loss: 0.05469710370089577\n",
      "new best w\n",
      "iterations:  48\n",
      "Training loss: 0.03021354138208797, Validation loss: 0.054398407136691716\n",
      "new best w\n",
      "iterations:  49\n",
      "Training loss: 0.030050535640673483, Validation loss: 0.05412914300305134\n",
      "new best w\n",
      "iterations:  50\n",
      "Training loss: 0.029892092551166188, Validation loss: 0.05381398163346187\n",
      "new best w\n",
      "iterations:  51\n",
      "Training loss: 0.029738554641439374, Validation loss: 0.053553647220406064\n",
      "new best w\n",
      "iterations:  52\n",
      "Training loss: 0.029589773390565746, Validation loss: 0.05328691886634314\n",
      "new best w\n",
      "iterations:  53\n",
      "Training loss: 0.029445089184150723, Validation loss: 0.05302572899321454\n",
      "new best w\n",
      "iterations:  54\n",
      "Training loss: 0.02930521187819889, Validation loss: 0.052786004969547565\n",
      "new best w\n",
      "iterations:  55\n",
      "Training loss: 0.02916991544352421, Validation loss: 0.05253360059652547\n",
      "new best w\n",
      "iterations:  56\n",
      "Training loss: 0.029038223900815924, Validation loss: 0.05230491368301354\n",
      "new best w\n",
      "iterations:  57\n",
      "Training loss: 0.028909848013247853, Validation loss: 0.0520878338978464\n",
      "new best w\n",
      "iterations:  58\n",
      "Training loss: 0.02878451191339907, Validation loss: 0.05185462011915959\n",
      "new best w\n",
      "iterations:  59\n",
      "Training loss: 0.028662460504738987, Validation loss: 0.05163380674463206\n",
      "new best w\n",
      "iterations:  60\n",
      "Training loss: 0.028543262342046037, Validation loss: 0.05142383477601795\n",
      "new best w\n",
      "iterations:  61\n",
      "Training loss: 0.028426662234823187, Validation loss: 0.05121529898543015\n",
      "new best w\n",
      "iterations:  62\n",
      "Training loss: 0.028312939172313794, Validation loss: 0.05101240501651949\n",
      "new best w\n",
      "iterations:  63\n",
      "Training loss: 0.028201824546074603, Validation loss: 0.05082691306976903\n",
      "new best w\n",
      "iterations:  64\n",
      "Training loss: 0.028094380481657612, Validation loss: 0.05062458980974251\n",
      "new best w\n",
      "iterations:  65\n",
      "Training loss: 0.027990200889298378, Validation loss: 0.050420876739774\n",
      "new best w\n",
      "iterations:  66\n",
      "Training loss: 0.02788810362142997, Validation loss: 0.050241099279159165\n",
      "new best w\n",
      "iterations:  67\n",
      "Training loss: 0.027788227800753657, Validation loss: 0.05004840587356337\n",
      "new best w\n",
      "iterations:  68\n",
      "Training loss: 0.027690603617898036, Validation loss: 0.049873494559025545\n",
      "new best w\n",
      "iterations:  69\n",
      "Training loss: 0.02759532938718479, Validation loss: 0.04971198958151051\n",
      "new best w\n",
      "iterations:  70\n",
      "Training loss: 0.027501869983820128, Validation loss: 0.049533673052729515\n",
      "new best w\n",
      "iterations:  71\n",
      "Training loss: 0.027410454223880745, Validation loss: 0.0493670101181681\n",
      "new best w\n",
      "iterations:  72\n",
      "Training loss: 0.027320823965844482, Validation loss: 0.04920518246856108\n",
      "new best w\n",
      "iterations:  73\n",
      "Training loss: 0.027231122221947443, Validation loss: 0.04905373660018222\n",
      "new best w\n",
      "iterations:  74\n",
      "Training loss: 0.02714210015677732, Validation loss: 0.04888721388404628\n",
      "new best w\n",
      "iterations:  75\n",
      "Training loss: 0.027051062475888045, Validation loss: 0.048724209281190575\n",
      "new best w\n",
      "iterations:  76\n",
      "Training loss: 0.026961763049178397, Validation loss: 0.04858139288961307\n",
      "new best w\n",
      "iterations:  77\n",
      "Training loss: 0.02687399610154979, Validation loss: 0.04842230569973267\n",
      "new best w\n",
      "iterations:  78\n",
      "Training loss: 0.02678797023960833, Validation loss: 0.048274575985512554\n",
      "new best w\n",
      "iterations:  79\n",
      "Training loss: 0.026704087838883814, Validation loss: 0.04813120208041054\n",
      "new best w\n",
      "iterations:  80\n",
      "Training loss: 0.026624686385544178, Validation loss: 0.047989639477483824\n",
      "new best w\n",
      "iterations:  81\n",
      "Training loss: 0.026546700945202725, Validation loss: 0.047820628720514\n",
      "new best w\n",
      "iterations:  82\n",
      "Training loss: 0.026470858486830607, Validation loss: 0.04767462431398895\n",
      "new best w\n",
      "iterations:  83\n",
      "Training loss: 0.026396362148878486, Validation loss: 0.047568600420328025\n",
      "new best w\n",
      "iterations:  84\n",
      "Training loss: 0.026323092070497756, Validation loss: 0.047416231831263436\n",
      "new best w\n",
      "iterations:  85\n",
      "Training loss: 0.026251536637464448, Validation loss: 0.04728530645669715\n",
      "new best w\n",
      "iterations:  86\n",
      "Training loss: 0.02618144989226575, Validation loss: 0.047176866578709814\n",
      "new best w\n",
      "iterations:  87\n",
      "Training loss: 0.02611289123695245, Validation loss: 0.047040306761379054\n",
      "new best w\n",
      "iterations:  88\n",
      "Training loss: 0.026046508767361178, Validation loss: 0.046928049341592865\n",
      "new best w\n",
      "iterations:  89\n",
      "Training loss: 0.02598181295094702, Validation loss: 0.04677751098998755\n",
      "new best w\n",
      "iterations:  90\n",
      "Training loss: 0.025918405572218334, Validation loss: 0.04667307080885624\n",
      "new best w\n",
      "iterations:  91\n",
      "Training loss: 0.02585651078302799, Validation loss: 0.046538683269408944\n",
      "new best w\n",
      "iterations:  92\n",
      "Training loss: 0.025795989176272006, Validation loss: 0.046443684344220984\n",
      "new best w\n",
      "iterations:  93\n",
      "Training loss: 0.025736373468700564, Validation loss: 0.04633017700004184\n",
      "new best w\n",
      "iterations:  94\n",
      "Training loss: 0.0256779115256012, Validation loss: 0.04620957600998392\n",
      "new best w\n",
      "iterations:  95\n",
      "Training loss: 0.025620618698554174, Validation loss: 0.04610182357196384\n",
      "new best w\n",
      "iterations:  96\n",
      "Training loss: 0.025564441564730478, Validation loss: 0.04599677986065643\n",
      "new best w\n",
      "iterations:  97\n",
      "Training loss: 0.02550946996058026, Validation loss: 0.04589534472508176\n",
      "new best w\n",
      "iterations:  98\n",
      "Training loss: 0.025455569427609757, Validation loss: 0.04582434820072491\n",
      "new best w\n",
      "iterations:  99\n",
      "Training loss: 0.02540234562370756, Validation loss: 0.04570342688515736\n",
      "new best w\n",
      "iterations:  100\n",
      "Training loss: 0.025350146211458174, Validation loss: 0.04560452734404528\n",
      "new best w\n",
      "iterations:  101\n",
      "Training loss: 0.02529911489236279, Validation loss: 0.04550626733408862\n",
      "new best w\n",
      "iterations:  102\n",
      "Training loss: 0.02524927344181659, Validation loss: 0.045418912847992224\n",
      "new best w\n",
      "iterations:  103\n",
      "Training loss: 0.025201442788557026, Validation loss: 0.04531546057253284\n",
      "new best w\n",
      "iterations:  104\n",
      "Training loss: 0.025154147534041337, Validation loss: 0.04523962842908745\n",
      "new best w\n",
      "iterations:  105\n",
      "Training loss: 0.025107172701668064, Validation loss: 0.045140837896516524\n",
      "new best w\n",
      "iterations:  106\n",
      "Training loss: 0.025061048385026492, Validation loss: 0.04505356897918855\n",
      "new best w\n",
      "iterations:  107\n",
      "Training loss: 0.025015717135981795, Validation loss: 0.044970137763612646\n",
      "new best w\n",
      "iterations:  108\n",
      "Training loss: 0.024971147091979402, Validation loss: 0.0448873119545278\n",
      "new best w\n",
      "iterations:  109\n",
      "Training loss: 0.024928404262057207, Validation loss: 0.044806757551971575\n",
      "new best w\n",
      "iterations:  110\n",
      "Training loss: 0.02488835588076824, Validation loss: 0.044749752888451906\n",
      "new best w\n",
      "iterations:  111\n",
      "Training loss: 0.024848796930322815, Validation loss: 0.04466535523916202\n",
      "new best w\n",
      "iterations:  112\n",
      "Training loss: 0.024809957438880797, Validation loss: 0.044589944438609065\n",
      "new best w\n",
      "iterations:  113\n",
      "Training loss: 0.02477175224364666, Validation loss: 0.04451793158001707\n",
      "new best w\n",
      "iterations:  114\n",
      "Training loss: 0.02473419450887325, Validation loss: 0.0444459674778548\n",
      "new best w\n",
      "iterations:  115\n",
      "Training loss: 0.024697597512204673, Validation loss: 0.04437197708462222\n",
      "new best w\n",
      "iterations:  116\n",
      "Training loss: 0.02466202330638789, Validation loss: 0.044311198625389125\n",
      "new best w\n",
      "iterations:  117\n",
      "Training loss: 0.02462746287918841, Validation loss: 0.04424860282286039\n",
      "new best w\n",
      "iterations:  118\n",
      "Training loss: 0.024595321247003095, Validation loss: 0.04417029338525544\n",
      "new best w\n",
      "iterations:  119\n",
      "Training loss: 0.024563828603001377, Validation loss: 0.044107188259584124\n",
      "new best w\n",
      "iterations:  120\n",
      "Training loss: 0.024532814471500224, Validation loss: 0.04403589138484432\n",
      "new best w\n",
      "iterations:  121\n",
      "Training loss: 0.024502274002633165, Validation loss: 0.04398090027503629\n",
      "new best w\n",
      "iterations:  122\n",
      "Training loss: 0.024472317393553697, Validation loss: 0.0439116165800568\n",
      "new best w\n",
      "iterations:  123\n",
      "Training loss: 0.02444286364730827, Validation loss: 0.043870687816135755\n",
      "new best w\n",
      "iterations:  124\n",
      "Training loss: 0.024413780768917856, Validation loss: 0.04379415112937944\n",
      "new best w\n",
      "iterations:  125\n",
      "Training loss: 0.024385095737668054, Validation loss: 0.043742271575916834\n",
      "new best w\n",
      "iterations:  126\n",
      "Training loss: 0.02435723199220907, Validation loss: 0.04366736659798089\n",
      "new best w\n",
      "iterations:  127\n",
      "Training loss: 0.024329528511360655, Validation loss: 0.043626866042806446\n",
      "new best w\n",
      "iterations:  128\n",
      "Training loss: 0.024302314231301993, Validation loss: 0.04355485224920684\n",
      "new best w\n",
      "iterations:  129\n",
      "Training loss: 0.024275441894960247, Validation loss: 0.043517818351100135\n",
      "new best w\n",
      "iterations:  130\n",
      "Training loss: 0.024248899018381533, Validation loss: 0.04344730558036159\n",
      "new best w\n",
      "iterations:  131\n",
      "Training loss: 0.02422292782587365, Validation loss: 0.04340293410476828\n",
      "new best w\n",
      "iterations:  132\n",
      "Training loss: 0.02419700590616868, Validation loss: 0.04336002105575877\n",
      "new best w\n",
      "iterations:  133\n",
      "Training loss: 0.024171404407781732, Validation loss: 0.04329173737329985\n",
      "new best w\n",
      "iterations:  134\n",
      "Training loss: 0.024146442981258134, Validation loss: 0.043259085804051946\n",
      "new best w\n",
      "iterations:  135\n",
      "Training loss: 0.024121465643294385, Validation loss: 0.04319313160586018\n",
      "new best w\n",
      "iterations:  136\n",
      "Training loss: 0.024097019648239803, Validation loss: 0.043142183827511005\n",
      "new best w\n",
      "iterations:  137\n",
      "Training loss: 0.02407304235702241, Validation loss: 0.043087507516368806\n",
      "new best w\n",
      "iterations:  138\n",
      "Training loss: 0.024049406968334193, Validation loss: 0.043066632980295005\n",
      "new best w\n",
      "iterations:  139\n",
      "Training loss: 0.02402589671788772, Validation loss: 0.04300093028134107\n",
      "new best w\n",
      "iterations:  140\n",
      "Training loss: 0.024003019148406936, Validation loss: 0.04295480126484091\n",
      "new best w\n",
      "iterations:  141\n",
      "Training loss: 0.02398052835475766, Validation loss: 0.04290435254838219\n",
      "new best w\n",
      "iterations:  142\n",
      "Training loss: 0.02395827131695443, Validation loss: 0.04287950229190698\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  143\n",
      "Training loss: 0.02393614126736966, Validation loss: 0.042819647140009146\n",
      "new best w\n",
      "iterations:  144\n",
      "Training loss: 0.023914394865071693, Validation loss: 0.04277389223080473\n",
      "new best w\n",
      "iterations:  145\n",
      "Training loss: 0.023893023025592725, Validation loss: 0.042731278909975336\n",
      "new best w\n",
      "iterations:  146\n",
      "Training loss: 0.023871987130138512, Validation loss: 0.04268237820226146\n",
      "new best w\n",
      "iterations:  147\n",
      "Training loss: 0.023853296064885513, Validation loss: 0.042659645925576944\n",
      "new best w\n",
      "iterations:  148\n",
      "Training loss: 0.0238338773718524, Validation loss: 0.04262136730864409\n",
      "new best w\n",
      "iterations:  149\n",
      "Training loss: 0.023814646163247247, Validation loss: 0.04256751311190263\n",
      "new best w\n",
      "iterations:  150\n",
      "Training loss: 0.023795745696477244, Validation loss: 0.042529684158489824\n",
      "new best w\n",
      "iterations:  151\n",
      "Training loss: 0.023777472684699017, Validation loss: 0.04249436232468431\n",
      "new best w\n",
      "iterations:  152\n",
      "Training loss: 0.023758769295967267, Validation loss: 0.042456314436536205\n",
      "new best w\n",
      "iterations:  153\n",
      "Training loss: 0.023740230175962655, Validation loss: 0.04242188223526946\n",
      "new best w\n",
      "iterations:  154\n",
      "Training loss: 0.023722198670387638, Validation loss: 0.04238888309739401\n",
      "new best w\n",
      "iterations:  155\n",
      "Training loss: 0.02370435256379388, Validation loss: 0.04235619017999795\n",
      "new best w\n",
      "iterations:  156\n",
      "Training loss: 0.023686226168217913, Validation loss: 0.04232609477044261\n",
      "new best w\n",
      "iterations:  157\n",
      "Training loss: 0.0236687503261602, Validation loss: 0.04229362967257814\n",
      "new best w\n",
      "iterations:  158\n",
      "Training loss: 0.023651973625091388, Validation loss: 0.04225908725633216\n",
      "new best w\n",
      "iterations:  159\n",
      "Training loss: 0.023634655734513225, Validation loss: 0.042227745602348446\n",
      "new best w\n",
      "iterations:  160\n",
      "Training loss: 0.023617336467764906, Validation loss: 0.0422122479661959\n",
      "new best w\n",
      "iterations:  161\n",
      "Training loss: 0.0236009698749612, Validation loss: 0.04216788091852309\n",
      "new best w\n",
      "iterations:  162\n",
      "Training loss: 0.023583985417931195, Validation loss: 0.042140644894446855\n",
      "new best w\n",
      "iterations:  163\n",
      "Training loss: 0.023566930285136324, Validation loss: 0.04210876100134507\n",
      "new best w\n",
      "iterations:  164\n",
      "Training loss: 0.02355073998973416, Validation loss: 0.04208120690347186\n",
      "new best w\n",
      "iterations:  165\n",
      "Training loss: 0.0235342589333814, Validation loss: 0.042050447835429446\n",
      "new best w\n",
      "iterations:  166\n",
      "Training loss: 0.023517501760741685, Validation loss: 0.042021787840438786\n",
      "new best w\n",
      "iterations:  167\n",
      "Training loss: 0.023501596170612697, Validation loss: 0.041995750315685344\n",
      "new best w\n",
      "iterations:  168\n",
      "Training loss: 0.02348540124302615, Validation loss: 0.041969931270860336\n",
      "new best w\n",
      "iterations:  169\n",
      "Training loss: 0.023468914878331214, Validation loss: 0.04193652327921389\n",
      "new best w\n",
      "iterations:  170\n",
      "Training loss: 0.02345339862799136, Validation loss: 0.04191347914123407\n",
      "new best w\n",
      "iterations:  171\n",
      "Training loss: 0.023437232388390603, Validation loss: 0.041888997038885885\n",
      "new best w\n",
      "iterations:  172\n",
      "Training loss: 0.023421274496074965, Validation loss: 0.04185603099131843\n",
      "new best w\n",
      "iterations:  173\n",
      "Training loss: 0.023406010187126694, Validation loss: 0.041847814987183506\n",
      "new best w\n",
      "iterations:  174\n",
      "Training loss: 0.023389846530711998, Validation loss: 0.041808256193989785\n",
      "new best w\n",
      "iterations:  175\n",
      "Training loss: 0.023374631638907007, Validation loss: 0.04177934386793305\n",
      "new best w\n",
      "iterations:  176\n",
      "Training loss: 0.023359313407023324, Validation loss: 0.041759326596039884\n",
      "new best w\n",
      "iterations:  177\n",
      "Training loss: 0.023343610662778885, Validation loss: 0.0417328813691292\n",
      "new best w\n",
      "iterations:  178\n",
      "Training loss: 0.023329123395622334, Validation loss: 0.041706286123434605\n",
      "new best w\n",
      "iterations:  179\n",
      "Training loss: 0.0233136475332533, Validation loss: 0.04168786176921503\n",
      "new best w\n",
      "iterations:  180\n",
      "Training loss: 0.023298467654200513, Validation loss: 0.04166197080756798\n",
      "new best w\n",
      "iterations:  181\n",
      "Training loss: 0.023283921983976816, Validation loss: 0.041635957281469856\n",
      "new best w\n",
      "iterations:  182\n",
      "Training loss: 0.023268433496205502, Validation loss: 0.041613407991259774\n",
      "new best w\n",
      "iterations:  183\n",
      "Training loss: 0.023254069209894493, Validation loss: 0.041593563335649966\n",
      "new best w\n",
      "iterations:  184\n",
      "Training loss: 0.023239129184917117, Validation loss: 0.04156811243074181\n",
      "new best w\n",
      "iterations:  185\n",
      "Training loss: 0.023224222129539757, Validation loss: 0.04154604052883597\n",
      "new best w\n",
      "iterations:  186\n",
      "Training loss: 0.023209895505291272, Validation loss: 0.0415261102512341\n",
      "new best w\n",
      "iterations:  187\n",
      "Training loss: 0.02319482572183315, Validation loss: 0.04151034353439535\n",
      "new best w\n",
      "iterations:  188\n",
      "Training loss: 0.023180887916959728, Validation loss: 0.04148184479404813\n",
      "new best w\n",
      "iterations:  189\n",
      "Training loss: 0.02316591500401047, Validation loss: 0.04146140668522989\n",
      "new best w\n",
      "iterations:  190\n",
      "Training loss: 0.02315178704731456, Validation loss: 0.041433009216670244\n",
      "new best w\n",
      "iterations:  191\n",
      "Training loss: 0.023137243202376253, Validation loss: 0.04141673832618367\n",
      "new best w\n",
      "iterations:  192\n",
      "Training loss: 0.02312285449213098, Validation loss: 0.04139370739854096\n",
      "new best w\n",
      "iterations:  193\n",
      "Training loss: 0.023108885824073826, Validation loss: 0.0413709792925696\n",
      "new best w\n",
      "iterations:  194\n",
      "Training loss: 0.02309399491207902, Validation loss: 0.041350307054655165\n",
      "new best w\n",
      "iterations:  195\n",
      "Training loss: 0.023080648544559424, Validation loss: 0.041327721454298694\n",
      "new best w\n",
      "iterations:  196\n",
      "Training loss: 0.02306583736567119, Validation loss: 0.04130832089336117\n",
      "new best w\n",
      "iterations:  197\n",
      "Training loss: 0.0230528035304274, Validation loss: 0.04129358821374603\n",
      "new best w\n",
      "iterations:  198\n",
      "Training loss: 0.02303903342663649, Validation loss: 0.0412715698704581\n",
      "new best w\n",
      "iterations:  199\n",
      "Training loss: 0.02302557076750926, Validation loss: 0.04125273510544416\n",
      "new best w\n",
      "iterations:  200\n",
      "Training loss: 0.023012291303698067, Validation loss: 0.04123595356185441\n",
      "new best w\n",
      "iterations:  201\n",
      "Training loss: 0.022998761961677657, Validation loss: 0.04121727717701949\n",
      "new best w\n",
      "iterations:  202\n",
      "Training loss: 0.02298565191388313, Validation loss: 0.041202355271944546\n",
      "new best w\n",
      "iterations:  203\n",
      "Training loss: 0.022972020987985026, Validation loss: 0.04117100938043627\n",
      "new best w\n",
      "iterations:  204\n",
      "Training loss: 0.02295904678351901, Validation loss: 0.0411569333564155\n",
      "new best w\n",
      "iterations:  205\n",
      "Training loss: 0.022945329185199027, Validation loss: 0.04113621302348282\n",
      "new best w\n",
      "iterations:  206\n",
      "Training loss: 0.022932672080898997, Validation loss: 0.041115261844744774\n",
      "new best w\n",
      "iterations:  207\n",
      "Training loss: 0.02291870524418993, Validation loss: 0.04109798062368854\n",
      "new best w\n",
      "iterations:  208\n",
      "Training loss: 0.02290637593684535, Validation loss: 0.04107796531965123\n",
      "new best w\n",
      "iterations:  209\n",
      "Training loss: 0.022892402677090327, Validation loss: 0.04106504145491482\n",
      "new best w\n",
      "iterations:  210\n",
      "Training loss: 0.022880012635209324, Validation loss: 0.04104517311354458\n",
      "new best w\n",
      "iterations:  211\n",
      "Training loss: 0.022866271407763917, Validation loss: 0.04102457610920328\n",
      "new best w\n",
      "iterations:  212\n",
      "Training loss: 0.022853686884050622, Validation loss: 0.041008307004004804\n",
      "new best w\n",
      "iterations:  213\n",
      "Training loss: 0.022840240249538565, Validation loss: 0.04098767107347821\n",
      "new best w\n",
      "iterations:  214\n",
      "Training loss: 0.02282747849621807, Validation loss: 0.04097080868687293\n",
      "new best w\n",
      "iterations:  215\n",
      "Training loss: 0.022814292225222278, Validation loss: 0.04095088037748395\n",
      "new best w\n",
      "iterations:  216\n",
      "Training loss: 0.02280160049741115, Validation loss: 0.0409418899903723\n",
      "new best w\n",
      "iterations:  217\n",
      "Training loss: 0.022788610502951633, Validation loss: 0.04092595505991197\n",
      "new best w\n",
      "iterations:  218\n",
      "Training loss: 0.02277650327709127, Validation loss: 0.04089975016323957\n",
      "new best w\n",
      "iterations:  219\n",
      "Training loss: 0.022763804668842667, Validation loss: 0.04089056378267767\n",
      "new best w\n",
      "iterations:  220\n",
      "Training loss: 0.022751900076571307, Validation loss: 0.04086735237240827\n",
      "new best w\n",
      "iterations:  221\n",
      "Training loss: 0.0227391276215247, Validation loss: 0.0408566695329534\n",
      "new best w\n",
      "iterations:  222\n",
      "Training loss: 0.022727434742910387, Validation loss: 0.04083337964429471\n",
      "new best w\n",
      "iterations:  223\n",
      "Training loss: 0.022714643809660325, Validation loss: 0.04082340991934326\n",
      "new best w\n",
      "iterations:  224\n",
      "Training loss: 0.02270295858129987, Validation loss: 0.04080476045022782\n",
      "new best w\n",
      "iterations:  225\n",
      "Training loss: 0.02269057548585291, Validation loss: 0.04078576876999947\n",
      "new best w\n",
      "iterations:  226\n",
      "Training loss: 0.02267971729122992, Validation loss: 0.040783034726396133\n",
      "new best w\n",
      "iterations:  227\n",
      "Training loss: 0.022667643848251803, Validation loss: 0.04075678512169825\n",
      "new best w\n",
      "iterations:  228\n",
      "Training loss: 0.022656507061262553, Validation loss: 0.04075274518555364\n",
      "new best w\n",
      "iterations:  229\n",
      "Training loss: 0.02264459647400483, Validation loss: 0.040723323519419666\n",
      "new best w\n",
      "iterations:  230\n",
      "Training loss: 0.022633298917920885, Validation loss: 0.04072898740784007\n",
      "iterations:  231\n",
      "Training loss: 0.02262127928296684, Validation loss: 0.040705283878366624\n",
      "new best w\n",
      "iterations:  232\n",
      "Training loss: 0.02260958702044876, Validation loss: 0.04068724587275902\n",
      "new best w\n",
      "iterations:  233\n",
      "Training loss: 0.022597454032583543, Validation loss: 0.04067555303441096\n",
      "new best w\n",
      "iterations:  234\n",
      "Training loss: 0.022585890385617136, Validation loss: 0.04064725902058476\n",
      "new best w\n",
      "iterations:  235\n",
      "Training loss: 0.022573641634868018, Validation loss: 0.04064363944850112\n",
      "new best w\n",
      "iterations:  236\n",
      "Training loss: 0.02256220894307558, Validation loss: 0.04061736330009123\n",
      "new best w\n",
      "iterations:  237\n",
      "Training loss: 0.02254988567145547, Validation loss: 0.040613534767122275\n",
      "new best w\n",
      "iterations:  238\n",
      "Training loss: 0.022538552163671243, Validation loss: 0.040586807139104786\n",
      "new best w\n",
      "iterations:  239\n",
      "Training loss: 0.022526198535192433, Validation loss: 0.040583498701806414\n",
      "new best w\n",
      "iterations:  240\n",
      "Training loss: 0.022514915138766717, Validation loss: 0.04055740065844715\n",
      "new best w\n",
      "iterations:  241\n",
      "Training loss: 0.02250266833338602, Validation loss: 0.04055378873309549\n",
      "new best w\n",
      "iterations:  242\n",
      "Training loss: 0.02249116609562563, Validation loss: 0.04054494820849559\n",
      "new best w\n",
      "iterations:  243\n",
      "Training loss: 0.022479467048649737, Validation loss: 0.04051202830601686\n",
      "new best w\n",
      "iterations:  244\n",
      "Training loss: 0.022467522716195204, Validation loss: 0.04052682182359084\n",
      "iterations:  245\n",
      "Training loss: 0.022456190003926874, Validation loss: 0.0404812113309449\n",
      "new best w\n",
      "iterations:  246\n",
      "Training loss: 0.022443829274982775, Validation loss: 0.04048408994071634\n",
      "iterations:  247\n",
      "Training loss: 0.02243264840566393, Validation loss: 0.04045282130921693\n",
      "new best w\n",
      "iterations:  248\n",
      "Training loss: 0.022420404821140626, Validation loss: 0.04044981762100645\n",
      "new best w\n",
      "iterations:  249\n",
      "Training loss: 0.02240907642938834, Validation loss: 0.040423800777181315\n",
      "new best w\n",
      "iterations:  250\n",
      "Training loss: 0.022397002790169287, Validation loss: 0.040420940531887314\n",
      "new best w\n",
      "iterations:  251\n",
      "Training loss: 0.0223855190859171, Validation loss: 0.04039425477187554\n",
      "new best w\n",
      "iterations:  252\n",
      "Training loss: 0.022373638695192963, Validation loss: 0.04039195203820528\n",
      "new best w\n",
      "iterations:  253\n",
      "Training loss: 0.022361975910809503, Validation loss: 0.04036598551697605\n",
      "new best w\n",
      "iterations:  254\n",
      "Training loss: 0.022350320995262928, Validation loss: 0.04036330002178355\n",
      "new best w\n",
      "iterations:  255\n",
      "Training loss: 0.022338560527413384, Validation loss: 0.04033665215976302\n",
      "new best w\n",
      "iterations:  256\n",
      "Training loss: 0.022326929935425618, Validation loss: 0.04033476220623705\n",
      "new best w\n",
      "iterations:  257\n",
      "Training loss: 0.02231553660856072, Validation loss: 0.0403196416956061\n",
      "new best w\n",
      "iterations:  258\n",
      "Training loss: 0.02230344112080761, Validation loss: 0.04030482035718548\n",
      "new best w\n",
      "iterations:  259\n",
      "Training loss: 0.02229238553796463, Validation loss: 0.04029245659812876\n",
      "new best w\n",
      "iterations:  260\n",
      "Training loss: 0.0222799160733092, Validation loss: 0.040264655019741684\n",
      "new best w\n",
      "iterations:  261\n",
      "Training loss: 0.022269240030838938, Validation loss: 0.04026292123641699\n",
      "new best w\n",
      "iterations:  262\n",
      "Training loss: 0.02225644685188892, Validation loss: 0.0402367189928046\n",
      "new best w\n",
      "iterations:  263\n",
      "Training loss: 0.022246111067246294, Validation loss: 0.04023507208230739\n",
      "new best w\n",
      "iterations:  264\n",
      "Training loss: 0.022233353048638008, Validation loss: 0.04020516759926292\n",
      "new best w\n",
      "iterations:  265\n",
      "Training loss: 0.02222254563155282, Validation loss: 0.04020589781095063\n",
      "iterations:  266\n",
      "Training loss: 0.02221034642039185, Validation loss: 0.04017392734094121\n",
      "new best w\n",
      "iterations:  267\n",
      "Training loss: 0.022198984289452505, Validation loss: 0.040178278892781215\n",
      "iterations:  268\n",
      "Training loss: 0.022187361184408543, Validation loss: 0.04014659297474695\n",
      "new best w\n",
      "iterations:  269\n",
      "Training loss: 0.022175466150000683, Validation loss: 0.04014953127713976\n",
      "iterations:  270\n",
      "Training loss: 0.022164367850124135, Validation loss: 0.040117908192148256\n",
      "new best w\n",
      "iterations:  271\n",
      "Training loss: 0.02215249216410829, Validation loss: 0.040125106337933\n",
      "iterations:  272\n",
      "Training loss: 0.022140991465984486, Validation loss: 0.04009945629064003\n",
      "new best w\n",
      "iterations:  273\n",
      "Training loss: 0.022129571528902447, Validation loss: 0.040089394031047124\n",
      "new best w\n",
      "iterations:  274\n",
      "Training loss: 0.02211742444378718, Validation loss: 0.04006191706965771\n",
      "new best w\n",
      "iterations:  275\n",
      "Training loss: 0.022106649799974662, Validation loss: 0.0400614704449968\n",
      "new best w\n",
      "iterations:  276\n",
      "Training loss: 0.022094116607238208, Validation loss: 0.04003390890441619\n",
      "new best w\n",
      "iterations:  277\n",
      "Training loss: 0.022083521534300453, Validation loss: 0.04003333816192288\n",
      "new best w\n",
      "iterations:  278\n",
      "Training loss: 0.022071203752123743, Validation loss: 0.04000182108155871\n",
      "new best w\n",
      "iterations:  279\n",
      "Training loss: 0.022060025590933098, Validation loss: 0.04000542355466806\n",
      "iterations:  280\n",
      "Training loss: 0.022048267320334995, Validation loss: 0.03997345690168825\n",
      "new best w\n",
      "iterations:  281\n",
      "Training loss: 0.022036546279815012, Validation loss: 0.039977934495503915\n",
      "iterations:  282\n",
      "Training loss: 0.022025346769643864, Validation loss: 0.03994666698229658\n",
      "new best w\n",
      "iterations:  283\n",
      "Training loss: 0.02201366794851445, Validation loss: 0.039951174473171154\n",
      "iterations:  284\n",
      "Training loss: 0.02200185451970814, Validation loss: 0.03991859446346724\n",
      "new best w\n",
      "iterations:  285\n",
      "Training loss: 0.021990893399140482, Validation loss: 0.039917683604816255\n",
      "new best w\n",
      "iterations:  286\n",
      "Training loss: 0.02197842116782968, Validation loss: 0.03990307275755654\n",
      "new best w\n",
      "iterations:  287\n",
      "Training loss: 0.021968396516042862, Validation loss: 0.039891460014545424\n",
      "new best w\n",
      "iterations:  288\n",
      "Training loss: 0.021955700019436072, Validation loss: 0.039863660183149484\n",
      "new best w\n",
      "iterations:  289\n",
      "Training loss: 0.021945557275247128, Validation loss: 0.03986386887047188\n",
      "iterations:  290\n",
      "Training loss: 0.021933437416246128, Validation loss: 0.03981648346290512\n",
      "new best w\n",
      "iterations:  291\n",
      "Training loss: 0.02192246221461804, Validation loss: 0.03982157578047596\n",
      "iterations:  292\n",
      "Training loss: 0.021911085642611648, Validation loss: 0.03981234514328709\n",
      "new best w\n",
      "iterations:  293\n",
      "Training loss: 0.0218997040063496, Validation loss: 0.039790645822573245\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  294\n",
      "Training loss: 0.02188834698201615, Validation loss: 0.039790833632368384\n",
      "iterations:  295\n",
      "Training loss: 0.021877277833389348, Validation loss: 0.039758610933136575\n",
      "new best w\n",
      "iterations:  296\n",
      "Training loss: 0.021865330544727266, Validation loss: 0.03976409542352442\n",
      "iterations:  297\n",
      "Training loss: 0.02185481147316747, Validation loss: 0.03973230812949899\n",
      "new best w\n",
      "iterations:  298\n",
      "Training loss: 0.021842763797235657, Validation loss: 0.03973838557482555\n",
      "iterations:  299\n",
      "Training loss: 0.02183188034349193, Validation loss: 0.039706232975168024\n",
      "new best w\n",
      "iterations:  300\n",
      "Training loss: 0.021820577820786658, Validation loss: 0.03970609801291622\n",
      "new best w\n",
      "iterations:  301\n",
      "Training loss: 0.02180869358934774, Validation loss: 0.039679985206469634\n",
      "new best w\n",
      "iterations:  302\n",
      "Training loss: 0.021798413152292126, Validation loss: 0.039679676135768935\n",
      "new best w\n",
      "iterations:  303\n",
      "Training loss: 0.02178611998947054, Validation loss: 0.0396649835380114\n",
      "new best w\n",
      "iterations:  304\n",
      "Training loss: 0.021775585835594247, Validation loss: 0.039654561238360295\n",
      "new best w\n",
      "iterations:  305\n",
      "Training loss: 0.02176371370093323, Validation loss: 0.03962091680965105\n",
      "new best w\n",
      "iterations:  306\n",
      "Training loss: 0.021752567002445034, Validation loss: 0.03962689908390692\n",
      "iterations:  307\n",
      "Training loss: 0.021741380731071812, Validation loss: 0.03959464069481533\n",
      "new best w\n",
      "iterations:  308\n",
      "Training loss: 0.021730193993434538, Validation loss: 0.039587305785719305\n",
      "new best w\n",
      "iterations:  309\n",
      "Training loss: 0.02171837706765542, Validation loss: 0.03957801598662485\n",
      "new best w\n",
      "iterations:  310\n",
      "Training loss: 0.021707817164789724, Validation loss: 0.03955035902580524\n",
      "new best w\n",
      "iterations:  311\n",
      "Training loss: 0.021695623717292545, Validation loss: 0.03955688736495389\n",
      "iterations:  312\n",
      "Training loss: 0.021685111677238827, Validation loss: 0.0395240456933393\n",
      "new best w\n",
      "iterations:  313\n",
      "Training loss: 0.021673499361255507, Validation loss: 0.039524811240542815\n",
      "iterations:  314\n",
      "Training loss: 0.021661794246194375, Validation loss: 0.0394979869152816\n",
      "new best w\n",
      "iterations:  315\n",
      "Training loss: 0.021651361930129134, Validation loss: 0.03949840766868537\n",
      "iterations:  316\n",
      "Training loss: 0.021638995811761853, Validation loss: 0.03947116577786354\n",
      "new best w\n",
      "iterations:  317\n",
      "Training loss: 0.021628659231246863, Validation loss: 0.039472597334312645\n",
      "iterations:  318\n",
      "Training loss: 0.021616687847959767, Validation loss: 0.03943947071365412\n",
      "new best w\n",
      "iterations:  319\n",
      "Training loss: 0.02160567759318859, Validation loss: 0.03944467246495459\n",
      "iterations:  320\n",
      "Training loss: 0.021594504094540605, Validation loss: 0.039424106458937694\n",
      "new best w\n",
      "iterations:  321\n",
      "Training loss: 0.021583273883764718, Validation loss: 0.03939511702290795\n",
      "new best w\n",
      "iterations:  322\n",
      "Training loss: 0.021571535477512897, Validation loss: 0.03940232358992594\n",
      "iterations:  323\n",
      "Training loss: 0.021560814890712446, Validation loss: 0.039368782106122505\n",
      "new best w\n",
      "iterations:  324\n",
      "Training loss: 0.021549023927830282, Validation loss: 0.03937540554264871\n",
      "iterations:  325\n",
      "Training loss: 0.02153776606265031, Validation loss: 0.03934301611663786\n",
      "new best w\n",
      "iterations:  326\n",
      "Training loss: 0.021526974287107564, Validation loss: 0.03934384891087361\n",
      "iterations:  327\n",
      "Training loss: 0.02151445128296611, Validation loss: 0.039316941291794534\n",
      "new best w\n",
      "iterations:  328\n",
      "Training loss: 0.02150471206853372, Validation loss: 0.03930462306042433\n",
      "new best w\n",
      "iterations:  329\n",
      "Training loss: 0.021492569982758794, Validation loss: 0.03929472611368368\n",
      "new best w\n",
      "iterations:  330\n",
      "Training loss: 0.02148154850596598, Validation loss: 0.039272672704014376\n",
      "new best w\n",
      "iterations:  331\n",
      "Training loss: 0.021470575654286613, Validation loss: 0.03927449790571996\n",
      "iterations:  332\n",
      "Training loss: 0.021458906990105563, Validation loss: 0.039247497197490974\n",
      "new best w\n",
      "iterations:  333\n",
      "Training loss: 0.02144801523321014, Validation loss: 0.039249733821764235\n",
      "iterations:  334\n",
      "Training loss: 0.021436569606270293, Validation loss: 0.039216124342976236\n",
      "new best w\n",
      "iterations:  335\n",
      "Training loss: 0.021425393603899563, Validation loss: 0.03922329123422727\n",
      "iterations:  336\n",
      "Training loss: 0.021414027099609028, Validation loss: 0.039191160498220794\n",
      "new best w\n",
      "iterations:  337\n",
      "Training loss: 0.021403467513898374, Validation loss: 0.039182584039159724\n",
      "new best w\n",
      "iterations:  338\n",
      "Training loss: 0.02139130068687503, Validation loss: 0.039183872008327444\n",
      "iterations:  339\n",
      "Training loss: 0.02138105193928037, Validation loss: 0.03914727621080063\n",
      "new best w\n",
      "iterations:  340\n",
      "Training loss: 0.021369389924938656, Validation loss: 0.03915423443490269\n",
      "iterations:  341\n",
      "Training loss: 0.021357951881431, Validation loss: 0.03912287643734592\n",
      "new best w\n",
      "iterations:  342\n",
      "Training loss: 0.02134771190672913, Validation loss: 0.039124569687573066\n",
      "iterations:  343\n",
      "Training loss: 0.021335450831507643, Validation loss: 0.03909662765874932\n",
      "new best w\n",
      "iterations:  344\n",
      "Training loss: 0.021325358271792647, Validation loss: 0.03908617953022994\n",
      "new best w\n",
      "iterations:  345\n",
      "Training loss: 0.02131378073463345, Validation loss: 0.03907610761976178\n",
      "new best w\n",
      "iterations:  346\n",
      "Training loss: 0.02130251502009063, Validation loss: 0.039053878165658536\n",
      "new best w\n",
      "iterations:  347\n",
      "Training loss: 0.021291795870471652, Validation loss: 0.03905715150837242\n",
      "iterations:  348\n",
      "Training loss: 0.021280410017010337, Validation loss: 0.03902315416656729\n",
      "new best w\n",
      "iterations:  349\n",
      "Training loss: 0.021269136279526524, Validation loss: 0.03903108514528171\n",
      "iterations:  350\n",
      "Training loss: 0.021258168860128452, Validation loss: 0.03899829199086889\n",
      "new best w\n",
      "iterations:  351\n",
      "Training loss: 0.02124742804835433, Validation loss: 0.0389928099919155\n",
      "new best w\n",
      "iterations:  352\n",
      "Training loss: 0.021235459474666336, Validation loss: 0.03898374846820485\n",
      "new best w\n",
      "iterations:  353\n",
      "Training loss: 0.021225243667871995, Validation loss: 0.03895485408383858\n",
      "new best w\n",
      "iterations:  354\n",
      "Training loss: 0.021213663517931792, Validation loss: 0.03896338025609773\n",
      "iterations:  355\n",
      "Training loss: 0.02120239779572717, Validation loss: 0.03893087772602401\n",
      "new best w\n",
      "iterations:  356\n",
      "Training loss: 0.021192207564316917, Validation loss: 0.03893310323738037\n",
      "iterations:  357\n",
      "Training loss: 0.021180044542778706, Validation loss: 0.038916501972830204\n",
      "new best w\n",
      "iterations:  358\n",
      "Training loss: 0.02117000771577806, Validation loss: 0.03889524226108146\n",
      "new best w\n",
      "iterations:  359\n",
      "Training loss: 0.02115859751772601, Validation loss: 0.038884314387241055\n",
      "new best w\n",
      "iterations:  360\n",
      "Training loss: 0.02114723822391204, Validation loss: 0.038861925169981756\n",
      "new best w\n",
      "iterations:  361\n",
      "Training loss: 0.021136690788525953, Validation loss: 0.038865376014185014\n",
      "iterations:  362\n",
      "Training loss: 0.021125201119206665, Validation loss: 0.038830796460140864\n",
      "new best w\n",
      "iterations:  363\n",
      "Training loss: 0.02111430041149032, Validation loss: 0.03883941481023001\n",
      "iterations:  364\n",
      "Training loss: 0.021103149841857086, Validation loss: 0.03880639493319983\n",
      "new best w\n",
      "iterations:  365\n",
      "Training loss: 0.021092513961282325, Validation loss: 0.038787790052410766\n",
      "new best w\n",
      "iterations:  366\n",
      "Training loss: 0.02108062077671757, Validation loss: 0.03879654037361309\n",
      "iterations:  367\n",
      "Training loss: 0.021070226637947248, Validation loss: 0.03876349849512615\n",
      "new best w\n",
      "iterations:  368\n",
      "Training loss: 0.021059247564174783, Validation loss: 0.03876638398516133\n",
      "iterations:  369\n",
      "Training loss: 0.02104709656995989, Validation loss: 0.03873835721699831\n",
      "new best w\n",
      "iterations:  370\n",
      "Training loss: 0.021037493540521045, Validation loss: 0.03872717463735052\n",
      "new best w\n",
      "iterations:  371\n",
      "Training loss: 0.02102547229529204, Validation loss: 0.03872353387472052\n",
      "new best w\n",
      "iterations:  372\n",
      "Training loss: 0.0210143681116726, Validation loss: 0.03869536767228483\n",
      "new best w\n",
      "iterations:  373\n",
      "Training loss: 0.021003961224700284, Validation loss: 0.0386984794853001\n",
      "iterations:  374\n",
      "Training loss: 0.020991891174793588, Validation loss: 0.03866953834325838\n",
      "new best w\n",
      "iterations:  375\n",
      "Training loss: 0.02098159729870706, Validation loss: 0.03867419464811523\n",
      "iterations:  376\n",
      "Training loss: 0.020970255049700178, Validation loss: 0.03863894287510591\n",
      "new best w\n",
      "iterations:  377\n",
      "Training loss: 0.020959174688227628, Validation loss: 0.03863716182029945\n",
      "new best w\n",
      "iterations:  378\n",
      "Training loss: 0.020948015625335707, Validation loss: 0.03863164653561145\n",
      "new best w\n",
      "iterations:  379\n",
      "Training loss: 0.020936959351129585, Validation loss: 0.03859580992953165\n",
      "new best w\n",
      "iterations:  380\n",
      "Training loss: 0.02092605093441654, Validation loss: 0.03860502874321608\n",
      "iterations:  381\n",
      "Training loss: 0.020914336821854267, Validation loss: 0.038570529314662996\n",
      "new best w\n",
      "iterations:  382\n",
      "Training loss: 0.02090413338897609, Validation loss: 0.038551496113873325\n",
      "new best w\n",
      "iterations:  383\n",
      "Training loss: 0.020892321598771413, Validation loss: 0.038561702304363864\n",
      "iterations:  384\n",
      "Training loss: 0.020881160169297613, Validation loss: 0.03852808826392341\n",
      "new best w\n",
      "iterations:  385\n",
      "Training loss: 0.0208708319119283, Validation loss: 0.038531315399690076\n",
      "iterations:  386\n",
      "Training loss: 0.020858547904281592, Validation loss: 0.03850244341157561\n",
      "new best w\n",
      "iterations:  387\n",
      "Training loss: 0.020848445022459975, Validation loss: 0.03848406097731049\n",
      "new best w\n",
      "iterations:  388\n",
      "Training loss: 0.020837596242538014, Validation loss: 0.038489688280181365\n",
      "iterations:  389\n",
      "Training loss: 0.02082622717264162, Validation loss: 0.03846063849441057\n",
      "new best w\n",
      "iterations:  390\n",
      "Training loss: 0.02081625414765799, Validation loss: 0.03846604443577659\n",
      "iterations:  391\n",
      "Training loss: 0.020804995336446152, Validation loss: 0.038430293709020596\n",
      "new best w\n",
      "iterations:  392\n",
      "Training loss: 0.020794507785665918, Validation loss: 0.03841772100823901\n",
      "new best w\n",
      "iterations:  393\n",
      "Training loss: 0.020783703856095247, Validation loss: 0.03842343217920818\n",
      "iterations:  394\n",
      "Training loss: 0.0207729979581101, Validation loss: 0.03838771727908889\n",
      "new best w\n",
      "iterations:  395\n",
      "Training loss: 0.020762248348512168, Validation loss: 0.03839784348947861\n",
      "iterations:  396\n",
      "Training loss: 0.020751092127376854, Validation loss: 0.03836368102264052\n",
      "new best w\n",
      "iterations:  397\n",
      "Training loss: 0.02074126707524396, Validation loss: 0.03834415731521787\n",
      "new best w\n",
      "iterations:  398\n",
      "Training loss: 0.02072974527670089, Validation loss: 0.03836037574070188\n",
      "iterations:  399\n",
      "Training loss: 0.020719125327838258, Validation loss: 0.0383276365883441\n",
      "new best w\n",
      "iterations:  400\n",
      "Training loss: 0.020709053601345565, Validation loss: 0.03832519137336411\n",
      "new best w\n",
      "iterations:  401\n",
      "Training loss: 0.0206973615123672, Validation loss: 0.03829502828650782\n",
      "new best w\n",
      "iterations:  402\n",
      "Training loss: 0.02068751658158098, Validation loss: 0.03827719420395029\n",
      "new best w\n",
      "iterations:  403\n",
      "Training loss: 0.020676700826751108, Validation loss: 0.03828292417889258\n",
      "iterations:  404\n",
      "Training loss: 0.0206654944430706, Validation loss: 0.03825320737193212\n",
      "new best w\n",
      "iterations:  405\n",
      "Training loss: 0.020655462347090214, Validation loss: 0.03825906404118927\n",
      "iterations:  406\n",
      "Training loss: 0.020644307987585805, Validation loss: 0.038223546811322465\n",
      "new best w\n",
      "iterations:  407\n",
      "Training loss: 0.020633915141853986, Validation loss: 0.038211000003285346\n",
      "new best w\n",
      "iterations:  408\n",
      "Training loss: 0.02062306776186308, Validation loss: 0.03821712082699057\n",
      "iterations:  409\n",
      "Training loss: 0.020612306074787697, Validation loss: 0.038181627198029214\n",
      "new best w\n",
      "iterations:  410\n",
      "Training loss: 0.02060200444050267, Validation loss: 0.03819244383143634\n",
      "iterations:  411\n",
      "Training loss: 0.020590609974146613, Validation loss: 0.03815738166584901\n",
      "new best w\n",
      "iterations:  412\n",
      "Training loss: 0.020580699377583826, Validation loss: 0.03813876979537852\n",
      "new best w\n",
      "iterations:  413\n",
      "Training loss: 0.020569646493967298, Validation loss: 0.03814959669448334\n",
      "iterations:  414\n",
      "Training loss: 0.02055836606216431, Validation loss: 0.03811572264570141\n",
      "new best w\n",
      "iterations:  415\n",
      "Training loss: 0.020548942466572236, Validation loss: 0.038120756757467726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  416\n",
      "Training loss: 0.020537241819723648, Validation loss: 0.03809015026110287\n",
      "new best w\n",
      "iterations:  417\n",
      "Training loss: 0.02052669302839619, Validation loss: 0.0380722252812741\n",
      "new best w\n",
      "iterations:  418\n",
      "Training loss: 0.020516733767080693, Validation loss: 0.03808008226925562\n",
      "iterations:  419\n",
      "Training loss: 0.02050521916182273, Validation loss: 0.03804973341980038\n",
      "new best w\n",
      "iterations:  420\n",
      "Training loss: 0.020495322856321708, Validation loss: 0.03804460695463916\n",
      "new best w\n",
      "iterations:  421\n",
      "Training loss: 0.020484775873912606, Validation loss: 0.0380397036378443\n",
      "new best w\n",
      "iterations:  422\n",
      "Training loss: 0.020473933069616982, Validation loss: 0.038007893913372184\n",
      "new best w\n",
      "iterations:  423\n",
      "Training loss: 0.020463362117867943, Validation loss: 0.0380144574917877\n",
      "iterations:  424\n",
      "Training loss: 0.02045276907023681, Validation loss: 0.037978255007172085\n",
      "new best w\n",
      "iterations:  425\n",
      "Training loss: 0.02044265808851726, Validation loss: 0.0379651882597804\n",
      "new best w\n",
      "iterations:  426\n",
      "Training loss: 0.020431301516548192, Validation loss: 0.03797253205541557\n",
      "iterations:  427\n",
      "Training loss: 0.020421137082417825, Validation loss: 0.03793684922291757\n",
      "new best w\n",
      "iterations:  428\n",
      "Training loss: 0.02041100304523427, Validation loss: 0.03794228299938065\n",
      "iterations:  429\n",
      "Training loss: 0.02039929174395397, Validation loss: 0.03791139347945276\n",
      "new best w\n",
      "iterations:  430\n",
      "Training loss: 0.020389730604851622, Validation loss: 0.03789409715426226\n",
      "new best w\n",
      "iterations:  431\n",
      "Training loss: 0.02037899184923447, Validation loss: 0.037900285084459404\n",
      "iterations:  432\n",
      "Training loss: 0.020367452838883394, Validation loss: 0.0378688283891893\n",
      "new best w\n",
      "iterations:  433\n",
      "Training loss: 0.020358174824022277, Validation loss: 0.037861595814361576\n",
      "new best w\n",
      "iterations:  434\n",
      "Training loss: 0.020346955816482937, Validation loss: 0.03785064943786266\n",
      "new best w\n",
      "iterations:  435\n",
      "Training loss: 0.020336133958979867, Validation loss: 0.03782645300746022\n",
      "new best w\n",
      "iterations:  436\n",
      "Training loss: 0.0203261171135626, Validation loss: 0.03783556050154132\n",
      "iterations:  437\n",
      "Training loss: 0.02031494753734973, Validation loss: 0.03779696207315813\n",
      "new best w\n",
      "iterations:  438\n",
      "Training loss: 0.020304840426819893, Validation loss: 0.03778315034958357\n",
      "new best w\n",
      "iterations:  439\n",
      "Training loss: 0.02029395832748777, Validation loss: 0.03779148936532115\n",
      "iterations:  440\n",
      "Training loss: 0.020283316236067092, Validation loss: 0.037754538539264054\n",
      "new best w\n",
      "iterations:  441\n",
      "Training loss: 0.020273198855211234, Validation loss: 0.03776683053662421\n",
      "iterations:  442\n",
      "Training loss: 0.020261791764674145, Validation loss: 0.03773020684573253\n",
      "new best w\n",
      "iterations:  443\n",
      "Training loss: 0.02025199934787817, Validation loss: 0.037722900303945595\n",
      "new best w\n",
      "iterations:  444\n",
      "Training loss: 0.020241270990580564, Validation loss: 0.03772476178353491\n",
      "iterations:  445\n",
      "Training loss: 0.020229657993657585, Validation loss: 0.037687714321103\n",
      "new best w\n",
      "iterations:  446\n",
      "Training loss: 0.020220639759647344, Validation loss: 0.03767891848874154\n",
      "new best w\n",
      "iterations:  447\n",
      "Training loss: 0.02020924962245183, Validation loss: 0.03767446982183799\n",
      "new best w\n",
      "iterations:  448\n",
      "Training loss: 0.020198104554576376, Validation loss: 0.03764500903713915\n",
      "new best w\n",
      "iterations:  449\n",
      "Training loss: 0.020188683486496092, Validation loss: 0.037652783760913786\n",
      "iterations:  450\n",
      "Training loss: 0.02017723709061274, Validation loss: 0.037614338613236833\n",
      "new best w\n",
      "iterations:  451\n",
      "Training loss: 0.0201667649157155, Validation loss: 0.03760142060127502\n",
      "new best w\n",
      "iterations:  452\n",
      "Training loss: 0.02015650015335295, Validation loss: 0.037610350134157255\n",
      "iterations:  453\n",
      "Training loss: 0.020145313962126515, Validation loss: 0.03757144240993841\n",
      "new best w\n",
      "iterations:  454\n",
      "Training loss: 0.020135333627653136, Validation loss: 0.03756851548119875\n",
      "new best w\n",
      "iterations:  455\n",
      "Training loss: 0.02012424548712372, Validation loss: 0.03755904457330406\n",
      "new best w\n",
      "iterations:  456\n",
      "Training loss: 0.020113816746113896, Validation loss: 0.03752867030271928\n",
      "new best w\n",
      "iterations:  457\n",
      "Training loss: 0.020103478803278528, Validation loss: 0.03754189020649415\n",
      "iterations:  458\n",
      "Training loss: 0.020092010771200326, Validation loss: 0.03750444722123885\n",
      "new best w\n",
      "iterations:  459\n",
      "Training loss: 0.02008239790028026, Validation loss: 0.03748629525387946\n",
      "new best w\n",
      "iterations:  460\n",
      "Training loss: 0.020071494952624322, Validation loss: 0.03749849206126134\n",
      "iterations:  461\n",
      "Training loss: 0.020059878453209792, Validation loss: 0.0374615800205745\n",
      "new best w\n",
      "iterations:  462\n",
      "Training loss: 0.020050884072921184, Validation loss: 0.037453166376037794\n",
      "new best w\n",
      "iterations:  463\n",
      "Training loss: 0.020039471545186767, Validation loss: 0.037448554434430674\n",
      "new best w\n",
      "iterations:  464\n",
      "Training loss: 0.020028301357177914, Validation loss: 0.03741863267152096\n",
      "new best w\n",
      "iterations:  465\n",
      "Training loss: 0.0200188795530737, Validation loss: 0.037430598786342625\n",
      "iterations:  466\n",
      "Training loss: 0.020007510298618407, Validation loss: 0.037389481856637714\n",
      "new best w\n",
      "iterations:  467\n",
      "Training loss: 0.01999698353739106, Validation loss: 0.03738021313874289\n",
      "new best w\n",
      "iterations:  468\n",
      "Training loss: 0.019986595446436006, Validation loss: 0.037384376760085686\n",
      "iterations:  469\n",
      "Training loss: 0.019975530243912947, Validation loss: 0.03734440742984883\n",
      "new best w\n",
      "iterations:  470\n",
      "Training loss: 0.01996552352849193, Validation loss: 0.037341915750564614\n",
      "new best w\n",
      "iterations:  471\n",
      "Training loss: 0.01995427100141312, Validation loss: 0.03733219401561543\n",
      "new best w\n",
      "iterations:  472\n",
      "Training loss: 0.01994386603144618, Validation loss: 0.037301398066968264\n",
      "new best w\n",
      "iterations:  473\n",
      "Training loss: 0.019933755213621944, Validation loss: 0.037315346911572116\n",
      "iterations:  474\n",
      "Training loss: 0.01992199607187385, Validation loss: 0.03727696578258447\n",
      "new best w\n",
      "iterations:  475\n",
      "Training loss: 0.019912328010214055, Validation loss: 0.03725942505618013\n",
      "new best w\n",
      "iterations:  476\n",
      "Training loss: 0.01990176844662272, Validation loss: 0.03726648486233395\n",
      "iterations:  477\n",
      "Training loss: 0.019890056590483797, Validation loss: 0.03723273112248727\n",
      "new best w\n",
      "iterations:  478\n",
      "Training loss: 0.01988053881047582, Validation loss: 0.03721597141251415\n",
      "new best w\n",
      "iterations:  479\n",
      "Training loss: 0.019869725195381636, Validation loss: 0.03722241174062413\n",
      "iterations:  480\n",
      "Training loss: 0.01985827230405235, Validation loss: 0.03718974798323198\n",
      "new best w\n",
      "iterations:  481\n",
      "Training loss: 0.019848565079018724, Validation loss: 0.037182410791583714\n",
      "new best w\n",
      "iterations:  482\n",
      "Training loss: 0.019837700640961232, Validation loss: 0.03717239934312426\n",
      "new best w\n",
      "iterations:  483\n",
      "Training loss: 0.019826679595362818, Validation loss: 0.03714660949597066\n",
      "new best w\n",
      "iterations:  484\n",
      "Training loss: 0.019816354637503256, Validation loss: 0.037155857148390974\n",
      "iterations:  485\n",
      "Training loss: 0.019805679246226296, Validation loss: 0.03711627697280021\n",
      "new best w\n",
      "iterations:  486\n",
      "Training loss: 0.019795131088331572, Validation loss: 0.03710328365170867\n",
      "new best w\n",
      "iterations:  487\n",
      "Training loss: 0.01978428126855657, Validation loss: 0.0371128036011218\n",
      "iterations:  488\n",
      "Training loss: 0.019773454039784106, Validation loss: 0.037073795705500456\n",
      "new best w\n",
      "iterations:  489\n",
      "Training loss: 0.019763574598245445, Validation loss: 0.03705933596998782\n",
      "new best w\n",
      "iterations:  490\n",
      "Training loss: 0.019752444051886742, Validation loss: 0.0370744315822913\n",
      "iterations:  491\n",
      "Training loss: 0.019741256853526394, Validation loss: 0.03703067115285926\n",
      "new best w\n",
      "iterations:  492\n",
      "Training loss: 0.01973189603318098, Validation loss: 0.03702156337831809\n",
      "new best w\n",
      "iterations:  493\n",
      "Training loss: 0.019720392978754497, Validation loss: 0.03701672823943891\n",
      "new best w\n",
      "iterations:  494\n",
      "Training loss: 0.019709272827607312, Validation loss: 0.036985996554412294\n",
      "new best w\n",
      "iterations:  495\n",
      "Training loss: 0.01969996329367042, Validation loss: 0.03699511068554146\n",
      "iterations:  496\n",
      "Training loss: 0.019688356034065782, Validation loss: 0.03696074211690649\n",
      "new best w\n",
      "iterations:  497\n",
      "Training loss: 0.01967765727014559, Validation loss: 0.03694178304929605\n",
      "new best w\n",
      "iterations:  498\n",
      "Training loss: 0.019667680626434324, Validation loss: 0.03695303013121531\n",
      "iterations:  499\n",
      "Training loss: 0.01965631749079949, Validation loss: 0.03691075905162627\n",
      "new best w\n",
      "iterations:  500\n",
      "Training loss: 0.01964614144088858, Validation loss: 0.036897954771501006\n",
      "new best w\n",
      "iterations:  501\n",
      "Training loss: 0.01963526611960663, Validation loss: 0.03690795503547408\n",
      "iterations:  502\n",
      "Training loss: 0.019624403043975388, Validation loss: 0.03686755078238958\n",
      "new best w\n",
      "iterations:  503\n",
      "Training loss: 0.01961442580153056, Validation loss: 0.03686609336385273\n",
      "new best w\n",
      "iterations:  504\n",
      "Training loss: 0.019602851359176252, Validation loss: 0.03685582965696638\n",
      "new best w\n",
      "iterations:  505\n",
      "Training loss: 0.019592575018978153, Validation loss: 0.03682485775483028\n",
      "new best w\n",
      "iterations:  506\n",
      "Training loss: 0.01958261295503274, Validation loss: 0.036834095552248966\n",
      "iterations:  507\n",
      "Training loss: 0.019570897782662328, Validation loss: 0.03679810576879021\n",
      "new best w\n",
      "iterations:  508\n",
      "Training loss: 0.019560545709740095, Validation loss: 0.0367813879089261\n",
      "new best w\n",
      "iterations:  509\n",
      "Training loss: 0.019550548133563796, Validation loss: 0.03678980747362367\n",
      "iterations:  510\n",
      "Training loss: 0.0195389404452583, Validation loss: 0.03675485346564184\n",
      "new best w\n",
      "iterations:  511\n",
      "Training loss: 0.01952845442774254, Validation loss: 0.036736663980445415\n",
      "new best w\n",
      "iterations:  512\n",
      "Training loss: 0.019518544758732, Validation loss: 0.03674651746143194\n",
      "iterations:  513\n",
      "Training loss: 0.019506956789910344, Validation loss: 0.036714110858557636\n",
      "new best w\n",
      "iterations:  514\n",
      "Training loss: 0.01949667536650964, Validation loss: 0.03670053952036572\n",
      "new best w\n",
      "iterations:  515\n",
      "Training loss: 0.01948631709240782, Validation loss: 0.036703609983209574\n",
      "iterations:  516\n",
      "Training loss: 0.01947500140520953, Validation loss: 0.03666141507455439\n",
      "new best w\n",
      "iterations:  517\n",
      "Training loss: 0.01946504060797025, Validation loss: 0.03666003402765759\n",
      "new best w\n",
      "iterations:  518\n",
      "Training loss: 0.019453793597261827, Validation loss: 0.03664954153104226\n",
      "new best w\n",
      "iterations:  519\n",
      "Training loss: 0.019443140676651205, Validation loss: 0.03661745365053566\n",
      "new best w\n",
      "iterations:  520\n",
      "Training loss: 0.019433228400547422, Validation loss: 0.03663376693174606\n",
      "iterations:  521\n",
      "Training loss: 0.019421427230960867, Validation loss: 0.03659264197838854\n",
      "new best w\n",
      "iterations:  522\n",
      "Training loss: 0.019411306371681092, Validation loss: 0.03657517148406473\n",
      "new best w\n",
      "iterations:  523\n",
      "Training loss: 0.019401177614480628, Validation loss: 0.03658398301194711\n",
      "iterations:  524\n",
      "Training loss: 0.019389503767911605, Validation loss: 0.0365476949700614\n",
      "new best w\n",
      "iterations:  525\n",
      "Training loss: 0.019379131829181506, Validation loss: 0.03653103044550014\n",
      "new best w\n",
      "iterations:  526\n",
      "Training loss: 0.019369127337585272, Validation loss: 0.03654006155532999\n",
      "iterations:  527\n",
      "Training loss: 0.019357507926124208, Validation loss: 0.036504222288454756\n",
      "new best w\n",
      "iterations:  528\n",
      "Training loss: 0.019346908247554237, Validation loss: 0.036486013204885634\n",
      "new best w\n",
      "iterations:  529\n",
      "Training loss: 0.01933712279429046, Validation loss: 0.03649577555819223\n",
      "iterations:  530\n",
      "Training loss: 0.01932548041196353, Validation loss: 0.036460511720178355\n",
      "new best w\n",
      "iterations:  531\n",
      "Training loss: 0.019315233111240938, Validation loss: 0.03644140747271167\n",
      "new best w\n",
      "iterations:  532\n",
      "Training loss: 0.01930462438017824, Validation loss: 0.03645356020839098\n",
      "iterations:  533\n",
      "Training loss: 0.019293466692094665, Validation loss: 0.03640990870456963\n",
      "new best w\n",
      "iterations:  534\n",
      "Training loss: 0.019283485609509963, Validation loss: 0.036409577996748314\n",
      "new best w\n",
      "iterations:  535\n",
      "Training loss: 0.019272037049972233, Validation loss: 0.03639879492487163\n",
      "new best w\n",
      "iterations:  536\n",
      "Training loss: 0.01926155569736582, Validation loss: 0.036366275274955416\n",
      "new best w\n",
      "iterations:  537\n",
      "Training loss: 0.019251627428256507, Validation loss: 0.036369578019066776\n",
      "iterations:  538\n",
      "Training loss: 0.019240073768428602, Validation loss: 0.03635367997164214\n",
      "new best w\n",
      "iterations:  539\n",
      "Training loss: 0.01922928047916965, Validation loss: 0.036330630881619023\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  540\n",
      "Training loss: 0.019219742436730757, Validation loss: 0.03633283038355966\n",
      "iterations:  541\n",
      "Training loss: 0.019208459870185646, Validation loss: 0.036295908390623265\n",
      "new best w\n",
      "iterations:  542\n",
      "Training loss: 0.01919732848944244, Validation loss: 0.03627971230929628\n",
      "new best w\n",
      "iterations:  543\n",
      "Training loss: 0.01918804723913717, Validation loss: 0.03629095057773572\n",
      "iterations:  544\n",
      "Training loss: 0.019176696735901236, Validation loss: 0.036253417383651176\n",
      "new best w\n",
      "iterations:  545\n",
      "Training loss: 0.01916591058621109, Validation loss: 0.03623398741112562\n",
      "new best w\n",
      "iterations:  546\n",
      "Training loss: 0.019155989347841046, Validation loss: 0.03624954806324102\n",
      "iterations:  547\n",
      "Training loss: 0.01914476956732763, Validation loss: 0.036204814024153036\n",
      "new best w\n",
      "iterations:  548\n",
      "Training loss: 0.01913452176068878, Validation loss: 0.03619259561669375\n",
      "new best w\n",
      "iterations:  549\n",
      "Training loss: 0.019123810779769535, Validation loss: 0.03620596911649784\n",
      "iterations:  550\n",
      "Training loss: 0.01911302454811731, Validation loss: 0.03616263420659418\n",
      "new best w\n",
      "iterations:  551\n",
      "Training loss: 0.01910296019922461, Validation loss: 0.03614998285593616\n",
      "new best w\n",
      "iterations:  552\n",
      "Training loss: 0.019092073562623962, Validation loss: 0.036161653200064675\n",
      "iterations:  553\n",
      "Training loss: 0.019080988635526926, Validation loss: 0.036121127027473596\n",
      "new best w\n",
      "iterations:  554\n",
      "Training loss: 0.019071224217071102, Validation loss: 0.03610069768423995\n",
      "new best w\n",
      "iterations:  555\n",
      "Training loss: 0.019060504582426637, Validation loss: 0.03611824862687333\n",
      "iterations:  556\n",
      "Training loss: 0.0190488082630174, Validation loss: 0.03607721533774222\n",
      "new best w\n",
      "iterations:  557\n",
      "Training loss: 0.019039640424661226, Validation loss: 0.036071930962862896\n",
      "new best w\n",
      "iterations:  558\n",
      "Training loss: 0.01902869872788604, Validation loss: 0.03606493803902101\n",
      "new best w\n",
      "iterations:  559\n",
      "Training loss: 0.01901711273566269, Validation loss: 0.036032500692270646\n",
      "new best w\n",
      "iterations:  560\n",
      "Training loss: 0.01900774878773532, Validation loss: 0.03602961883422674\n",
      "new best w\n",
      "iterations:  561\n",
      "Training loss: 0.01899674859083472, Validation loss: 0.036016065326849006\n",
      "new best w\n",
      "iterations:  562\n",
      "Training loss: 0.01898561220806675, Validation loss: 0.035988800012748574\n",
      "new best w\n",
      "iterations:  563\n",
      "Training loss: 0.01897562861911568, Validation loss: 0.03598560806490615\n",
      "new best w\n",
      "iterations:  564\n",
      "Training loss: 0.018964976562427825, Validation loss: 0.03597716090991313\n",
      "new best w\n",
      "iterations:  565\n",
      "Training loss: 0.018953940519661042, Validation loss: 0.03594636541494858\n",
      "new best w\n",
      "iterations:  566\n",
      "Training loss: 0.018943984836296508, Validation loss: 0.035948117153177694\n",
      "iterations:  567\n",
      "Training loss: 0.018932945146240172, Validation loss: 0.03593167464892082\n",
      "new best w\n",
      "iterations:  568\n",
      "Training loss: 0.018922172703573957, Validation loss: 0.03589658917448136\n",
      "new best w\n",
      "iterations:  569\n",
      "Training loss: 0.01891246661858674, Validation loss: 0.0359151934531522\n",
      "iterations:  570\n",
      "Training loss: 0.018900673472806395, Validation loss: 0.035873026052130226\n",
      "new best w\n",
      "iterations:  571\n",
      "Training loss: 0.018890553052956203, Validation loss: 0.03585416326854793\n",
      "new best w\n",
      "iterations:  572\n",
      "Training loss: 0.018880671196173072, Validation loss: 0.03587201459316896\n",
      "iterations:  573\n",
      "Training loss: 0.018868956393727425, Validation loss: 0.035827853806279036\n",
      "new best w\n",
      "iterations:  574\n",
      "Training loss: 0.01885861358981223, Validation loss: 0.03581208431149568\n",
      "new best w\n",
      "iterations:  575\n",
      "Training loss: 0.018848695103543246, Validation loss: 0.035823440119029826\n",
      "iterations:  576\n",
      "Training loss: 0.018837355396396183, Validation loss: 0.03578383170278547\n",
      "new best w\n",
      "iterations:  577\n",
      "Training loss: 0.01882649903848439, Validation loss: 0.03576764715179262\n",
      "new best w\n",
      "iterations:  578\n",
      "Training loss: 0.01881693104273079, Validation loss: 0.035780179855799024\n",
      "iterations:  579\n",
      "Training loss: 0.018805544131591443, Validation loss: 0.035741372397382135\n",
      "new best w\n",
      "iterations:  580\n",
      "Training loss: 0.018794956802409887, Validation loss: 0.03572106313326028\n",
      "new best w\n",
      "iterations:  581\n",
      "Training loss: 0.018784790620497843, Validation loss: 0.03573830166794762\n",
      "iterations:  582\n",
      "Training loss: 0.0187735450900034, Validation loss: 0.03569108391105981\n",
      "new best w\n",
      "iterations:  583\n",
      "Training loss: 0.018763444206425964, Validation loss: 0.0356784221068097\n",
      "new best w\n",
      "iterations:  584\n",
      "Training loss: 0.01875251515424973, Validation loss: 0.035693837671408636\n",
      "iterations:  585\n",
      "Training loss: 0.018741820532742526, Validation loss: 0.03564816721435959\n",
      "new best w\n",
      "iterations:  586\n",
      "Training loss: 0.018731729142765323, Validation loss: 0.03563563131900142\n",
      "new best w\n",
      "iterations:  587\n",
      "Training loss: 0.018720926611262623, Validation loss: 0.03564704648927129\n",
      "iterations:  588\n",
      "Training loss: 0.01870956479683122, Validation loss: 0.035605852724710056\n",
      "new best w\n",
      "iterations:  589\n",
      "Training loss: 0.018699837871244674, Validation loss: 0.03558526056699135\n",
      "new best w\n",
      "iterations:  590\n",
      "Training loss: 0.018689281639634854, Validation loss: 0.03560442837994226\n",
      "iterations:  591\n",
      "Training loss: 0.01867735884505997, Validation loss: 0.03556107523792369\n",
      "new best w\n",
      "iterations:  592\n",
      "Training loss: 0.018668154169853873, Validation loss: 0.035546669761921654\n",
      "new best w\n",
      "iterations:  593\n",
      "Training loss: 0.018657351931079397, Validation loss: 0.03555482651489369\n",
      "iterations:  594\n",
      "Training loss: 0.018645839787016498, Validation loss: 0.03552135561529137\n",
      "new best w\n",
      "iterations:  595\n",
      "Training loss: 0.018636003487594553, Validation loss: 0.03549978368910185\n",
      "new best w\n",
      "iterations:  596\n",
      "Training loss: 0.01862545110453237, Validation loss: 0.03551136281637221\n",
      "iterations:  597\n",
      "Training loss: 0.018614173505084385, Validation loss: 0.035471513000057325\n",
      "new best w\n",
      "iterations:  598\n",
      "Training loss: 0.018604048659476503, Validation loss: 0.03546707582711772\n",
      "new best w\n",
      "iterations:  599\n",
      "Training loss: 0.0185934613327145, Validation loss: 0.03545686732396262\n",
      "new best w\n",
      "iterations:  600\n",
      "Training loss: 0.018582205185458, Validation loss: 0.03542111605205776\n",
      "new best w\n",
      "iterations:  601\n",
      "Training loss: 0.01857253410325115, Validation loss: 0.0354226372701337\n",
      "iterations:  602\n",
      "Training loss: 0.01856114613481631, Validation loss: 0.035412599149194625\n",
      "new best w\n",
      "iterations:  603\n",
      "Training loss: 0.01855040126094537, Validation loss: 0.03537887293896246\n",
      "new best w\n",
      "iterations:  604\n",
      "Training loss: 0.01854078098171825, Validation loss: 0.03538036161763909\n",
      "iterations:  605\n",
      "Training loss: 0.01852931983834168, Validation loss: 0.03536702158351389\n",
      "new best w\n",
      "iterations:  606\n",
      "Training loss: 0.018518358920105928, Validation loss: 0.035335638930113995\n",
      "new best w\n",
      "iterations:  607\n",
      "Training loss: 0.01850881984898515, Validation loss: 0.03533585354147299\n",
      "iterations:  608\n",
      "Training loss: 0.0184977330667388, Validation loss: 0.035322832084345834\n",
      "new best w\n",
      "iterations:  609\n",
      "Training loss: 0.018486058397814368, Validation loss: 0.03529092513142139\n",
      "new best w\n",
      "iterations:  610\n",
      "Training loss: 0.0184770777503268, Validation loss: 0.03528630492486919\n",
      "new best w\n",
      "iterations:  611\n",
      "Training loss: 0.01846588065421355, Validation loss: 0.03528002838105325\n",
      "new best w\n",
      "iterations:  612\n",
      "Training loss: 0.018454480469208424, Validation loss: 0.03524269266948349\n",
      "new best w\n",
      "iterations:  613\n",
      "Training loss: 0.018444857700522246, Validation loss: 0.03524524320378742\n",
      "iterations:  614\n",
      "Training loss: 0.018433849449909622, Validation loss: 0.03522877610709878\n",
      "new best w\n",
      "iterations:  615\n",
      "Training loss: 0.018422841880373186, Validation loss: 0.035200205007672854\n",
      "new best w\n",
      "iterations:  616\n",
      "Training loss: 0.01841271171318609, Validation loss: 0.035205177928818616\n",
      "iterations:  617\n",
      "Training loss: 0.01840200457254254, Validation loss: 0.035186530985821876\n",
      "new best w\n",
      "iterations:  618\n",
      "Training loss: 0.01839087746929317, Validation loss: 0.03515034730569795\n",
      "new best w\n",
      "iterations:  619\n",
      "Training loss: 0.018381196979895723, Validation loss: 0.035155174330565225\n",
      "iterations:  620\n",
      "Training loss: 0.018369688096455596, Validation loss: 0.03514302382121032\n",
      "new best w\n",
      "iterations:  621\n",
      "Training loss: 0.018359079352140928, Validation loss: 0.03510610450287667\n",
      "new best w\n",
      "iterations:  622\n",
      "Training loss: 0.01834954324627119, Validation loss: 0.03512229962482682\n",
      "iterations:  623\n",
      "Training loss: 0.01833769823807362, Validation loss: 0.035095144289329236\n",
      "new best w\n",
      "iterations:  624\n",
      "Training loss: 0.018326143420777184, Validation loss: 0.035063368139734444\n",
      "new best w\n",
      "iterations:  625\n",
      "Training loss: 0.01831546974895942, Validation loss: 0.035057714397996896\n",
      "new best w\n",
      "iterations:  626\n",
      "Training loss: 0.01830304755960524, Validation loss: 0.03504452322936198\n",
      "new best w\n",
      "iterations:  627\n",
      "Training loss: 0.018290308882115504, Validation loss: 0.03501143815391834\n",
      "new best w\n",
      "iterations:  628\n",
      "Training loss: 0.018280172603957298, Validation loss: 0.035007491458566464\n",
      "new best w\n",
      "iterations:  629\n",
      "Training loss: 0.01826765551015217, Validation loss: 0.03499195883755356\n",
      "new best w\n",
      "iterations:  630\n",
      "Training loss: 0.018255294053000307, Validation loss: 0.03496215865861291\n",
      "new best w\n",
      "iterations:  631\n",
      "Training loss: 0.018244340120714738, Validation loss: 0.034967323158804305\n",
      "iterations:  632\n",
      "Training loss: 0.018232270457995036, Validation loss: 0.03494652675457831\n",
      "new best w\n",
      "iterations:  633\n",
      "Training loss: 0.01822009775831005, Validation loss: 0.03491638471681489\n",
      "new best w\n",
      "iterations:  634\n",
      "Training loss: 0.018208876474857853, Validation loss: 0.03492520225958021\n",
      "iterations:  635\n",
      "Training loss: 0.018196824653884433, Validation loss: 0.034901263070141124\n",
      "new best w\n",
      "iterations:  636\n",
      "Training loss: 0.018184576941496843, Validation loss: 0.0348698771798399\n",
      "new best w\n",
      "iterations:  637\n",
      "Training loss: 0.018173806404708365, Validation loss: 0.03486461193951603\n",
      "new best w\n",
      "iterations:  638\n",
      "Training loss: 0.01816095117966178, Validation loss: 0.03485389778602825\n",
      "new best w\n",
      "iterations:  639\n",
      "Training loss: 0.018149303683396078, Validation loss: 0.03481734917072892\n",
      "new best w\n",
      "iterations:  640\n",
      "Training loss: 0.018138377752367894, Validation loss: 0.034819686657637156\n",
      "iterations:  641\n",
      "Training loss: 0.018125955786906154, Validation loss: 0.03480341769026308\n",
      "new best w\n",
      "iterations:  642\n",
      "Training loss: 0.018113410245134246, Validation loss: 0.03477909300821434\n",
      "new best w\n",
      "iterations:  643\n",
      "Training loss: 0.018102855364169935, Validation loss: 0.034765676985138556\n",
      "new best w\n",
      "iterations:  644\n",
      "Training loss: 0.018090862460067386, Validation loss: 0.03477169710453202\n",
      "iterations:  645\n",
      "Training loss: 0.018077921525627524, Validation loss: 0.034728233048621174\n",
      "new best w\n",
      "iterations:  646\n",
      "Training loss: 0.018067346902095435, Validation loss: 0.034721773642036054\n",
      "new best w\n",
      "iterations:  647\n",
      "Training loss: 0.018055256919052007, Validation loss: 0.03470530810509457\n",
      "new best w\n",
      "iterations:  648\n",
      "Training loss: 0.018042833815946405, Validation loss: 0.03467778519801708\n",
      "new best w\n",
      "iterations:  649\n",
      "Training loss: 0.018031384114820307, Validation loss: 0.03467324694882372\n",
      "new best w\n",
      "iterations:  650\n",
      "Training loss: 0.018020025316875028, Validation loss: 0.03466523985556385\n",
      "new best w\n",
      "iterations:  651\n",
      "Training loss: 0.01800739218458005, Validation loss: 0.03462918193094202\n",
      "new best w\n",
      "iterations:  652\n",
      "Training loss: 0.017996368931458832, Validation loss: 0.034620028469481444\n",
      "new best w\n",
      "iterations:  653\n",
      "Training loss: 0.01798424742732868, Validation loss: 0.034633701284063186\n",
      "iterations:  654\n",
      "Training loss: 0.017971990416887948, Validation loss: 0.034574735942669535\n",
      "new best w\n",
      "iterations:  655\n",
      "Training loss: 0.017961030214523985, Validation loss: 0.03456238324243199\n",
      "new best w\n",
      "iterations:  656\n",
      "Training loss: 0.017948810053074676, Validation loss: 0.03457711747805711\n",
      "iterations:  657\n",
      "Training loss: 0.01793638034534347, Validation loss: 0.034529373290882716\n",
      "new best w\n",
      "iterations:  658\n",
      "Training loss: 0.017925271260830444, Validation loss: 0.034514099127885896\n",
      "new best w\n",
      "iterations:  659\n",
      "Training loss: 0.017913750804375773, Validation loss: 0.03452872575525417\n",
      "iterations:  660\n",
      "Training loss: 0.017900691763533916, Validation loss: 0.03448086561839108\n",
      "new best w\n",
      "iterations:  661\n",
      "Training loss: 0.017889807668208826, Validation loss: 0.03448222915399423\n",
      "iterations:  662\n",
      "Training loss: 0.017878239348353274, Validation loss: 0.034476295231012845\n",
      "new best w\n",
      "iterations:  663\n",
      "Training loss: 0.01786562225425959, Validation loss: 0.03443124315177557\n",
      "new best w\n",
      "iterations:  664\n",
      "Training loss: 0.017853731710316297, Validation loss: 0.034416759052827255\n",
      "new best w\n",
      "iterations:  665\n",
      "Training loss: 0.017842919518661807, Validation loss: 0.03443540065543478\n",
      "iterations:  666\n",
      "Training loss: 0.017830298340071937, Validation loss: 0.03438568771214891\n",
      "new best w\n",
      "iterations:  667\n",
      "Training loss: 0.017818295463922777, Validation loss: 0.034378328048070984\n",
      "new best w\n",
      "iterations:  668\n",
      "Training loss: 0.017807178275370046, Validation loss: 0.03438491921957244\n",
      "iterations:  669\n",
      "Training loss: 0.01779472822498453, Validation loss: 0.03433722899659831\n",
      "new best w\n",
      "iterations:  670\n",
      "Training loss: 0.01778306290425263, Validation loss: 0.03431918607613345\n",
      "new best w\n",
      "iterations:  671\n",
      "Training loss: 0.01777151181495494, Validation loss: 0.03433365023611788\n",
      "iterations:  672\n",
      "Training loss: 0.017759284999509267, Validation loss: 0.03428543604475649\n",
      "new best w\n",
      "iterations:  673\n",
      "Training loss: 0.01774720201833697, Validation loss: 0.03426991724216165\n",
      "new best w\n",
      "iterations:  674\n",
      "Training loss: 0.017736487149652855, Validation loss: 0.03428640902972878\n",
      "iterations:  675\n",
      "Training loss: 0.017723379765293954, Validation loss: 0.03424160788178658\n",
      "new best w\n",
      "iterations:  676\n",
      "Training loss: 0.017711810369697146, Validation loss: 0.034231020949791344\n",
      "new best w\n",
      "iterations:  677\n",
      "Training loss: 0.017700955301597827, Validation loss: 0.034220095782712213\n",
      "new best w\n",
      "iterations:  678\n",
      "Training loss: 0.017688280234934194, Validation loss: 0.03420347931332853\n",
      "new best w\n",
      "iterations:  679\n",
      "Training loss: 0.017675797884400067, Validation loss: 0.03417480142288911\n",
      "new best w\n",
      "iterations:  680\n",
      "Training loss: 0.017665312637024356, Validation loss: 0.03417309937031957\n",
      "new best w\n",
      "iterations:  681\n",
      "Training loss: 0.01765309221470558, Validation loss: 0.03416149074607776\n",
      "new best w\n",
      "iterations:  682\n",
      "Training loss: 0.017640319617039536, Validation loss: 0.03413775905093175\n",
      "new best w\n",
      "iterations:  683\n",
      "Training loss: 0.017629560904710035, Validation loss: 0.03412004811307543\n",
      "new best w\n",
      "iterations:  684\n",
      "Training loss: 0.017617405469761024, Validation loss: 0.03410337415233265\n",
      "new best w\n",
      "iterations:  685\n",
      "Training loss: 0.01760509035097406, Validation loss: 0.03407217453197452\n",
      "new best w\n",
      "iterations:  686\n",
      "Training loss: 0.01759365932434495, Validation loss: 0.034077890765939055\n",
      "iterations:  687\n",
      "Training loss: 0.017581961856625937, Validation loss: 0.034057285335955555\n",
      "new best w\n",
      "iterations:  688\n",
      "Training loss: 0.017569402595548036, Validation loss: 0.03403041725502252\n",
      "new best w\n",
      "iterations:  689\n",
      "Training loss: 0.017558516646235308, Validation loss: 0.03401068178038582\n",
      "new best w\n",
      "iterations:  690\n",
      "Training loss: 0.017546003241480194, Validation loss: 0.03402398466066955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  691\n",
      "Training loss: 0.017534048723473902, Validation loss: 0.033970425607349045\n",
      "new best w\n",
      "iterations:  692\n",
      "Training loss: 0.017522799377243978, Validation loss: 0.03396352570342766\n",
      "new best w\n",
      "iterations:  693\n",
      "Training loss: 0.01751093426509584, Validation loss: 0.03397255979132621\n",
      "iterations:  694\n",
      "Training loss: 0.01749813165412055, Validation loss: 0.03392446016059579\n",
      "new best w\n",
      "iterations:  695\n",
      "Training loss: 0.017486976855690613, Validation loss: 0.033921066205137566\n",
      "new best w\n",
      "iterations:  696\n",
      "Training loss: 0.01747565685173795, Validation loss: 0.03392668409446221\n",
      "iterations:  697\n",
      "Training loss: 0.017462610186940685, Validation loss: 0.03387325597176628\n",
      "new best w\n",
      "iterations:  698\n",
      "Training loss: 0.017450999299098914, Validation loss: 0.03386375745548438\n",
      "new best w\n",
      "iterations:  699\n",
      "Training loss: 0.01743997243848751, Validation loss: 0.033872400866335604\n",
      "iterations:  700\n",
      "Training loss: 0.017427546365445305, Validation loss: 0.03382549784825046\n",
      "new best w\n",
      "iterations:  701\n",
      "Training loss: 0.01741495552993341, Validation loss: 0.03382661485875303\n",
      "iterations:  702\n",
      "Training loss: 0.01740447958909256, Validation loss: 0.03382583058181428\n",
      "iterations:  703\n",
      "Training loss: 0.017391818002159738, Validation loss: 0.03377118408688657\n",
      "new best w\n",
      "iterations:  704\n",
      "Training loss: 0.017379715026367436, Validation loss: 0.033768315015998474\n",
      "new best w\n",
      "iterations:  705\n",
      "Training loss: 0.017368302658560823, Validation loss: 0.03375697065574512\n",
      "new best w\n",
      "iterations:  706\n",
      "Training loss: 0.017356498465103284, Validation loss: 0.033747571569544506\n",
      "new best w\n",
      "iterations:  707\n",
      "Training loss: 0.01734405107501132, Validation loss: 0.03371913858634536\n",
      "new best w\n",
      "iterations:  708\n",
      "Training loss: 0.017333152326924343, Validation loss: 0.03370760176974698\n",
      "new best w\n",
      "iterations:  709\n",
      "Training loss: 0.017320423825208084, Validation loss: 0.033701316345343214\n",
      "new best w\n",
      "iterations:  710\n",
      "Training loss: 0.017308470326536957, Validation loss: 0.033660646735801554\n",
      "new best w\n",
      "iterations:  711\n",
      "Training loss: 0.017297691407661263, Validation loss: 0.03367641514091841\n",
      "iterations:  712\n",
      "Training loss: 0.017285117259932002, Validation loss: 0.03364968261907088\n",
      "new best w\n",
      "iterations:  713\n",
      "Training loss: 0.017272454078506125, Validation loss: 0.03360942066157832\n",
      "new best w\n",
      "iterations:  714\n",
      "Training loss: 0.01726181706935906, Validation loss: 0.03360582002211814\n",
      "new best w\n",
      "iterations:  715\n",
      "Training loss: 0.01724987845206863, Validation loss: 0.03360158143841504\n",
      "new best w\n",
      "iterations:  716\n",
      "Training loss: 0.0172368581177628, Validation loss: 0.033562499167291905\n",
      "new best w\n",
      "iterations:  717\n",
      "Training loss: 0.01722589805016588, Validation loss: 0.03355902136510827\n",
      "new best w\n",
      "iterations:  718\n",
      "Training loss: 0.017214212227491543, Validation loss: 0.03355681175729662\n",
      "new best w\n",
      "iterations:  719\n",
      "Training loss: 0.017201659795214035, Validation loss: 0.03350968645053594\n",
      "new best w\n",
      "iterations:  720\n",
      "Training loss: 0.017189682970552407, Validation loss: 0.03350250919510383\n",
      "new best w\n",
      "iterations:  721\n",
      "Training loss: 0.017178684604615996, Validation loss: 0.03351151950301663\n",
      "iterations:  722\n",
      "Training loss: 0.01716600829520527, Validation loss: 0.03346575972637841\n",
      "new best w\n",
      "iterations:  723\n",
      "Training loss: 0.017154344302484477, Validation loss: 0.033446360072353215\n",
      "new best w\n",
      "iterations:  724\n",
      "Training loss: 0.017142578652164377, Validation loss: 0.03346300636740645\n",
      "iterations:  725\n",
      "Training loss: 0.017130566371163017, Validation loss: 0.03340781475449734\n",
      "new best w\n",
      "iterations:  726\n",
      "Training loss: 0.017118390849907697, Validation loss: 0.03340819933523413\n",
      "iterations:  727\n",
      "Training loss: 0.01710757450405485, Validation loss: 0.033417306084687085\n",
      "iterations:  728\n",
      "Training loss: 0.017094407915901687, Validation loss: 0.03336100903124946\n",
      "new best w\n",
      "iterations:  729\n",
      "Training loss: 0.017082674145370905, Validation loss: 0.03334462877728382\n",
      "new best w\n",
      "iterations:  730\n",
      "Training loss: 0.017071894553004906, Validation loss: 0.03334859127419477\n",
      "iterations:  731\n",
      "Training loss: 0.017059256396304484, Validation loss: 0.03332657801947814\n",
      "new best w\n",
      "iterations:  732\n",
      "Training loss: 0.017046815326946477, Validation loss: 0.03330774259397197\n",
      "new best w\n",
      "iterations:  733\n",
      "Training loss: 0.01703634367537974, Validation loss: 0.03328422404575339\n",
      "new best w\n",
      "iterations:  734\n",
      "Training loss: 0.017024285295074326, Validation loss: 0.03327632521353526\n",
      "new best w\n",
      "iterations:  735\n",
      "Training loss: 0.017011368932266638, Validation loss: 0.03324350465965224\n",
      "new best w\n",
      "iterations:  736\n",
      "Training loss: 0.017000609944176345, Validation loss: 0.03324698196710342\n",
      "iterations:  737\n",
      "Training loss: 0.016988521126606575, Validation loss: 0.03322105108575665\n",
      "new best w\n",
      "iterations:  738\n",
      "Training loss: 0.016975028973590676, Validation loss: 0.03318387838247119\n",
      "new best w\n",
      "iterations:  739\n",
      "Training loss: 0.01696240741544173, Validation loss: 0.033163334704684716\n",
      "new best w\n",
      "iterations:  740\n",
      "Training loss: 0.01695009614221607, Validation loss: 0.03318009336085932\n",
      "iterations:  741\n",
      "Training loss: 0.01693651529330012, Validation loss: 0.03311836858093701\n",
      "new best w\n",
      "iterations:  742\n",
      "Training loss: 0.01692398055197811, Validation loss: 0.03311263049785669\n",
      "new best w\n",
      "iterations:  743\n",
      "Training loss: 0.016911243422253294, Validation loss: 0.03311605587037936\n",
      "iterations:  744\n",
      "Training loss: 0.016898116679369104, Validation loss: 0.03306144566609453\n",
      "new best w\n",
      "iterations:  745\n",
      "Training loss: 0.016884991577012332, Validation loss: 0.03304761685844848\n",
      "new best w\n",
      "iterations:  746\n",
      "Training loss: 0.016873252737894074, Validation loss: 0.03303939973202433\n",
      "new best w\n",
      "iterations:  747\n",
      "Training loss: 0.016859188383639322, Validation loss: 0.03303506350964476\n",
      "new best w\n",
      "iterations:  748\n",
      "Training loss: 0.01684646947824581, Validation loss: 0.03298851712800948\n",
      "new best w\n",
      "iterations:  749\n",
      "Training loss: 0.016834504182120312, Validation loss: 0.0329795532041146\n",
      "new best w\n",
      "iterations:  750\n",
      "Training loss: 0.01682123326542682, Validation loss: 0.03296607478389444\n",
      "new best w\n",
      "iterations:  751\n",
      "Training loss: 0.016807236327852026, Validation loss: 0.032935171471797545\n",
      "new best w\n",
      "iterations:  752\n",
      "Training loss: 0.01679586960431849, Validation loss: 0.03291211391598576\n",
      "new best w\n",
      "iterations:  753\n",
      "Training loss: 0.016783181799438992, Validation loss: 0.03293356230366222\n",
      "iterations:  754\n",
      "Training loss: 0.01676922667395122, Validation loss: 0.03287200737039808\n",
      "new best w\n",
      "iterations:  755\n",
      "Training loss: 0.016756444081458974, Validation loss: 0.032852608204115924\n",
      "new best w\n",
      "iterations:  756\n",
      "Training loss: 0.016744618028443313, Validation loss: 0.032867689343422074\n",
      "iterations:  757\n",
      "Training loss: 0.016731067336044465, Validation loss: 0.03281417371235556\n",
      "new best w\n",
      "iterations:  758\n",
      "Training loss: 0.016717639203685897, Validation loss: 0.03280647824646674\n",
      "new best w\n",
      "iterations:  759\n",
      "Training loss: 0.01670576844904522, Validation loss: 0.032798752488680846\n",
      "new best w\n",
      "iterations:  760\n",
      "Training loss: 0.016692817634708826, Validation loss: 0.0327767243067946\n",
      "new best w\n",
      "iterations:  761\n",
      "Training loss: 0.016680142299492037, Validation loss: 0.03273387719217535\n",
      "new best w\n",
      "iterations:  762\n",
      "Training loss: 0.01666836258789054, Validation loss: 0.03273438509787297\n",
      "iterations:  763\n",
      "Training loss: 0.016655777214978768, Validation loss: 0.032719119244001976\n",
      "new best w\n",
      "iterations:  764\n",
      "Training loss: 0.016642912662565942, Validation loss: 0.03268340447899808\n",
      "new best w\n",
      "iterations:  765\n",
      "Training loss: 0.016631475227654874, Validation loss: 0.032675811333123674\n",
      "new best w\n",
      "iterations:  766\n",
      "Training loss: 0.016618906370445392, Validation loss: 0.03267797746906323\n",
      "iterations:  767\n",
      "Training loss: 0.01660592232733442, Validation loss: 0.03262254462239311\n",
      "new best w\n",
      "iterations:  768\n",
      "Training loss: 0.01659375800610481, Validation loss: 0.032603810105868256\n",
      "new best w\n",
      "iterations:  769\n",
      "Training loss: 0.016582565374023518, Validation loss: 0.032625057194591814\n",
      "iterations:  770\n",
      "Training loss: 0.016568956794527218, Validation loss: 0.032563608429944484\n",
      "new best w\n",
      "iterations:  771\n",
      "Training loss: 0.016556615884160554, Validation loss: 0.03256682399865664\n",
      "iterations:  772\n",
      "Training loss: 0.01654529173700563, Validation loss: 0.03253966984449104\n",
      "new best w\n",
      "iterations:  773\n",
      "Training loss: 0.0165326238338921, Validation loss: 0.032529687485043986\n",
      "new best w\n",
      "iterations:  774\n",
      "Training loss: 0.01651901167402071, Validation loss: 0.032490346444239246\n",
      "new best w\n",
      "iterations:  775\n",
      "Training loss: 0.016508147687044852, Validation loss: 0.03249503086750757\n",
      "iterations:  776\n",
      "Training loss: 0.016495727086890063, Validation loss: 0.03246829303076442\n",
      "new best w\n",
      "iterations:  777\n",
      "Training loss: 0.016482784779753575, Validation loss: 0.03245071698282036\n",
      "new best w\n",
      "iterations:  778\n",
      "Training loss: 0.01647021915169188, Validation loss: 0.03241761278918532\n",
      "new best w\n",
      "iterations:  779\n",
      "Training loss: 0.016459167232534978, Validation loss: 0.03243236887404367\n",
      "iterations:  780\n",
      "Training loss: 0.01644599335069101, Validation loss: 0.03237086674909211\n",
      "new best w\n",
      "iterations:  781\n",
      "Training loss: 0.016433436189346375, Validation loss: 0.03237212466464377\n",
      "iterations:  782\n",
      "Training loss: 0.016421533402232406, Validation loss: 0.032355483055113855\n",
      "new best w\n",
      "iterations:  783\n",
      "Training loss: 0.0164093793997041, Validation loss: 0.03234303150463929\n",
      "new best w\n",
      "iterations:  784\n",
      "Training loss: 0.016396311135287465, Validation loss: 0.03230542320146703\n",
      "new best w\n",
      "iterations:  785\n",
      "Training loss: 0.016384976840772406, Validation loss: 0.032299441537214216\n",
      "new best w\n",
      "iterations:  786\n",
      "Training loss: 0.016372049425413947, Validation loss: 0.03228658496808337\n",
      "new best w\n",
      "iterations:  787\n",
      "Training loss: 0.016359844859742134, Validation loss: 0.03224207873565017\n",
      "new best w\n",
      "iterations:  788\n",
      "Training loss: 0.01634803814686285, Validation loss: 0.032234511924961844\n",
      "new best w\n",
      "iterations:  789\n",
      "Training loss: 0.016336396208374376, Validation loss: 0.032243518958947184\n",
      "iterations:  790\n",
      "Training loss: 0.016323037346191026, Validation loss: 0.032198784360363296\n",
      "new best w\n",
      "iterations:  791\n",
      "Training loss: 0.01631115770519665, Validation loss: 0.032165022281630894\n",
      "new best w\n",
      "iterations:  792\n",
      "Training loss: 0.01630006298372807, Validation loss: 0.032170635630954306\n",
      "iterations:  793\n",
      "Training loss: 0.016287108025566873, Validation loss: 0.03215014891247052\n",
      "new best w\n",
      "iterations:  794\n",
      "Training loss: 0.0162741756759321, Validation loss: 0.03212083094049605\n",
      "new best w\n",
      "iterations:  795\n",
      "Training loss: 0.016263419947997418, Validation loss: 0.0321194912711386\n",
      "new best w\n",
      "iterations:  796\n",
      "Training loss: 0.016250976320867846, Validation loss: 0.032095021908008094\n",
      "new best w\n",
      "iterations:  797\n",
      "Training loss: 0.01623750473281681, Validation loss: 0.03205284062093573\n",
      "new best w\n",
      "iterations:  798\n",
      "Training loss: 0.016225412616182877, Validation loss: 0.032040022384084946\n",
      "new best w\n",
      "iterations:  799\n",
      "Training loss: 0.016214000627260246, Validation loss: 0.03205199546255306\n",
      "iterations:  800\n",
      "Training loss: 0.016201044773846235, Validation loss: 0.03199962781971967\n",
      "new best w\n",
      "iterations:  801\n",
      "Training loss: 0.016187869036716918, Validation loss: 0.031989966012368753\n",
      "new best w\n",
      "iterations:  802\n",
      "Training loss: 0.0161766124135643, Validation loss: 0.031976469108622896\n",
      "new best w\n",
      "iterations:  803\n",
      "Training loss: 0.016163986159224115, Validation loss: 0.03195107050778814\n",
      "new best w\n",
      "iterations:  804\n",
      "Training loss: 0.01615115326428047, Validation loss: 0.03192340543832932\n",
      "new best w\n",
      "iterations:  805\n",
      "Training loss: 0.016139084027659713, Validation loss: 0.031900326626690415\n",
      "new best w\n",
      "iterations:  806\n",
      "Training loss: 0.01612721278922273, Validation loss: 0.03192528447907904\n",
      "iterations:  807\n",
      "Training loss: 0.016114121932648506, Validation loss: 0.03185940504298315\n",
      "new best w\n",
      "iterations:  808\n",
      "Training loss: 0.016101840960946297, Validation loss: 0.03184699692271546\n",
      "new best w\n",
      "iterations:  809\n",
      "Training loss: 0.016089947883338705, Validation loss: 0.03184194608823028\n",
      "new best w\n",
      "iterations:  810\n",
      "Training loss: 0.0160772685915502, Validation loss: 0.03182370630527632\n",
      "new best w\n",
      "iterations:  811\n",
      "Training loss: 0.016064494141769532, Validation loss: 0.0317819467699482\n",
      "new best w\n",
      "iterations:  812\n",
      "Training loss: 0.016053310985213887, Validation loss: 0.0317983605510296\n",
      "iterations:  813\n",
      "Training loss: 0.01604023179035416, Validation loss: 0.0317659911147687\n",
      "new best w\n",
      "iterations:  814\n",
      "Training loss: 0.016027499286697614, Validation loss: 0.031726350448825144\n",
      "new best w\n",
      "iterations:  815\n",
      "Training loss: 0.01601539062186602, Validation loss: 0.03170896720241967\n",
      "new best w\n",
      "iterations:  816\n",
      "Training loss: 0.016004747548402688, Validation loss: 0.031729077991330144\n",
      "iterations:  817\n",
      "Training loss: 0.015992167252614233, Validation loss: 0.031677385108630114\n",
      "new best w\n",
      "iterations:  818\n",
      "Training loss: 0.01598074298454681, Validation loss: 0.03166603926269623\n",
      "new best w\n",
      "iterations:  819\n",
      "Training loss: 0.01597050236105177, Validation loss: 0.0316475701286514\n",
      "new best w\n",
      "iterations:  820\n",
      "Training loss: 0.015959147965230215, Validation loss: 0.03164775385046544\n",
      "iterations:  821\n",
      "Training loss: 0.01594659900581675, Validation loss: 0.03160866830725363\n",
      "new best w\n",
      "iterations:  822\n",
      "Training loss: 0.015936147882468502, Validation loss: 0.03158565878579844\n",
      "new best w\n",
      "iterations:  823\n",
      "Training loss: 0.01592540817137206, Validation loss: 0.03160166567932212\n",
      "iterations:  824\n",
      "Training loss: 0.015913551545826427, Validation loss: 0.03155138709026137\n",
      "new best w\n",
      "iterations:  825\n",
      "Training loss: 0.015901519294187275, Validation loss: 0.031542022442755745\n",
      "new best w\n",
      "iterations:  826\n",
      "Training loss: 0.015891472996317994, Validation loss: 0.0315281633448681\n",
      "new best w\n",
      "iterations:  827\n",
      "Training loss: 0.015879819501832385, Validation loss: 0.03150789802062979\n",
      "new best w\n",
      "iterations:  828\n",
      "Training loss: 0.01586811031686946, Validation loss: 0.03148892692793907\n",
      "new best w\n",
      "iterations:  829\n",
      "Training loss: 0.015857288517728257, Validation loss: 0.031471969493062274\n",
      "new best w\n",
      "iterations:  830\n",
      "Training loss: 0.015846090567360985, Validation loss: 0.03146264821251349\n",
      "new best w\n",
      "iterations:  831\n",
      "Training loss: 0.0158342777434614, Validation loss: 0.03142519633488354\n",
      "new best w\n",
      "iterations:  832\n",
      "Training loss: 0.015823246370140664, Validation loss: 0.031404401384789414\n",
      "new best w\n",
      "iterations:  833\n",
      "Training loss: 0.01581257449698241, Validation loss: 0.0314232093813537\n",
      "iterations:  834\n",
      "Training loss: 0.01580055433104578, Validation loss: 0.031369828501012983\n",
      "new best w\n",
      "iterations:  835\n",
      "Training loss: 0.01578904798286658, Validation loss: 0.03135052808138037\n",
      "new best w\n",
      "iterations:  836\n",
      "Training loss: 0.015778911410995012, Validation loss: 0.0313476598389941\n",
      "new best w\n",
      "iterations:  837\n",
      "Training loss: 0.015767144579233186, Validation loss: 0.03133205486241569\n",
      "new best w\n",
      "iterations:  838\n",
      "Training loss: 0.015755030528873934, Validation loss: 0.03130593422001367\n",
      "new best w\n",
      "iterations:  839\n",
      "Training loss: 0.015744449851708295, Validation loss: 0.03126553999444549\n",
      "new best w\n",
      "iterations:  840\n",
      "Training loss: 0.015734133216461114, Validation loss: 0.03130609320746862\n",
      "iterations:  841\n",
      "Training loss: 0.01572176480461432, Validation loss: 0.03123420138863444\n",
      "new best w\n",
      "iterations:  842\n",
      "Training loss: 0.01570986631517296, Validation loss: 0.03123244950610689\n",
      "new best w\n",
      "iterations:  843\n",
      "Training loss: 0.015700055111524603, Validation loss: 0.031210746519255476\n",
      "new best w\n",
      "iterations:  844\n",
      "Training loss: 0.01568864614715873, Validation loss: 0.031218987793621207\n",
      "iterations:  845\n",
      "Training loss: 0.01567643769766187, Validation loss: 0.031158112800593073\n",
      "new best w\n",
      "iterations:  846\n",
      "Training loss: 0.01566523583816029, Validation loss: 0.031155894302181797\n",
      "new best w\n",
      "iterations:  847\n",
      "Training loss: 0.015655257566779004, Validation loss: 0.031168498390814436\n",
      "iterations:  848\n",
      "Training loss: 0.015643126181064157, Validation loss: 0.031110867261537882\n",
      "new best w\n",
      "iterations:  849\n",
      "Training loss: 0.015631552742537122, Validation loss: 0.03110166643671434\n",
      "new best w\n",
      "iterations:  850\n",
      "Training loss: 0.015620758211627879, Validation loss: 0.03109551166019094\n",
      "new best w\n",
      "iterations:  851\n",
      "Training loss: 0.015609738226236305, Validation loss: 0.031067424899970227\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  852\n",
      "Training loss: 0.015597750673028675, Validation loss: 0.03104211337801068\n",
      "new best w\n",
      "iterations:  853\n",
      "Training loss: 0.015587206068571129, Validation loss: 0.03101514180639136\n",
      "new best w\n",
      "iterations:  854\n",
      "Training loss: 0.015575982631045381, Validation loss: 0.031043969844613124\n",
      "iterations:  855\n",
      "Training loss: 0.015564362107391864, Validation loss: 0.030977181133206848\n",
      "new best w\n",
      "iterations:  856\n",
      "Training loss: 0.015552744891501616, Validation loss: 0.030970092064095435\n",
      "new best w\n",
      "iterations:  857\n",
      "Training loss: 0.015542729995362025, Validation loss: 0.030964103543819947\n",
      "new best w\n",
      "iterations:  858\n",
      "Training loss: 0.015530751445058182, Validation loss: 0.030957624084824893\n",
      "new best w\n",
      "iterations:  859\n",
      "Training loss: 0.015518990707919424, Validation loss: 0.030898677610892072\n",
      "new best w\n",
      "iterations:  860\n",
      "Training loss: 0.015508393745142924, Validation loss: 0.03089897689880251\n",
      "iterations:  861\n",
      "Training loss: 0.015497966933644448, Validation loss: 0.030909655969047482\n",
      "iterations:  862\n",
      "Training loss: 0.015485684345565425, Validation loss: 0.03084360688354263\n",
      "new best w\n",
      "iterations:  863\n",
      "Training loss: 0.015474010194238504, Validation loss: 0.03085658638907336\n",
      "iterations:  864\n",
      "Training loss: 0.015464054241496472, Validation loss: 0.030824557784742884\n",
      "new best w\n",
      "iterations:  865\n",
      "Training loss: 0.015452705295666593, Validation loss: 0.030831525089587593\n",
      "iterations:  866\n",
      "Training loss: 0.01544058314216539, Validation loss: 0.030770398284743976\n",
      "new best w\n",
      "iterations:  867\n",
      "Training loss: 0.015429438532122199, Validation loss: 0.030771807153019293\n",
      "iterations:  868\n",
      "Training loss: 0.015419490901182869, Validation loss: 0.03077988423516751\n",
      "iterations:  869\n",
      "Training loss: 0.015407509981091427, Validation loss: 0.030718762223214992\n",
      "new best w\n",
      "iterations:  870\n",
      "Training loss: 0.015395821384072758, Validation loss: 0.030716570602332433\n",
      "new best w\n",
      "iterations:  871\n",
      "Training loss: 0.015384991360855478, Validation loss: 0.03070331582114283\n",
      "new best w\n",
      "iterations:  872\n",
      "Training loss: 0.015374193830686687, Validation loss: 0.030690838054559445\n",
      "new best w\n",
      "iterations:  873\n",
      "Training loss: 0.015362215002652493, Validation loss: 0.03064845304564452\n",
      "new best w\n",
      "iterations:  874\n",
      "Training loss: 0.015351442821518464, Validation loss: 0.030635109564387412\n",
      "new best w\n",
      "iterations:  875\n",
      "Training loss: 0.015340658311130398, Validation loss: 0.030655376784782185\n",
      "iterations:  876\n",
      "Training loss: 0.015329035771992496, Validation loss: 0.0305855531285527\n",
      "new best w\n",
      "iterations:  877\n",
      "Training loss: 0.015317271966116975, Validation loss: 0.030584443352404074\n",
      "new best w\n",
      "iterations:  878\n",
      "Training loss: 0.015307313332960164, Validation loss: 0.030571457713396706\n",
      "new best w\n",
      "iterations:  879\n",
      "Training loss: 0.015295719991228479, Validation loss: 0.03057447224844531\n",
      "iterations:  880\n",
      "Training loss: 0.015283884732071184, Validation loss: 0.030512482067763608\n",
      "new best w\n",
      "iterations:  881\n",
      "Training loss: 0.015272965841351406, Validation loss: 0.0305086858844832\n",
      "new best w\n",
      "iterations:  882\n",
      "Training loss: 0.015262911226737832, Validation loss: 0.030495556903056463\n",
      "new best w\n",
      "iterations:  883\n",
      "Training loss: 0.015250994355411447, Validation loss: 0.030487921396919814\n",
      "new best w\n",
      "iterations:  884\n",
      "Training loss: 0.01523882592438818, Validation loss: 0.030446576459997987\n",
      "new best w\n",
      "iterations:  885\n",
      "Training loss: 0.01522891551912877, Validation loss: 0.030433180445751627\n",
      "new best w\n",
      "iterations:  886\n",
      "Training loss: 0.015218190787514107, Validation loss: 0.030449025540907483\n",
      "iterations:  887\n",
      "Training loss: 0.015206259004247458, Validation loss: 0.030384743270641567\n",
      "new best w\n",
      "iterations:  888\n",
      "Training loss: 0.015194266392502662, Validation loss: 0.030386936225997623\n",
      "iterations:  889\n",
      "Training loss: 0.015184673162159469, Validation loss: 0.030364594830011103\n",
      "new best w\n",
      "iterations:  890\n",
      "Training loss: 0.015173164001272135, Validation loss: 0.030352230988283335\n",
      "new best w\n",
      "iterations:  891\n",
      "Training loss: 0.015161489736232136, Validation loss: 0.030324231678292735\n",
      "new best w\n",
      "iterations:  892\n",
      "Training loss: 0.01515013232067737, Validation loss: 0.03029322774237017\n",
      "new best w\n",
      "iterations:  893\n",
      "Training loss: 0.015140265419174658, Validation loss: 0.030331316195632202\n",
      "iterations:  894\n",
      "Training loss: 0.015128392814623848, Validation loss: 0.030244366286945063\n",
      "new best w\n",
      "iterations:  895\n",
      "Training loss: 0.015116826320181728, Validation loss: 0.03025543501285542\n",
      "iterations:  896\n",
      "Training loss: 0.015106337118179259, Validation loss: 0.03023474336017769\n",
      "new best w\n",
      "iterations:  897\n",
      "Training loss: 0.01509510700845232, Validation loss: 0.030234690337675008\n",
      "new best w\n",
      "iterations:  898\n",
      "Training loss: 0.01508366645167972, Validation loss: 0.03017843132092362\n",
      "new best w\n",
      "iterations:  899\n",
      "Training loss: 0.015072434637807124, Validation loss: 0.03017504617657756\n",
      "new best w\n",
      "iterations:  900\n",
      "Training loss: 0.015062126487177913, Validation loss: 0.030164353994032257\n",
      "new best w\n",
      "iterations:  901\n",
      "Training loss: 0.015050093938276133, Validation loss: 0.03015617085431086\n",
      "new best w\n",
      "iterations:  902\n",
      "Training loss: 0.015039003447042676, Validation loss: 0.03011103630604155\n",
      "new best w\n",
      "iterations:  903\n",
      "Training loss: 0.015028206403882215, Validation loss: 0.03009610698254873\n",
      "new best w\n",
      "iterations:  904\n",
      "Training loss: 0.01501806609218567, Validation loss: 0.0301169280588841\n",
      "iterations:  905\n",
      "Training loss: 0.015005599290237622, Validation loss: 0.030047966716368994\n",
      "new best w\n",
      "iterations:  906\n",
      "Training loss: 0.014994192628869996, Validation loss: 0.03004604871916306\n",
      "new best w\n",
      "iterations:  907\n",
      "Training loss: 0.014984098278337995, Validation loss: 0.030033753681541965\n",
      "new best w\n",
      "iterations:  908\n",
      "Training loss: 0.014973226885910192, Validation loss: 0.030021462201068338\n",
      "new best w\n",
      "iterations:  909\n",
      "Training loss: 0.014961031181977362, Validation loss: 0.029984799387236638\n",
      "new best w\n",
      "iterations:  910\n",
      "Training loss: 0.0149497259678444, Validation loss: 0.029962975951964493\n",
      "new best w\n",
      "iterations:  911\n",
      "Training loss: 0.014939929146357613, Validation loss: 0.02995597520133623\n",
      "new best w\n",
      "iterations:  912\n",
      "Training loss: 0.014928547799170761, Validation loss: 0.02994824579132374\n",
      "new best w\n",
      "iterations:  913\n",
      "Training loss: 0.01491668075333537, Validation loss: 0.02990098734681403\n",
      "new best w\n",
      "iterations:  914\n",
      "Training loss: 0.014905334508275114, Validation loss: 0.029895356266594765\n",
      "new best w\n",
      "iterations:  915\n",
      "Training loss: 0.014895915708024829, Validation loss: 0.02990760176821484\n",
      "iterations:  916\n",
      "Training loss: 0.014884217804359808, Validation loss: 0.029843664725214276\n",
      "new best w\n",
      "iterations:  917\n",
      "Training loss: 0.01487404339534586, Validation loss: 0.029863327885555985\n",
      "iterations:  918\n",
      "Training loss: 0.01486363636807934, Validation loss: 0.02982945710016773\n",
      "new best w\n",
      "iterations:  919\n",
      "Training loss: 0.014852876391415025, Validation loss: 0.029827637329524315\n",
      "new best w\n",
      "iterations:  920\n",
      "Training loss: 0.014841998950808136, Validation loss: 0.029772070448519435\n",
      "new best w\n",
      "iterations:  921\n",
      "Training loss: 0.01483177657065041, Validation loss: 0.02979407998552023\n",
      "iterations:  922\n",
      "Training loss: 0.01482154672198378, Validation loss: 0.029759023682756836\n",
      "new best w\n",
      "iterations:  923\n",
      "Training loss: 0.014809859965407046, Validation loss: 0.029757569970637075\n",
      "new best w\n",
      "iterations:  924\n",
      "Training loss: 0.014799802811358403, Validation loss: 0.029702088425732194\n",
      "new best w\n",
      "iterations:  925\n",
      "Training loss: 0.014789691715235814, Validation loss: 0.02972401058078571\n",
      "iterations:  926\n",
      "Training loss: 0.014779345209811688, Validation loss: 0.029711135378237274\n",
      "iterations:  927\n",
      "Training loss: 0.014767403285035105, Validation loss: 0.029662447695971032\n",
      "new best w\n",
      "iterations:  928\n",
      "Training loss: 0.014757510407874914, Validation loss: 0.02966803350498089\n",
      "iterations:  929\n",
      "Training loss: 0.01474756476552603, Validation loss: 0.029638395106780225\n",
      "new best w\n",
      "iterations:  930\n",
      "Training loss: 0.014736576560907502, Validation loss: 0.029633031381193158\n",
      "new best w\n",
      "iterations:  931\n",
      "Training loss: 0.014725327771034065, Validation loss: 0.029590218054739232\n",
      "new best w\n",
      "iterations:  932\n",
      "Training loss: 0.01471516322412494, Validation loss: 0.029597729285374595\n",
      "iterations:  933\n",
      "Training loss: 0.014705426564019797, Validation loss: 0.02959345113589402\n",
      "iterations:  934\n",
      "Training loss: 0.014694121029096832, Validation loss: 0.02953829922828737\n",
      "new best w\n",
      "iterations:  935\n",
      "Training loss: 0.014683424524111595, Validation loss: 0.029544986706843596\n",
      "iterations:  936\n",
      "Training loss: 0.014673250429496163, Validation loss: 0.029527308909090243\n",
      "new best w\n",
      "iterations:  937\n",
      "Training loss: 0.014662660021578183, Validation loss: 0.02950516237210469\n",
      "new best w\n",
      "iterations:  938\n",
      "Training loss: 0.014651917015169394, Validation loss: 0.029475009845333838\n",
      "new best w\n",
      "iterations:  939\n",
      "Training loss: 0.014641489117595975, Validation loss: 0.029469712891890643\n",
      "new best w\n",
      "iterations:  940\n",
      "Training loss: 0.014631037890264764, Validation loss: 0.029456613708896913\n",
      "new best w\n",
      "iterations:  941\n",
      "Training loss: 0.014620025317088354, Validation loss: 0.02943471128010163\n",
      "new best w\n",
      "iterations:  942\n",
      "Training loss: 0.014609687246435497, Validation loss: 0.029411304494568767\n",
      "new best w\n",
      "iterations:  943\n",
      "Training loss: 0.014600015069648984, Validation loss: 0.029403021214033244\n",
      "new best w\n",
      "iterations:  944\n",
      "Training loss: 0.014588458019002635, Validation loss: 0.029386200578192856\n",
      "new best w\n",
      "iterations:  945\n",
      "Training loss: 0.014577812952149178, Validation loss: 0.029342886839457148\n",
      "new best w\n",
      "iterations:  946\n",
      "Training loss: 0.014567582974629614, Validation loss: 0.029344945981234827\n",
      "iterations:  947\n",
      "Training loss: 0.014557985707567674, Validation loss: 0.029330973966213703\n",
      "new best w\n",
      "iterations:  948\n",
      "Training loss: 0.01454607920664057, Validation loss: 0.02931068878187093\n",
      "new best w\n",
      "iterations:  949\n",
      "Training loss: 0.014535761395105335, Validation loss: 0.029297475689987452\n",
      "new best w\n",
      "iterations:  950\n",
      "Training loss: 0.014526328483955046, Validation loss: 0.02927208225539592\n",
      "new best w\n",
      "iterations:  951\n",
      "Training loss: 0.014515575202849264, Validation loss: 0.02926482047548836\n",
      "new best w\n",
      "iterations:  952\n",
      "Training loss: 0.014504492674095086, Validation loss: 0.029217171463833652\n",
      "new best w\n",
      "iterations:  953\n",
      "Training loss: 0.014493826156151721, Validation loss: 0.029229121420700686\n",
      "iterations:  954\n",
      "Training loss: 0.014484606560328551, Validation loss: 0.029201664558755242\n",
      "new best w\n",
      "iterations:  955\n",
      "Training loss: 0.01447315722528177, Validation loss: 0.02919295653567347\n",
      "new best w\n",
      "iterations:  956\n",
      "Training loss: 0.014462938019784814, Validation loss: 0.029142634739559786\n",
      "new best w\n",
      "iterations:  957\n",
      "Training loss: 0.014452637284865904, Validation loss: 0.029183682258576166\n",
      "iterations:  958\n",
      "Training loss: 0.014442219371878822, Validation loss: 0.029129211035947865\n",
      "new best w\n",
      "iterations:  959\n",
      "Training loss: 0.014431335170158828, Validation loss: 0.029090186197856277\n",
      "new best w\n",
      "iterations:  960\n",
      "Training loss: 0.014421251165745684, Validation loss: 0.02910240694424666\n",
      "iterations:  961\n",
      "Training loss: 0.01441089991790749, Validation loss: 0.029081849742264975\n",
      "new best w\n",
      "iterations:  962\n",
      "Training loss: 0.0143997945238123, Validation loss: 0.029064997358868493\n",
      "new best w\n",
      "iterations:  963\n",
      "Training loss: 0.014389600304977433, Validation loss: 0.029015655087233354\n",
      "new best w\n",
      "iterations:  964\n",
      "Training loss: 0.014380143108575473, Validation loss: 0.029057356652248546\n",
      "iterations:  965\n",
      "Training loss: 0.014368831253587535, Validation loss: 0.02900633007478239\n",
      "new best w\n",
      "iterations:  966\n",
      "Training loss: 0.014358005032675708, Validation loss: 0.028971987829490654\n",
      "new best w\n",
      "iterations:  967\n",
      "Training loss: 0.01434797837181212, Validation loss: 0.028972303618136554\n",
      "iterations:  968\n",
      "Training loss: 0.014338407446672075, Validation loss: 0.028958888774802894\n",
      "new best w\n",
      "iterations:  969\n",
      "Training loss: 0.014326702986755577, Validation loss: 0.028938125504591183\n",
      "new best w\n",
      "iterations:  970\n",
      "Training loss: 0.014316178177561156, Validation loss: 0.028898563672541337\n",
      "new best w\n",
      "iterations:  971\n",
      "Training loss: 0.01430669758638163, Validation loss: 0.028899923515032195\n",
      "iterations:  972\n",
      "Training loss: 0.014296415522596253, Validation loss: 0.028913492657770053\n",
      "iterations:  973\n",
      "Training loss: 0.014285202161171686, Validation loss: 0.02883865210546403\n",
      "new best w\n",
      "iterations:  974\n",
      "Training loss: 0.014274498299658185, Validation loss: 0.028854287575324922\n",
      "iterations:  975\n",
      "Training loss: 0.01426532431238496, Validation loss: 0.028828248748640244\n",
      "new best w\n",
      "iterations:  976\n",
      "Training loss: 0.014254051643351306, Validation loss: 0.02882227957543524\n",
      "new best w\n",
      "iterations:  977\n",
      "Training loss: 0.014243699059704488, Validation loss: 0.02876723891224065\n",
      "new best w\n",
      "iterations:  978\n",
      "Training loss: 0.01423315865791325, Validation loss: 0.02878341034402331\n",
      "iterations:  979\n",
      "Training loss: 0.014223329210275531, Validation loss: 0.028784495585086584\n",
      "iterations:  980\n",
      "Training loss: 0.014212280107132663, Validation loss: 0.028709904001031405\n",
      "new best w\n",
      "iterations:  981\n",
      "Training loss: 0.014202216722801402, Validation loss: 0.02873351207412575\n",
      "iterations:  982\n",
      "Training loss: 0.014191908622186661, Validation loss: 0.02870396453687544\n",
      "new best w\n",
      "iterations:  983\n",
      "Training loss: 0.014180902767705582, Validation loss: 0.028690936644218325\n",
      "new best w\n",
      "iterations:  984\n",
      "Training loss: 0.014170668619426216, Validation loss: 0.028640054855239905\n",
      "new best w\n",
      "iterations:  985\n",
      "Training loss: 0.014160980477796395, Validation loss: 0.028655066827140023\n",
      "iterations:  986\n",
      "Training loss: 0.01415018840742738, Validation loss: 0.02866215805621608\n",
      "iterations:  987\n",
      "Training loss: 0.014139192516646198, Validation loss: 0.028590855237774777\n",
      "new best w\n",
      "iterations:  988\n",
      "Training loss: 0.014129136663370718, Validation loss: 0.02859898515081148\n",
      "iterations:  989\n",
      "Training loss: 0.014119688601652218, Validation loss: 0.028583471018014055\n",
      "new best w\n",
      "iterations:  990\n",
      "Training loss: 0.014108045583143363, Validation loss: 0.028560600011854332\n",
      "new best w\n",
      "iterations:  991\n",
      "Training loss: 0.014097573606252696, Validation loss: 0.028523332788081\n",
      "new best w\n",
      "iterations:  992\n",
      "Training loss: 0.014088023413905295, Validation loss: 0.02852428120990212\n",
      "iterations:  993\n",
      "Training loss: 0.01407791410465923, Validation loss: 0.028538840245295394\n",
      "iterations:  994\n",
      "Training loss: 0.014066632045964702, Validation loss: 0.028463025317914975\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  995\n",
      "Training loss: 0.01405597854123019, Validation loss: 0.0284823850051587\n",
      "iterations:  996\n",
      "Training loss: 0.01404679301236045, Validation loss: 0.028452704655585692\n",
      "new best w\n",
      "iterations:  997\n",
      "Training loss: 0.014035670161780185, Validation loss: 0.028443421028653937\n",
      "new best w\n",
      "iterations:  998\n",
      "Training loss: 0.014025316883886592, Validation loss: 0.028389452412645753\n",
      "new best w\n",
      "iterations:  999\n",
      "Training loss: 0.01401476010660178, Validation loss: 0.028408224736779418\n",
      "iterations:  1000\n",
      "Training loss: 0.014005241262632872, Validation loss: 0.028407217470750794\n",
      "iterations:  1001\n",
      "Training loss: 0.013994105333040718, Validation loss: 0.028343550174640315\n",
      "new best w\n",
      "iterations:  1002\n",
      "Training loss: 0.013983903546592633, Validation loss: 0.028350629615151147\n",
      "iterations:  1003\n",
      "Training loss: 0.01397358445484942, Validation loss: 0.02833618419390881\n",
      "new best w\n",
      "iterations:  1004\n",
      "Training loss: 0.013962794315511031, Validation loss: 0.028313970257044983\n",
      "new best w\n",
      "iterations:  1005\n",
      "Training loss: 0.013952616575654159, Validation loss: 0.028264406961069084\n",
      "new best w\n",
      "iterations:  1006\n",
      "Training loss: 0.013942891641708359, Validation loss: 0.028276694391284284\n",
      "iterations:  1007\n",
      "Training loss: 0.013932108871598473, Validation loss: 0.028287370671582213\n",
      "iterations:  1008\n",
      "Training loss: 0.013921371328162443, Validation loss: 0.028213990653674074\n",
      "new best w\n",
      "iterations:  1009\n",
      "Training loss: 0.013911160720362586, Validation loss: 0.028222986414517183\n",
      "iterations:  1010\n",
      "Training loss: 0.01390174341954819, Validation loss: 0.028207624015510756\n",
      "new best w\n",
      "iterations:  1011\n",
      "Training loss: 0.013889985466330752, Validation loss: 0.02819118679334104\n",
      "new best w\n",
      "iterations:  1012\n",
      "Training loss: 0.01387987001379032, Validation loss: 0.028146206740174113\n",
      "new best w\n",
      "iterations:  1013\n",
      "Training loss: 0.013870243774723545, Validation loss: 0.02814803402345059\n",
      "iterations:  1014\n",
      "Training loss: 0.013860237788818492, Validation loss: 0.028161693939891524\n",
      "iterations:  1015\n",
      "Training loss: 0.013848878977668426, Validation loss: 0.02808634536291778\n",
      "new best w\n",
      "iterations:  1016\n",
      "Training loss: 0.013838473965190832, Validation loss: 0.028105844302732812\n",
      "iterations:  1017\n",
      "Training loss: 0.013829150556536997, Validation loss: 0.028076688712582388\n",
      "new best w\n",
      "iterations:  1018\n",
      "Training loss: 0.013818055673596836, Validation loss: 0.028067537945360516\n",
      "new best w\n",
      "iterations:  1019\n",
      "Training loss: 0.01380759023841708, Validation loss: 0.028018165523526928\n",
      "new best w\n",
      "iterations:  1020\n",
      "Training loss: 0.01379745897030574, Validation loss: 0.028031709955268387\n",
      "iterations:  1021\n",
      "Training loss: 0.013787669342448825, Validation loss: 0.02802871172423236\n",
      "iterations:  1022\n",
      "Training loss: 0.013776832416498241, Validation loss: 0.027966739033175807\n",
      "new best w\n",
      "iterations:  1023\n",
      "Training loss: 0.013766398425896897, Validation loss: 0.027977048177325485\n",
      "iterations:  1024\n",
      "Training loss: 0.013756304662432646, Validation loss: 0.0279597594340295\n",
      "new best w\n",
      "iterations:  1025\n",
      "Training loss: 0.013745692692592208, Validation loss: 0.027935573949885367\n",
      "new best w\n",
      "iterations:  1026\n",
      "Training loss: 0.013735454885125535, Validation loss: 0.02792519660039535\n",
      "new best w\n",
      "iterations:  1027\n",
      "Training loss: 0.013725842175914554, Validation loss: 0.027903191706488705\n",
      "new best w\n",
      "iterations:  1028\n",
      "Training loss: 0.013714558827582274, Validation loss: 0.027895537656298597\n",
      "new best w\n",
      "iterations:  1029\n",
      "Training loss: 0.013704478237959984, Validation loss: 0.027840334159613577\n",
      "new best w\n",
      "iterations:  1030\n",
      "Training loss: 0.013694193600235136, Validation loss: 0.02784545470717926\n",
      "iterations:  1031\n",
      "Training loss: 0.013684762995737447, Validation loss: 0.027831567863679824\n",
      "new best w\n",
      "iterations:  1032\n",
      "Training loss: 0.013672922909108036, Validation loss: 0.027814610245247333\n",
      "new best w\n",
      "iterations:  1033\n",
      "Training loss: 0.0136630914308916, Validation loss: 0.027795516578194422\n",
      "new best w\n",
      "iterations:  1034\n",
      "Training loss: 0.013653533014306158, Validation loss: 0.027779949181905093\n",
      "new best w\n",
      "iterations:  1035\n",
      "Training loss: 0.013643055747370172, Validation loss: 0.027762301303711424\n",
      "new best w\n",
      "iterations:  1036\n",
      "Training loss: 0.01363187910651651, Validation loss: 0.027719031644376484\n",
      "new best w\n",
      "iterations:  1037\n",
      "Training loss: 0.013621987703707848, Validation loss: 0.027727091174449027\n",
      "iterations:  1038\n",
      "Training loss: 0.013612462786978294, Validation loss: 0.027698430301667944\n",
      "new best w\n",
      "iterations:  1039\n",
      "Training loss: 0.01360156376317466, Validation loss: 0.027691433904474066\n",
      "new best w\n",
      "iterations:  1040\n",
      "Training loss: 0.013590826789768962, Validation loss: 0.027667273320047436\n",
      "new best w\n",
      "iterations:  1041\n",
      "Training loss: 0.013581304480358375, Validation loss: 0.02765874062923344\n",
      "new best w\n",
      "iterations:  1042\n",
      "Training loss: 0.01357075629109125, Validation loss: 0.027632020346727355\n",
      "new best w\n",
      "iterations:  1043\n",
      "Training loss: 0.01356044915492754, Validation loss: 0.027595107329957493\n",
      "new best w\n",
      "iterations:  1044\n",
      "Training loss: 0.013549870953774499, Validation loss: 0.027599778403720937\n",
      "iterations:  1045\n",
      "Training loss: 0.013540217738698082, Validation loss: 0.027580809754898065\n",
      "new best w\n",
      "iterations:  1046\n",
      "Training loss: 0.013529354164657574, Validation loss: 0.0275593948183128\n",
      "new best w\n",
      "iterations:  1047\n",
      "Training loss: 0.013519333904802096, Validation loss: 0.02754828606464222\n",
      "new best w\n",
      "iterations:  1048\n",
      "Training loss: 0.01350948407974547, Validation loss: 0.027529588058501177\n",
      "new best w\n",
      "iterations:  1049\n",
      "Training loss: 0.013498415200729839, Validation loss: 0.027515336575861842\n",
      "new best w\n",
      "iterations:  1050\n",
      "Training loss: 0.013488397911517527, Validation loss: 0.02746274244924083\n",
      "new best w\n",
      "iterations:  1051\n",
      "Training loss: 0.01347834991513206, Validation loss: 0.027480381394708698\n",
      "iterations:  1052\n",
      "Training loss: 0.013468580102685564, Validation loss: 0.027455241639590955\n",
      "new best w\n",
      "iterations:  1053\n",
      "Training loss: 0.013457046545225705, Validation loss: 0.027439045659852963\n",
      "new best w\n",
      "iterations:  1054\n",
      "Training loss: 0.013447396396643795, Validation loss: 0.027416276391524986\n",
      "new best w\n",
      "iterations:  1055\n",
      "Training loss: 0.01343793483742715, Validation loss: 0.027410247265201092\n",
      "new best w\n",
      "iterations:  1056\n",
      "Training loss: 0.013426932054680842, Validation loss: 0.027386183302063577\n",
      "new best w\n",
      "iterations:  1057\n",
      "Training loss: 0.01341604549465681, Validation loss: 0.02734590403178531\n",
      "new best w\n",
      "iterations:  1058\n",
      "Training loss: 0.013406586692177274, Validation loss: 0.027348100318918777\n",
      "iterations:  1059\n",
      "Training loss: 0.013397053521696529, Validation loss: 0.027365868425705937\n",
      "iterations:  1060\n",
      "Training loss: 0.013385964900745917, Validation loss: 0.02728109736078586\n",
      "new best w\n",
      "iterations:  1061\n",
      "Training loss: 0.013375090655980873, Validation loss: 0.02730654166969552\n",
      "iterations:  1062\n",
      "Training loss: 0.01336616694762069, Validation loss: 0.027275258357367965\n",
      "new best w\n",
      "iterations:  1063\n",
      "Training loss: 0.013355184118812074, Validation loss: 0.02725751783618327\n",
      "new best w\n",
      "iterations:  1064\n",
      "Training loss: 0.013345080007191179, Validation loss: 0.027217035474656648\n",
      "new best w\n",
      "iterations:  1065\n",
      "Training loss: 0.013334435410324698, Validation loss: 0.02722729202024164\n",
      "iterations:  1066\n",
      "Training loss: 0.013325189739215891, Validation loss: 0.027236261175899723\n",
      "iterations:  1067\n",
      "Training loss: 0.013314227375042582, Validation loss: 0.027153851513772316\n",
      "new best w\n",
      "iterations:  1068\n",
      "Training loss: 0.01330419142802589, Validation loss: 0.027177997696230704\n",
      "iterations:  1069\n",
      "Training loss: 0.013294120360271338, Validation loss: 0.027152742299993975\n",
      "new best w\n",
      "iterations:  1070\n",
      "Training loss: 0.013283486549855944, Validation loss: 0.02713767868402283\n",
      "new best w\n",
      "iterations:  1071\n",
      "Training loss: 0.013273385576158227, Validation loss: 0.027086378528088227\n",
      "new best w\n",
      "iterations:  1072\n",
      "Training loss: 0.01326369396622111, Validation loss: 0.027102557601096266\n",
      "iterations:  1073\n",
      "Training loss: 0.013253261063955322, Validation loss: 0.027112495179778907\n",
      "iterations:  1074\n",
      "Training loss: 0.013242620179677946, Validation loss: 0.027035160188287066\n",
      "new best w\n",
      "iterations:  1075\n",
      "Training loss: 0.013232483259962806, Validation loss: 0.027046785827071557\n",
      "iterations:  1076\n",
      "Training loss: 0.013223300174270772, Validation loss: 0.027031683775642322\n",
      "new best w\n",
      "iterations:  1077\n",
      "Training loss: 0.013211742554099588, Validation loss: 0.027011074988717652\n",
      "new best w\n",
      "iterations:  1078\n",
      "Training loss: 0.013201719840039405, Validation loss: 0.026994185387247238\n",
      "new best w\n",
      "iterations:  1079\n",
      "Training loss: 0.013192430202956067, Validation loss: 0.026944028738483145\n",
      "new best w\n",
      "iterations:  1080\n",
      "Training loss: 0.013182548998805199, Validation loss: 0.026996180729926884\n",
      "iterations:  1081\n",
      "Training loss: 0.013171228453909622, Validation loss: 0.026904449366323423\n",
      "new best w\n",
      "iterations:  1082\n",
      "Training loss: 0.013161014670356146, Validation loss: 0.026933879123991068\n",
      "iterations:  1083\n",
      "Training loss: 0.013151765367646196, Validation loss: 0.026898980251609927\n",
      "new best w\n",
      "iterations:  1084\n",
      "Training loss: 0.01314097042313497, Validation loss: 0.026891236464206537\n",
      "new best w\n",
      "iterations:  1085\n",
      "Training loss: 0.013130426078536182, Validation loss: 0.02686281065753377\n",
      "new best w\n",
      "iterations:  1086\n",
      "Training loss: 0.013120887928897172, Validation loss: 0.02685867056230276\n",
      "new best w\n",
      "iterations:  1087\n",
      "Training loss: 0.013110636129657223, Validation loss: 0.0268307789670576\n",
      "new best w\n",
      "iterations:  1088\n",
      "Training loss: 0.013100345740434142, Validation loss: 0.026794680309733275\n",
      "new best w\n",
      "iterations:  1089\n",
      "Training loss: 0.013089898172781526, Validation loss: 0.02679840506966007\n",
      "iterations:  1090\n",
      "Training loss: 0.013080212925258348, Validation loss: 0.026784138461218847\n",
      "new best w\n",
      "iterations:  1091\n",
      "Training loss: 0.013069592457557112, Validation loss: 0.02675797934168963\n",
      "new best w\n",
      "iterations:  1092\n",
      "Training loss: 0.013059600224492778, Validation loss: 0.02674712985152966\n",
      "new best w\n",
      "iterations:  1093\n",
      "Training loss: 0.01304998810792253, Validation loss: 0.026726427741662513\n",
      "new best w\n",
      "iterations:  1094\n",
      "Training loss: 0.013038986344474222, Validation loss: 0.026717603568533782\n",
      "new best w\n",
      "iterations:  1095\n",
      "Training loss: 0.013029150183919203, Validation loss: 0.02665971292284113\n",
      "new best w\n",
      "iterations:  1096\n",
      "Training loss: 0.013019132009044343, Validation loss: 0.026679784192505652\n",
      "iterations:  1097\n",
      "Training loss: 0.013009514870475948, Validation loss: 0.026654242070923862\n",
      "new best w\n",
      "iterations:  1098\n",
      "Training loss: 0.012998029755849554, Validation loss: 0.026641636995623424\n",
      "new best w\n",
      "iterations:  1099\n",
      "Training loss: 0.012988477740845595, Validation loss: 0.02661415152591416\n",
      "new best w\n",
      "iterations:  1100\n",
      "Training loss: 0.012979173878840091, Validation loss: 0.026610894385099837\n",
      "new best w\n",
      "iterations:  1101\n",
      "Training loss: 0.012968255194962565, Validation loss: 0.026581736329538697\n",
      "new best w\n",
      "iterations:  1102\n",
      "Training loss: 0.012957590193089794, Validation loss: 0.026543973820353443\n",
      "new best w\n",
      "iterations:  1103\n",
      "Training loss: 0.012948304831363997, Validation loss: 0.026546668131354346\n",
      "iterations:  1104\n",
      "Training loss: 0.012938810091109603, Validation loss: 0.02656667391342865\n",
      "iterations:  1105\n",
      "Training loss: 0.01292790192723345, Validation loss: 0.02647799557755251\n",
      "new best w\n",
      "iterations:  1106\n",
      "Training loss: 0.012917051571887937, Validation loss: 0.0265079676015882\n",
      "iterations:  1107\n",
      "Training loss: 0.012908401819613183, Validation loss: 0.02647552892131776\n",
      "new best w\n",
      "iterations:  1108\n",
      "Training loss: 0.012897457255845297, Validation loss: 0.026467326442491584\n",
      "new best w\n",
      "iterations:  1109\n",
      "Training loss: 0.012887392168179723, Validation loss: 0.026436373128309175\n",
      "new best w\n",
      "iterations:  1110\n",
      "Training loss: 0.012877282348854817, Validation loss: 0.026403380216393067\n",
      "new best w\n",
      "iterations:  1111\n",
      "Training loss: 0.012867879763135047, Validation loss: 0.026443494157623447\n",
      "iterations:  1112\n",
      "Training loss: 0.012857104236364762, Validation loss: 0.02634966731501326\n",
      "new best w\n",
      "iterations:  1113\n",
      "Training loss: 0.012847069846860331, Validation loss: 0.02637863819430674\n",
      "iterations:  1114\n",
      "Training loss: 0.012837052541353906, Validation loss: 0.02635588073576515\n",
      "iterations:  1115\n",
      "Training loss: 0.012826639591272719, Validation loss: 0.026338196487854477\n",
      "new best w\n",
      "iterations:  1116\n",
      "Training loss: 0.012816549324649745, Validation loss: 0.026311975739834025\n",
      "new best w\n",
      "iterations:  1117\n",
      "Training loss: 0.01280745366068684, Validation loss: 0.026305362334949546\n",
      "new best w\n",
      "iterations:  1118\n",
      "Training loss: 0.012796304678455802, Validation loss: 0.026286448525168777\n",
      "new best w\n",
      "iterations:  1119\n",
      "Training loss: 0.012786423991619418, Validation loss: 0.026240626282419265\n",
      "new best w\n",
      "iterations:  1120\n",
      "Training loss: 0.01277639137055669, Validation loss: 0.026245938501325627\n",
      "iterations:  1121\n",
      "Training loss: 0.012767231683763387, Validation loss: 0.02623247617139664\n",
      "new best w\n",
      "iterations:  1122\n",
      "Training loss: 0.012755667541891766, Validation loss: 0.026214522047239446\n",
      "new best w\n",
      "iterations:  1123\n",
      "Training loss: 0.012745998740885705, Validation loss: 0.026194567717940438\n",
      "new best w\n",
      "iterations:  1124\n",
      "Training loss: 0.012736800322763525, Validation loss: 0.026177169666099537\n",
      "new best w\n",
      "iterations:  1125\n",
      "Training loss: 0.01272637656776879, Validation loss: 0.026159591213743\n",
      "new best w\n",
      "iterations:  1126\n",
      "Training loss: 0.012715567169655842, Validation loss: 0.026117842755753195\n",
      "new best w\n",
      "iterations:  1127\n",
      "Training loss: 0.012705967985969608, Validation loss: 0.026127583800980892\n",
      "iterations:  1128\n",
      "Training loss: 0.012696738569742629, Validation loss: 0.026133391498220117\n",
      "iterations:  1129\n",
      "Training loss: 0.012686084557940744, Validation loss: 0.026058429595602596\n",
      "new best w\n",
      "iterations:  1130\n",
      "Training loss: 0.012675294398160945, Validation loss: 0.026080720757272413\n",
      "iterations:  1131\n",
      "Training loss: 0.01266636781609598, Validation loss: 0.02605708783292617\n",
      "new best w\n",
      "iterations:  1132\n",
      "Training loss: 0.012655659322557002, Validation loss: 0.02603316725502196\n",
      "new best w\n",
      "iterations:  1133\n",
      "Training loss: 0.012645964530821428, Validation loss: 0.02598944366598412\n",
      "new best w\n",
      "iterations:  1134\n",
      "Training loss: 0.012635559512519514, Validation loss: 0.02600529569423827\n",
      "iterations:  1135\n",
      "Training loss: 0.012626237202509655, Validation loss: 0.026015244928444148\n",
      "iterations:  1136\n",
      "Training loss: 0.012615687655702805, Validation loss: 0.025927339743529752\n",
      "new best w\n",
      "iterations:  1137\n",
      "Training loss: 0.012605766397240554, Validation loss: 0.02595641193412973\n",
      "iterations:  1138\n",
      "Training loss: 0.01259589563835294, Validation loss: 0.02593370190070619\n",
      "iterations:  1139\n",
      "Training loss: 0.012585350853136152, Validation loss: 0.02591601700712968\n",
      "new best w\n",
      "iterations:  1140\n",
      "Training loss: 0.012575396342484493, Validation loss: 0.02588838843033226\n",
      "new best w\n",
      "iterations:  1141\n",
      "Training loss: 0.012566442273882115, Validation loss: 0.025882532926314393\n",
      "new best w\n",
      "iterations:  1142\n",
      "Training loss: 0.012555431030496483, Validation loss: 0.025865187142274787\n",
      "new best w\n",
      "iterations:  1143\n",
      "Training loss: 0.012545420644857837, Validation loss: 0.025817669558364103\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1144\n",
      "Training loss: 0.012535520308544481, Validation loss: 0.025822802606629093\n",
      "iterations:  1145\n",
      "Training loss: 0.012526497211045966, Validation loss: 0.025810502956173813\n",
      "new best w\n",
      "iterations:  1146\n",
      "Training loss: 0.012515082252881636, Validation loss: 0.02579195318373837\n",
      "new best w\n",
      "iterations:  1147\n",
      "Training loss: 0.01250526874640128, Validation loss: 0.02577208228823664\n",
      "new best w\n",
      "iterations:  1148\n",
      "Training loss: 0.012496247580178008, Validation loss: 0.025754255442600904\n",
      "new best w\n",
      "iterations:  1149\n",
      "Training loss: 0.01248587895275079, Validation loss: 0.02574034933653211\n",
      "new best w\n",
      "iterations:  1150\n",
      "Training loss: 0.012475261770727514, Validation loss: 0.025694059044491246\n",
      "new best w\n",
      "iterations:  1151\n",
      "Training loss: 0.012465610366525739, Validation loss: 0.025705692569259912\n",
      "iterations:  1152\n",
      "Training loss: 0.012456393972598592, Validation loss: 0.025710645984524404\n",
      "iterations:  1153\n",
      "Training loss: 0.012445949180244702, Validation loss: 0.025635328410481384\n",
      "new best w\n",
      "iterations:  1154\n",
      "Training loss: 0.01243524508553531, Validation loss: 0.02565810413899895\n",
      "iterations:  1155\n",
      "Training loss: 0.012426250325479535, Validation loss: 0.025635146765816658\n",
      "new best w\n",
      "iterations:  1156\n",
      "Training loss: 0.012415713899158807, Validation loss: 0.025607140687103864\n",
      "new best w\n",
      "iterations:  1157\n",
      "Training loss: 0.012405896722425078, Validation loss: 0.02559814875632329\n",
      "new best w\n",
      "iterations:  1158\n",
      "Training loss: 0.012396194848746107, Validation loss: 0.025552719011480114\n",
      "new best w\n",
      "iterations:  1159\n",
      "Training loss: 0.01238642126709945, Validation loss: 0.025602774421664247\n",
      "iterations:  1160\n",
      "Training loss: 0.01237605369255789, Validation loss: 0.025501138860052945\n",
      "new best w\n",
      "iterations:  1161\n",
      "Training loss: 0.012366213975956766, Validation loss: 0.025538452046005793\n",
      "iterations:  1162\n",
      "Training loss: 0.012356432897669089, Validation loss: 0.025510070912066522\n",
      "iterations:  1163\n",
      "Training loss: 0.012345852391034804, Validation loss: 0.02549377082018412\n",
      "new best w\n",
      "iterations:  1164\n",
      "Training loss: 0.012336045538536112, Validation loss: 0.025462893501517186\n",
      "new best w\n",
      "iterations:  1165\n",
      "Training loss: 0.012327174219468641, Validation loss: 0.02546358549471517\n",
      "iterations:  1166\n",
      "Training loss: 0.012316030694418906, Validation loss: 0.025441758592383144\n",
      "new best w\n",
      "iterations:  1167\n",
      "Training loss: 0.01230622327861802, Validation loss: 0.025395254085585194\n",
      "new best w\n",
      "iterations:  1168\n",
      "Training loss: 0.012296522522654153, Validation loss: 0.025400340253439346\n",
      "iterations:  1169\n",
      "Training loss: 0.012287569256656408, Validation loss: 0.025420253218063148\n",
      "iterations:  1170\n",
      "Training loss: 0.012276312078816253, Validation loss: 0.025336085044876073\n",
      "new best w\n",
      "iterations:  1171\n",
      "Training loss: 0.012266452475191099, Validation loss: 0.025359944989021506\n",
      "iterations:  1172\n",
      "Training loss: 0.012257494144023556, Validation loss: 0.025326356962113394\n",
      "new best w\n",
      "iterations:  1173\n",
      "Training loss: 0.012247084367894337, Validation loss: 0.025318846235769115\n",
      "new best w\n",
      "iterations:  1174\n",
      "Training loss: 0.012236393998907889, Validation loss: 0.025298457590241928\n",
      "new best w\n",
      "iterations:  1175\n",
      "Training loss: 0.012227551555461569, Validation loss: 0.025289258262764597\n",
      "new best w\n",
      "iterations:  1176\n",
      "Training loss: 0.012217423007937031, Validation loss: 0.025260060494659405\n",
      "new best w\n",
      "iterations:  1177\n",
      "Training loss: 0.012207480687393584, Validation loss: 0.025221590919895575\n",
      "new best w\n",
      "iterations:  1178\n",
      "Training loss: 0.012196936700889487, Validation loss: 0.025233093241381937\n",
      "iterations:  1179\n",
      "Training loss: 0.012188054250862335, Validation loss: 0.025213888576325543\n",
      "new best w\n",
      "iterations:  1180\n",
      "Training loss: 0.012177572764208024, Validation loss: 0.02518444234788056\n",
      "new best w\n",
      "iterations:  1181\n",
      "Training loss: 0.01216780558948995, Validation loss: 0.0251746265032666\n",
      "new best w\n",
      "iterations:  1182\n",
      "Training loss: 0.012158107771292566, Validation loss: 0.025165539806558872\n",
      "new best w\n",
      "iterations:  1183\n",
      "Training loss: 0.012147821593952009, Validation loss: 0.025143134846259747\n",
      "new best w\n",
      "iterations:  1184\n",
      "Training loss: 0.012138106791596279, Validation loss: 0.02508965256201586\n",
      "new best w\n",
      "iterations:  1185\n",
      "Training loss: 0.012128554012232442, Validation loss: 0.025109694020790736\n",
      "iterations:  1186\n",
      "Training loss: 0.012118627321739715, Validation loss: 0.02512227131356906\n",
      "iterations:  1187\n",
      "Training loss: 0.012108421103566119, Validation loss: 0.02503720880624997\n",
      "new best w\n",
      "iterations:  1188\n",
      "Training loss: 0.012098574887031177, Validation loss: 0.02505404877398636\n",
      "iterations:  1189\n",
      "Training loss: 0.012089691645465408, Validation loss: 0.025035449951926136\n",
      "new best w\n",
      "iterations:  1190\n",
      "Training loss: 0.012078470302724938, Validation loss: 0.02501950052367056\n",
      "new best w\n",
      "iterations:  1191\n",
      "Training loss: 0.012068831945970774, Validation loss: 0.02500018116394646\n",
      "new best w\n",
      "iterations:  1192\n",
      "Training loss: 0.01205987988184589, Validation loss: 0.0249850666617289\n",
      "new best w\n",
      "iterations:  1193\n",
      "Training loss: 0.01204974274620761, Validation loss: 0.02496921762219275\n",
      "new best w\n",
      "iterations:  1194\n",
      "Training loss: 0.01203904404939274, Validation loss: 0.024922539338341442\n",
      "new best w\n",
      "iterations:  1195\n",
      "Training loss: 0.012029683024476726, Validation loss: 0.024935026871273115\n",
      "iterations:  1196\n",
      "Training loss: 0.012020659356566744, Validation loss: 0.024906085414525655\n",
      "new best w\n",
      "iterations:  1197\n",
      "Training loss: 0.012010216309099502, Validation loss: 0.024896287772282578\n",
      "new best w\n",
      "iterations:  1198\n",
      "Training loss: 0.011999571418262401, Validation loss: 0.024876311417257515\n",
      "new best w\n",
      "iterations:  1199\n",
      "Training loss: 0.011991062459775749, Validation loss: 0.024867526727082447\n",
      "new best w\n",
      "iterations:  1200\n",
      "Training loss: 0.011980579040790357, Validation loss: 0.02483870301960555\n",
      "new best w\n",
      "iterations:  1201\n",
      "Training loss: 0.011970765902754785, Validation loss: 0.024826216931870426\n",
      "new best w\n",
      "iterations:  1202\n",
      "Training loss: 0.011960940131277308, Validation loss: 0.02478141893633392\n",
      "new best w\n",
      "iterations:  1203\n",
      "Training loss: 0.011951909056031274, Validation loss: 0.024834423973270802\n",
      "iterations:  1204\n",
      "Training loss: 0.011941581339187919, Validation loss: 0.024725562614002855\n",
      "new best w\n",
      "iterations:  1205\n",
      "Training loss: 0.011931594940899848, Validation loss: 0.024764142174506273\n",
      "iterations:  1206\n",
      "Training loss: 0.011921926024832403, Validation loss: 0.024744696214117064\n",
      "iterations:  1207\n",
      "Training loss: 0.01191215427409052, Validation loss: 0.024716102503880587\n",
      "new best w\n",
      "iterations:  1208\n",
      "Training loss: 0.011902175012575105, Validation loss: 0.02470776081560244\n",
      "new best w\n",
      "iterations:  1209\n",
      "Training loss: 0.011893001160918425, Validation loss: 0.02469221666164887\n",
      "new best w\n",
      "iterations:  1210\n",
      "Training loss: 0.011882118212510524, Validation loss: 0.02467615710896562\n",
      "new best w\n",
      "iterations:  1211\n",
      "Training loss: 0.011872933920133071, Validation loss: 0.02461946955248103\n",
      "new best w\n",
      "iterations:  1212\n",
      "Training loss: 0.011863383927050202, Validation loss: 0.024639273645715616\n",
      "iterations:  1213\n",
      "Training loss: 0.011853964628049097, Validation loss: 0.024648398050049286\n",
      "iterations:  1214\n",
      "Training loss: 0.011843166725771646, Validation loss: 0.02456832474623122\n",
      "new best w\n",
      "iterations:  1215\n",
      "Training loss: 0.011833685170290438, Validation loss: 0.024584896723690252\n",
      "iterations:  1216\n",
      "Training loss: 0.011824857375881546, Validation loss: 0.02457131794471109\n",
      "iterations:  1217\n",
      "Training loss: 0.011814055983014651, Validation loss: 0.024546443556281966\n",
      "new best w\n",
      "iterations:  1218\n",
      "Training loss: 0.011803832994729877, Validation loss: 0.024532651678241493\n",
      "new best w\n",
      "iterations:  1219\n",
      "Training loss: 0.011795349968573897, Validation loss: 0.02451484769708882\n",
      "new best w\n",
      "iterations:  1220\n",
      "Training loss: 0.011785322875106188, Validation loss: 0.024498690567077187\n",
      "new best w\n",
      "iterations:  1221\n",
      "Training loss: 0.011775071194554114, Validation loss: 0.0244473702856255\n",
      "new best w\n",
      "iterations:  1222\n",
      "Training loss: 0.011765166021560879, Validation loss: 0.02446772304084185\n",
      "iterations:  1223\n",
      "Training loss: 0.011756549605458107, Validation loss: 0.024436219076540932\n",
      "new best w\n",
      "iterations:  1224\n",
      "Training loss: 0.01174625977426703, Validation loss: 0.024430145371288507\n",
      "new best w\n",
      "iterations:  1225\n",
      "Training loss: 0.011735852373878834, Validation loss: 0.024401785675448047\n",
      "new best w\n",
      "iterations:  1226\n",
      "Training loss: 0.011726951966434552, Validation loss: 0.024401311931195088\n",
      "new best w\n",
      "iterations:  1227\n",
      "Training loss: 0.011716829606398874, Validation loss: 0.024368375322345433\n",
      "new best w\n",
      "iterations:  1228\n",
      "Training loss: 0.01170717187763626, Validation loss: 0.02434964366697536\n",
      "new best w\n",
      "iterations:  1229\n",
      "Training loss: 0.011697696161929824, Validation loss: 0.024349244556099763\n",
      "new best w\n",
      "iterations:  1230\n",
      "Training loss: 0.01168779043185831, Validation loss: 0.0243244208106432\n",
      "new best w\n",
      "iterations:  1231\n",
      "Training loss: 0.011678060828430117, Validation loss: 0.024270452848982136\n",
      "new best w\n",
      "iterations:  1232\n",
      "Training loss: 0.011668471947113982, Validation loss: 0.024291735528038254\n",
      "iterations:  1233\n",
      "Training loss: 0.011658863349779018, Validation loss: 0.02427191310991671\n",
      "iterations:  1234\n",
      "Training loss: 0.011648734000910412, Validation loss: 0.024253008597511193\n",
      "new best w\n",
      "iterations:  1235\n",
      "Training loss: 0.011639126298434485, Validation loss: 0.024224884460202267\n",
      "new best w\n",
      "iterations:  1236\n",
      "Training loss: 0.01163046028047916, Validation loss: 0.02421737509920516\n",
      "new best w\n",
      "iterations:  1237\n",
      "Training loss: 0.011619450804686914, Validation loss: 0.024201702046531524\n",
      "new best w\n",
      "iterations:  1238\n",
      "Training loss: 0.0116099666280498, Validation loss: 0.02418073711062479\n",
      "new best w\n",
      "iterations:  1239\n",
      "Training loss: 0.01160117548639567, Validation loss: 0.024127967953188122\n",
      "new best w\n",
      "iterations:  1240\n",
      "Training loss: 0.011591844027354841, Validation loss: 0.024192886031899855\n",
      "iterations:  1241\n",
      "Training loss: 0.01158079284016097, Validation loss: 0.02408881707599639\n",
      "new best w\n",
      "iterations:  1242\n",
      "Training loss: 0.011571474124675385, Validation loss: 0.024122163998564983\n",
      "iterations:  1243\n",
      "Training loss: 0.011562797533322953, Validation loss: 0.024082934541743824\n",
      "new best w\n",
      "iterations:  1244\n",
      "Training loss: 0.011552356300565218, Validation loss: 0.024078871669097782\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1245\n",
      "Training loss: 0.011541904406411058, Validation loss: 0.0240604717574936\n",
      "new best w\n",
      "iterations:  1246\n",
      "Training loss: 0.011533750632882613, Validation loss: 0.02404541942393938\n",
      "new best w\n",
      "iterations:  1247\n",
      "Training loss: 0.011523532322624536, Validation loss: 0.024031441994193582\n",
      "new best w\n",
      "iterations:  1248\n",
      "Training loss: 0.01151353792539545, Validation loss: 0.02400506782071438\n",
      "new best w\n",
      "iterations:  1249\n",
      "Training loss: 0.011503987077197457, Validation loss: 0.023967414396064528\n",
      "new best w\n",
      "iterations:  1250\n",
      "Training loss: 0.011495316806488328, Validation loss: 0.024012923550752054\n",
      "iterations:  1251\n",
      "Training loss: 0.01148522510956674, Validation loss: 0.023914169060714124\n",
      "new best w\n",
      "iterations:  1252\n",
      "Training loss: 0.011474995703565163, Validation loss: 0.023944742565286236\n",
      "iterations:  1253\n",
      "Training loss: 0.011465935699648811, Validation loss: 0.02392502880971199\n",
      "iterations:  1254\n",
      "Training loss: 0.01145608914698748, Validation loss: 0.02389671507893205\n",
      "new best w\n",
      "iterations:  1255\n",
      "Training loss: 0.01144640761226079, Validation loss: 0.023888281354764417\n",
      "new best w\n",
      "iterations:  1256\n",
      "Training loss: 0.011437116941154625, Validation loss: 0.023873714641763202\n",
      "new best w\n",
      "iterations:  1257\n",
      "Training loss: 0.011426877627495583, Validation loss: 0.023857248482837774\n",
      "new best w\n",
      "iterations:  1258\n",
      "Training loss: 0.01141751886281474, Validation loss: 0.023799472382577392\n",
      "new best w\n",
      "iterations:  1259\n",
      "Training loss: 0.011408281647705592, Validation loss: 0.023820361178385034\n",
      "iterations:  1260\n",
      "Training loss: 0.011398817203977469, Validation loss: 0.02383221620501881\n",
      "iterations:  1261\n",
      "Training loss: 0.011388709423973901, Validation loss: 0.023746764028309948\n",
      "new best w\n",
      "iterations:  1262\n",
      "Training loss: 0.011379049052040293, Validation loss: 0.0237671691922644\n",
      "iterations:  1263\n",
      "Training loss: 0.011370470879569487, Validation loss: 0.023751039936134075\n",
      "iterations:  1264\n",
      "Training loss: 0.011359579147607603, Validation loss: 0.023732529856973238\n",
      "new best w\n",
      "iterations:  1265\n",
      "Training loss: 0.011350011647465146, Validation loss: 0.023711194476331056\n",
      "new best w\n",
      "iterations:  1266\n",
      "Training loss: 0.01134146414170446, Validation loss: 0.023698834251459532\n",
      "new best w\n",
      "iterations:  1267\n",
      "Training loss: 0.011331641348801625, Validation loss: 0.023675684221922042\n",
      "new best w\n",
      "iterations:  1268\n",
      "Training loss: 0.011321366413626402, Validation loss: 0.02362932388943504\n",
      "new best w\n",
      "iterations:  1269\n",
      "Training loss: 0.011312197835721634, Validation loss: 0.02364829272824167\n",
      "iterations:  1270\n",
      "Training loss: 0.011303415236658135, Validation loss: 0.023620923367747404\n",
      "new best w\n",
      "iterations:  1271\n",
      "Training loss: 0.011293227285848998, Validation loss: 0.023607614630876093\n",
      "new best w\n",
      "iterations:  1272\n",
      "Training loss: 0.01128285693521886, Validation loss: 0.023593083166818456\n",
      "new best w\n",
      "iterations:  1273\n",
      "Training loss: 0.011274779852383843, Validation loss: 0.02357624444933446\n",
      "new best w\n",
      "iterations:  1274\n",
      "Training loss: 0.011264674892623817, Validation loss: 0.023563274020289453\n",
      "new best w\n",
      "iterations:  1275\n",
      "Training loss: 0.01125472948417961, Validation loss: 0.023528162137972847\n",
      "new best w\n",
      "iterations:  1276\n",
      "Training loss: 0.011245634928007071, Validation loss: 0.023537734034687237\n",
      "iterations:  1277\n",
      "Training loss: 0.01123614321663068, Validation loss: 0.023500376464565036\n",
      "new best w\n",
      "iterations:  1278\n",
      "Training loss: 0.011226635569907141, Validation loss: 0.023465003197826715\n",
      "new best w\n",
      "iterations:  1279\n",
      "Training loss: 0.011216669529554144, Validation loss: 0.02347112876628332\n",
      "iterations:  1280\n",
      "Training loss: 0.011207668004383742, Validation loss: 0.023460292575725163\n",
      "new best w\n",
      "iterations:  1281\n",
      "Training loss: 0.01119789374697181, Validation loss: 0.02342740663437292\n",
      "new best w\n",
      "iterations:  1282\n",
      "Training loss: 0.011188482111174347, Validation loss: 0.023420652611934923\n",
      "new best w\n",
      "iterations:  1283\n",
      "Training loss: 0.01117927500463934, Validation loss: 0.02339795127919977\n",
      "new best w\n",
      "iterations:  1284\n",
      "Training loss: 0.01116915702047076, Validation loss: 0.023389657706937527\n",
      "new best w\n",
      "iterations:  1285\n",
      "Training loss: 0.011159564867932013, Validation loss: 0.023358124553221664\n",
      "new best w\n",
      "iterations:  1286\n",
      "Training loss: 0.011151184878354243, Validation loss: 0.023361777804796233\n",
      "iterations:  1287\n",
      "Training loss: 0.011140597476734354, Validation loss: 0.023335244509341328\n",
      "new best w\n",
      "iterations:  1288\n",
      "Training loss: 0.01113121764772537, Validation loss: 0.02328944877798094\n",
      "new best w\n",
      "iterations:  1289\n",
      "Training loss: 0.011121809941938223, Validation loss: 0.023295705187368402\n",
      "iterations:  1290\n",
      "Training loss: 0.011113327637254634, Validation loss: 0.023279762675616815\n",
      "new best w\n",
      "iterations:  1291\n",
      "Training loss: 0.011102515227103543, Validation loss: 0.023263070952590843\n",
      "new best w\n",
      "iterations:  1292\n",
      "Training loss: 0.01109325857645615, Validation loss: 0.02324214236116525\n",
      "new best w\n",
      "iterations:  1293\n",
      "Training loss: 0.01108468192042124, Validation loss: 0.023231034400969134\n",
      "new best w\n",
      "iterations:  1294\n",
      "Training loss: 0.01107479830725135, Validation loss: 0.02321277714427183\n",
      "new best w\n",
      "iterations:  1295\n",
      "Training loss: 0.011064250590880982, Validation loss: 0.023191684663033982\n",
      "new best w\n",
      "iterations:  1296\n",
      "Training loss: 0.011056254663112362, Validation loss: 0.023188386476997315\n",
      "new best w\n",
      "iterations:  1297\n",
      "Training loss: 0.011046511238103764, Validation loss: 0.02315489696205048\n",
      "new best w\n",
      "iterations:  1298\n",
      "Training loss: 0.011036780219773935, Validation loss: 0.023118495553184515\n",
      "new best w\n",
      "iterations:  1299\n",
      "Training loss: 0.011026916599028562, Validation loss: 0.02312616675421234\n",
      "iterations:  1300\n",
      "Training loss: 0.011018682234967088, Validation loss: 0.023107469691885256\n",
      "new best w\n",
      "iterations:  1301\n",
      "Training loss: 0.011008736003973422, Validation loss: 0.023095751407752402\n",
      "new best w\n",
      "iterations:  1302\n",
      "Training loss: 0.010998745163366446, Validation loss: 0.023067812125765762\n",
      "new best w\n",
      "iterations:  1303\n",
      "Training loss: 0.010989906913300238, Validation loss: 0.023070517518796543\n",
      "iterations:  1304\n",
      "Training loss: 0.010980229663045363, Validation loss: 0.02303370598662039\n",
      "new best w\n",
      "iterations:  1305\n",
      "Training loss: 0.010970772556230635, Validation loss: 0.023026792278297275\n",
      "new best w\n",
      "iterations:  1306\n",
      "Training loss: 0.01096179226357354, Validation loss: 0.023004979917430515\n",
      "new best w\n",
      "iterations:  1307\n",
      "Training loss: 0.01095200311368065, Validation loss: 0.022993734781447187\n",
      "new best w\n",
      "iterations:  1308\n",
      "Training loss: 0.01094267646646159, Validation loss: 0.022936779461002386\n",
      "new best w\n",
      "iterations:  1309\n",
      "Training loss: 0.010933369761114663, Validation loss: 0.022960949788395784\n",
      "iterations:  1310\n",
      "Training loss: 0.010924012121101952, Validation loss: 0.02294081354944149\n",
      "iterations:  1311\n",
      "Training loss: 0.010914242788664867, Validation loss: 0.022919920081936416\n",
      "new best w\n",
      "iterations:  1312\n",
      "Training loss: 0.01090485491197598, Validation loss: 0.022893914912468484\n",
      "new best w\n",
      "iterations:  1313\n",
      "Training loss: 0.010896423126728151, Validation loss: 0.022895680361083977\n",
      "iterations:  1314\n",
      "Training loss: 0.010885793837607594, Validation loss: 0.02286442028833304\n",
      "new best w\n",
      "iterations:  1315\n",
      "Training loss: 0.010876626691137758, Validation loss: 0.022848519802879247\n",
      "new best w\n",
      "iterations:  1316\n",
      "Training loss: 0.010868298632118857, Validation loss: 0.022837338577243058\n",
      "new best w\n",
      "iterations:  1317\n",
      "Training loss: 0.010858308927129568, Validation loss: 0.022821893020570035\n",
      "new best w\n",
      "iterations:  1318\n",
      "Training loss: 0.010848351977540692, Validation loss: 0.022774231294396034\n",
      "new best w\n",
      "iterations:  1319\n",
      "Training loss: 0.010839463780647506, Validation loss: 0.022783309428119\n",
      "iterations:  1320\n",
      "Training loss: 0.01083097396310579, Validation loss: 0.02277401965738416\n",
      "new best w\n",
      "iterations:  1321\n",
      "Training loss: 0.010820598089107698, Validation loss: 0.022746382698546935\n",
      "new best w\n",
      "iterations:  1322\n",
      "Training loss: 0.010811048278954499, Validation loss: 0.022725635605757008\n",
      "new best w\n",
      "iterations:  1323\n",
      "Training loss: 0.01080279287877308, Validation loss: 0.022716449762238942\n",
      "new best w\n",
      "iterations:  1324\n",
      "Training loss: 0.010793122672602944, Validation loss: 0.022702163476089856\n",
      "new best w\n",
      "iterations:  1325\n",
      "Training loss: 0.010782876136608366, Validation loss: 0.022674913824395626\n",
      "new best w\n",
      "iterations:  1326\n",
      "Training loss: 0.010774470930934817, Validation loss: 0.022678140425821995\n",
      "iterations:  1327\n",
      "Training loss: 0.010764823845318705, Validation loss: 0.02264124060357954\n",
      "new best w\n",
      "iterations:  1328\n",
      "Training loss: 0.010755518257691943, Validation loss: 0.02263244373924552\n",
      "new best w\n",
      "iterations:  1329\n",
      "Training loss: 0.010746003527217707, Validation loss: 0.022583451073254962\n",
      "new best w\n",
      "iterations:  1330\n",
      "Training loss: 0.010737708708955507, Validation loss: 0.0226405767948808\n",
      "iterations:  1331\n",
      "Training loss: 0.0107278534734899, Validation loss: 0.022528123723009056\n",
      "new best w\n",
      "iterations:  1332\n",
      "Training loss: 0.010718287320414039, Validation loss: 0.022575239977420412\n",
      "iterations:  1333\n",
      "Training loss: 0.010708960249724597, Validation loss: 0.022545903606993507\n",
      "iterations:  1334\n",
      "Training loss: 0.010699690171727576, Validation loss: 0.022528609805745968\n",
      "iterations:  1335\n",
      "Training loss: 0.01069031046681652, Validation loss: 0.02249976850887025\n",
      "new best w\n",
      "iterations:  1336\n",
      "Training loss: 0.010681511993925655, Validation loss: 0.022506582103172048\n",
      "iterations:  1337\n",
      "Training loss: 0.01067148020998324, Validation loss: 0.022481607683205666\n",
      "new best w\n",
      "iterations:  1338\n",
      "Training loss: 0.010662409887921246, Validation loss: 0.02244592271480771\n",
      "new best w\n",
      "iterations:  1339\n",
      "Training loss: 0.010653974031148945, Validation loss: 0.022419998368694894\n",
      "new best w\n",
      "iterations:  1340\n",
      "Training loss: 0.01064465113952944, Validation loss: 0.02246960410755595\n",
      "iterations:  1341\n",
      "Training loss: 0.010634686889278811, Validation loss: 0.022367796700006586\n",
      "new best w\n",
      "iterations:  1342\n",
      "Training loss: 0.0106253650125841, Validation loss: 0.022397959080001365\n",
      "iterations:  1343\n",
      "Training loss: 0.010617020973681366, Validation loss: 0.02237816242944963\n",
      "iterations:  1344\n",
      "Training loss: 0.01060651334407734, Validation loss: 0.02236030658333659\n",
      "new best w\n",
      "iterations:  1345\n",
      "Training loss: 0.010597441776624077, Validation loss: 0.022332670504661683\n",
      "new best w\n",
      "iterations:  1346\n",
      "Training loss: 0.010589149825865521, Validation loss: 0.02232613513146024\n",
      "new best w\n",
      "iterations:  1347\n",
      "Training loss: 0.010579514976840877, Validation loss: 0.022308501640766882\n",
      "new best w\n",
      "iterations:  1348\n",
      "Training loss: 0.010569131900663584, Validation loss: 0.02228867262109875\n",
      "new best w\n",
      "iterations:  1349\n",
      "Training loss: 0.010561341479227708, Validation loss: 0.02228442780484987\n",
      "new best w\n",
      "iterations:  1350\n",
      "Training loss: 0.010551936122597196, Validation loss: 0.022250390492727333\n",
      "new best w\n",
      "iterations:  1351\n",
      "Training loss: 0.010542324596928268, Validation loss: 0.022214027091812253\n",
      "new best w\n",
      "iterations:  1352\n",
      "Training loss: 0.010532663484923675, Validation loss: 0.022229330995295213\n",
      "iterations:  1353\n",
      "Training loss: 0.010524654215671518, Validation loss: 0.022198464025510053\n",
      "new best w\n",
      "iterations:  1354\n",
      "Training loss: 0.010515094460673233, Validation loss: 0.02219148643153495\n",
      "new best w\n",
      "iterations:  1355\n",
      "Training loss: 0.010505201743768828, Validation loss: 0.022162602074341946\n",
      "new best w\n",
      "iterations:  1356\n",
      "Training loss: 0.010496734259728613, Validation loss: 0.022168700528972504\n",
      "iterations:  1357\n",
      "Training loss: 0.010487261063500695, Validation loss: 0.02213027338036073\n",
      "new best w\n",
      "iterations:  1358\n",
      "Training loss: 0.010477897799789644, Validation loss: 0.02213261892580463\n",
      "iterations:  1359\n",
      "Training loss: 0.010468851260772597, Validation loss: 0.02211103075631619\n",
      "new best w\n",
      "iterations:  1360\n",
      "Training loss: 0.010459529393267475, Validation loss: 0.022092148776012274\n",
      "new best w\n",
      "iterations:  1361\n",
      "Training loss: 0.010450382364560567, Validation loss: 0.022056004728793162\n",
      "new best w\n",
      "iterations:  1362\n",
      "Training loss: 0.01044181454438243, Validation loss: 0.022024990624378633\n",
      "new best w\n",
      "iterations:  1363\n",
      "Training loss: 0.010432451477285297, Validation loss: 0.022088850489735665\n",
      "iterations:  1364\n",
      "Training loss: 0.010423343097671258, Validation loss: 0.02196927075246698\n",
      "new best w\n",
      "iterations:  1365\n",
      "Training loss: 0.010413964230018717, Validation loss: 0.022022698944133133\n",
      "iterations:  1366\n",
      "Training loss: 0.010405152144807257, Validation loss: 0.021983805495221224\n",
      "iterations:  1367\n",
      "Training loss: 0.01039514294468919, Validation loss: 0.021977954194163412\n",
      "iterations:  1368\n",
      "Training loss: 0.010386164032640793, Validation loss: 0.021942221194466253\n",
      "new best w\n",
      "iterations:  1369\n",
      "Training loss: 0.010378161394481823, Validation loss: 0.02194770069311573\n",
      "iterations:  1370\n",
      "Training loss: 0.010367979561258275, Validation loss: 0.021912100193317064\n",
      "new best w\n",
      "iterations:  1371\n",
      "Training loss: 0.010358486268800322, Validation loss: 0.02190757933318527\n",
      "new best w\n",
      "iterations:  1372\n",
      "Training loss: 0.010350403910755814, Validation loss: 0.02189005933308004\n",
      "new best w\n",
      "iterations:  1373\n",
      "Training loss: 0.010341051341450257, Validation loss: 0.021874539652565854\n",
      "new best w\n",
      "iterations:  1374\n",
      "Training loss: 0.010331097916323955, Validation loss: 0.021824861525102966\n",
      "new best w\n",
      "iterations:  1375\n",
      "Training loss: 0.010322328396090381, Validation loss: 0.021842416886945635\n",
      "iterations:  1376\n",
      "Training loss: 0.010314200050753541, Validation loss: 0.021815689651317034\n",
      "new best w\n",
      "iterations:  1377\n",
      "Training loss: 0.010304330718820863, Validation loss: 0.021798967361307035\n",
      "new best w\n",
      "iterations:  1378\n",
      "Training loss: 0.010294584529606085, Validation loss: 0.021787035123330183\n",
      "new best w\n",
      "iterations:  1379\n",
      "Training loss: 0.0102867578848835, Validation loss: 0.021773308386162628\n",
      "new best w\n",
      "iterations:  1380\n",
      "Training loss: 0.01027736960156628, Validation loss: 0.02175832069337199\n",
      "new best w\n",
      "iterations:  1381\n",
      "Training loss: 0.010267560547060966, Validation loss: 0.021729528864864046\n",
      "new best w\n",
      "iterations:  1382\n",
      "Training loss: 0.010259217412713543, Validation loss: 0.021736871838150127\n",
      "iterations:  1383\n",
      "Training loss: 0.010249983256723742, Validation loss: 0.0216982991307581\n",
      "new best w\n",
      "iterations:  1384\n",
      "Training loss: 0.010240893232352041, Validation loss: 0.02169028032944393\n",
      "new best w\n",
      "iterations:  1385\n",
      "Training loss: 0.01023206203411802, Validation loss: 0.021634055288005355\n",
      "new best w\n",
      "iterations:  1386\n",
      "Training loss: 0.010223684263896662, Validation loss: 0.021710577519598494\n",
      "iterations:  1387\n",
      "Training loss: 0.010214317188978037, Validation loss: 0.021581552523753843\n",
      "new best w\n",
      "iterations:  1388\n",
      "Training loss: 0.010205045408490314, Validation loss: 0.021648360644258516\n",
      "iterations:  1389\n",
      "Training loss: 0.010195906355561078, Validation loss: 0.02159900554557555\n",
      "iterations:  1390\n",
      "Training loss: 0.01018692172461546, Validation loss: 0.02159084591332646\n",
      "iterations:  1391\n",
      "Training loss: 0.010177956251803698, Validation loss: 0.021558163004384717\n",
      "new best w\n",
      "iterations:  1392\n",
      "Training loss: 0.010169412543870002, Validation loss: 0.021568876422891688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1393\n",
      "Training loss: 0.010159726195634542, Validation loss: 0.021532247951567052\n",
      "new best w\n",
      "iterations:  1394\n",
      "Training loss: 0.010150770790363622, Validation loss: 0.021517415052381368\n",
      "new best w\n",
      "iterations:  1395\n",
      "Training loss: 0.010142824498534495, Validation loss: 0.021518558102532406\n",
      "iterations:  1396\n",
      "Training loss: 0.010132764076798209, Validation loss: 0.021492124336957543\n",
      "new best w\n",
      "iterations:  1397\n",
      "Training loss: 0.010123601869686907, Validation loss: 0.021472999214773913\n",
      "new best w\n",
      "iterations:  1398\n",
      "Training loss: 0.010115645534940598, Validation loss: 0.021421483355059424\n",
      "new best w\n",
      "iterations:  1399\n",
      "Training loss: 0.01010723078943206, Validation loss: 0.02149500259528342\n",
      "iterations:  1400\n",
      "Training loss: 0.010097042460415555, Validation loss: 0.021367415358156085\n",
      "new best w\n",
      "iterations:  1401\n",
      "Training loss: 0.01008852556485743, Validation loss: 0.021439230290170423\n",
      "iterations:  1402\n",
      "Training loss: 0.010080295690057948, Validation loss: 0.021372702782287784\n",
      "iterations:  1403\n",
      "Training loss: 0.010070445250372064, Validation loss: 0.021382043623169682\n",
      "iterations:  1404\n",
      "Training loss: 0.010060977855046123, Validation loss: 0.02135264559631407\n",
      "new best w\n",
      "iterations:  1405\n",
      "Training loss: 0.010053215831703186, Validation loss: 0.021346030278783255\n",
      "new best w\n",
      "iterations:  1406\n",
      "Training loss: 0.010043990011796487, Validation loss: 0.021326969164267743\n",
      "new best w\n",
      "iterations:  1407\n",
      "Training loss: 0.010034138766809371, Validation loss: 0.021305532683125506\n",
      "new best w\n",
      "iterations:  1408\n",
      "Training loss: 0.01002630859046941, Validation loss: 0.021306161589572086\n",
      "iterations:  1409\n",
      "Training loss: 0.010017217691852669, Validation loss: 0.021261363258818\n",
      "new best w\n",
      "iterations:  1410\n",
      "Training loss: 0.010007974274734311, Validation loss: 0.021272621564240802\n",
      "iterations:  1411\n",
      "Training loss: 0.009999439219242913, Validation loss: 0.02120518991660022\n",
      "new best w\n",
      "iterations:  1412\n",
      "Training loss: 0.009991112777754248, Validation loss: 0.021283404072161415\n",
      "iterations:  1413\n",
      "Training loss: 0.009982098806497747, Validation loss: 0.02114835422539879\n",
      "new best w\n",
      "iterations:  1414\n",
      "Training loss: 0.009972437791412249, Validation loss: 0.021222313947251856\n",
      "iterations:  1415\n",
      "Training loss: 0.009963883912325494, Validation loss: 0.021173930614222562\n",
      "iterations:  1416\n",
      "Training loss: 0.009954955699966333, Validation loss: 0.021156916915970266\n",
      "iterations:  1417\n",
      "Training loss: 0.009946151265663027, Validation loss: 0.02113237906063751\n",
      "new best w\n",
      "iterations:  1418\n",
      "Training loss: 0.009937342415503942, Validation loss: 0.02114262995007236\n",
      "iterations:  1419\n",
      "Training loss: 0.00992809824118026, Validation loss: 0.021107953422924295\n",
      "new best w\n",
      "iterations:  1420\n",
      "Training loss: 0.009919247146985256, Validation loss: 0.021087120972265987\n",
      "new best w\n",
      "iterations:  1421\n",
      "Training loss: 0.00991125109316696, Validation loss: 0.021087950697187666\n",
      "iterations:  1422\n",
      "Training loss: 0.009901257782250173, Validation loss: 0.02106839091418733\n",
      "new best w\n",
      "iterations:  1423\n",
      "Training loss: 0.009892724186085613, Validation loss: 0.02103651217259438\n",
      "new best w\n",
      "iterations:  1424\n",
      "Training loss: 0.009884950209456149, Validation loss: 0.02104918827389752\n",
      "iterations:  1425\n",
      "Training loss: 0.00987530417693747, Validation loss: 0.021001750233316525\n",
      "new best w\n",
      "iterations:  1426\n",
      "Training loss: 0.009865990316068673, Validation loss: 0.021020360787117033\n",
      "iterations:  1427\n",
      "Training loss: 0.009858461717603939, Validation loss: 0.02093344974283318\n",
      "new best w\n",
      "iterations:  1428\n",
      "Training loss: 0.009849898759726684, Validation loss: 0.021026632299806388\n",
      "iterations:  1429\n",
      "Training loss: 0.009839856563142079, Validation loss: 0.020894362173780233\n",
      "new best w\n",
      "iterations:  1430\n",
      "Training loss: 0.009831096535992267, Validation loss: 0.020960521386849493\n",
      "iterations:  1431\n",
      "Training loss: 0.009823178744334062, Validation loss: 0.02090111009178543\n",
      "iterations:  1432\n",
      "Training loss: 0.009813694279380083, Validation loss: 0.020906761983249735\n",
      "iterations:  1433\n",
      "Training loss: 0.00980427735883686, Validation loss: 0.02087042273336402\n",
      "new best w\n",
      "iterations:  1434\n",
      "Training loss: 0.009796748247380851, Validation loss: 0.020883014627701906\n",
      "iterations:  1435\n",
      "Training loss: 0.009787701232537827, Validation loss: 0.0208476926817512\n",
      "new best w\n",
      "iterations:  1436\n",
      "Training loss: 0.009778066582548763, Validation loss: 0.02082754726588153\n",
      "new best w\n",
      "iterations:  1437\n",
      "Training loss: 0.0097700536786205, Validation loss: 0.020832627768332737\n",
      "iterations:  1438\n",
      "Training loss: 0.009761111777169283, Validation loss: 0.020792458050413777\n",
      "new best w\n",
      "iterations:  1439\n",
      "Training loss: 0.009752079718574906, Validation loss: 0.020786070737078603\n",
      "new best w\n",
      "iterations:  1440\n",
      "Training loss: 0.009743438151240688, Validation loss: 0.02077921485616408\n",
      "new best w\n",
      "iterations:  1441\n",
      "Training loss: 0.00973488371149518, Validation loss: 0.020747709829941677\n",
      "new best w\n",
      "iterations:  1442\n",
      "Training loss: 0.009725906456543204, Validation loss: 0.020727958712342516\n",
      "new best w\n",
      "iterations:  1443\n",
      "Training loss: 0.009717642566478144, Validation loss: 0.020691670693032953\n",
      "new best w\n",
      "iterations:  1444\n",
      "Training loss: 0.009708885241556109, Validation loss: 0.020761204375448337\n",
      "iterations:  1445\n",
      "Training loss: 0.009700262510363076, Validation loss: 0.020628841618617937\n",
      "new best w\n",
      "iterations:  1446\n",
      "Training loss: 0.009691342950949786, Validation loss: 0.020706482966815033\n",
      "iterations:  1447\n",
      "Training loss: 0.009682487557365971, Validation loss: 0.020645298517797308\n",
      "iterations:  1448\n",
      "Training loss: 0.009673488884831563, Validation loss: 0.020646318188091838\n",
      "iterations:  1449\n",
      "Training loss: 0.009665103131909785, Validation loss: 0.020599320023957805\n",
      "new best w\n",
      "iterations:  1450\n",
      "Training loss: 0.009656837731347704, Validation loss: 0.020623241020242117\n",
      "iterations:  1451\n",
      "Training loss: 0.009647098814456034, Validation loss: 0.020588878139115828\n",
      "new best w\n",
      "iterations:  1452\n",
      "Training loss: 0.00963867798969834, Validation loss: 0.02057514260124086\n",
      "new best w\n",
      "iterations:  1453\n",
      "Training loss: 0.00963107914464614, Validation loss: 0.020578205977497724\n",
      "iterations:  1454\n",
      "Training loss: 0.00962134999399481, Validation loss: 0.020531338652035207\n",
      "new best w\n",
      "iterations:  1455\n",
      "Training loss: 0.00961229858676583, Validation loss: 0.02053023166686673\n",
      "new best w\n",
      "iterations:  1456\n",
      "Training loss: 0.009604695775320693, Validation loss: 0.020512402437833668\n",
      "new best w\n",
      "iterations:  1457\n",
      "Training loss: 0.00959580509408605, Validation loss: 0.020491293672872814\n",
      "new best w\n",
      "iterations:  1458\n",
      "Training loss: 0.009585924949012118, Validation loss: 0.02048275838215523\n",
      "new best w\n",
      "iterations:  1459\n",
      "Training loss: 0.009578674578235118, Validation loss: 0.020470223818875546\n",
      "new best w\n",
      "iterations:  1460\n",
      "Training loss: 0.009569956712991658, Validation loss: 0.020437894025758182\n",
      "new best w\n",
      "iterations:  1461\n",
      "Training loss: 0.009560342169201139, Validation loss: 0.020444063216982913\n",
      "iterations:  1462\n",
      "Training loss: 0.009552573744056757, Validation loss: 0.020375754255434666\n",
      "new best w\n",
      "iterations:  1463\n",
      "Training loss: 0.00954432195341101, Validation loss: 0.020450571959219337\n",
      "iterations:  1464\n",
      "Training loss: 0.009535419962304892, Validation loss: 0.020327177946707534\n",
      "new best w\n",
      "iterations:  1465\n",
      "Training loss: 0.009526025508745636, Validation loss: 0.020400636930279555\n",
      "iterations:  1466\n",
      "Training loss: 0.00951823835038066, Validation loss: 0.020329320434881677\n",
      "iterations:  1467\n",
      "Training loss: 0.009509377026107263, Validation loss: 0.020332051152349456\n",
      "iterations:  1468\n",
      "Training loss: 0.009500293224770666, Validation loss: 0.02030741329521496\n",
      "new best w\n",
      "iterations:  1469\n",
      "Training loss: 0.0094920108932681, Validation loss: 0.020313288867862773\n",
      "iterations:  1470\n",
      "Training loss: 0.009483310618540394, Validation loss: 0.02027381300120866\n",
      "new best w\n",
      "iterations:  1471\n",
      "Training loss: 0.009474682666999832, Validation loss: 0.020267165556529625\n",
      "new best w\n",
      "iterations:  1472\n",
      "Training loss: 0.009466101929792706, Validation loss: 0.020261551578402936\n",
      "new best w\n",
      "iterations:  1473\n",
      "Training loss: 0.009457388425376898, Validation loss: 0.020246200253439216\n",
      "new best w\n",
      "iterations:  1474\n",
      "Training loss: 0.009448982643595684, Validation loss: 0.020195733762363867\n",
      "new best w\n",
      "iterations:  1475\n",
      "Training loss: 0.009440874787359482, Validation loss: 0.020222584367908688\n",
      "iterations:  1476\n",
      "Training loss: 0.009431424661374546, Validation loss: 0.020185640272446716\n",
      "new best w\n",
      "iterations:  1477\n",
      "Training loss: 0.009422931291058615, Validation loss: 0.020178241494983616\n",
      "new best w\n",
      "iterations:  1478\n",
      "Training loss: 0.00941539021999735, Validation loss: 0.02016291640840827\n",
      "new best w\n",
      "iterations:  1479\n",
      "Training loss: 0.00940578064832627, Validation loss: 0.02014222557277862\n",
      "new best w\n",
      "iterations:  1480\n",
      "Training loss: 0.009397020541477901, Validation loss: 0.020144446697389844\n",
      "iterations:  1481\n",
      "Training loss: 0.009389624957165806, Validation loss: 0.02010328863714397\n",
      "new best w\n",
      "iterations:  1482\n",
      "Training loss: 0.009380803113918207, Validation loss: 0.020102478229157766\n",
      "new best w\n",
      "iterations:  1483\n",
      "Training loss: 0.009371162459564599, Validation loss: 0.020068198406936345\n",
      "new best w\n",
      "iterations:  1484\n",
      "Training loss: 0.009364084455106563, Validation loss: 0.020081258975213633\n",
      "iterations:  1485\n",
      "Training loss: 0.009355347979464872, Validation loss: 0.020039592122076795\n",
      "new best w\n",
      "iterations:  1486\n",
      "Training loss: 0.009345683229307223, Validation loss: 0.020044217535845\n",
      "iterations:  1487\n",
      "Training loss: 0.009338357807622358, Validation loss: 0.019971472128443933\n",
      "new best w\n",
      "iterations:  1488\n",
      "Training loss: 0.009330208656830615, Validation loss: 0.020050582096633122\n",
      "iterations:  1489\n",
      "Training loss: 0.009321418747507497, Validation loss: 0.01992054584953836\n",
      "new best w\n",
      "iterations:  1490\n",
      "Training loss: 0.009312493389710698, Validation loss: 0.020022758795250197\n",
      "iterations:  1491\n",
      "Training loss: 0.009304803755513237, Validation loss: 0.019910776383145475\n",
      "new best w\n",
      "iterations:  1492\n",
      "Training loss: 0.009296207430802261, Validation loss: 0.019979600424766604\n",
      "iterations:  1493\n",
      "Training loss: 0.009287400404673556, Validation loss: 0.019857197368172732\n",
      "new best w\n",
      "iterations:  1494\n",
      "Training loss: 0.009279883713515305, Validation loss: 0.019986779041594106\n",
      "iterations:  1495\n",
      "Training loss: 0.009270890244979927, Validation loss: 0.019806053688006264\n",
      "new best w\n",
      "iterations:  1496\n",
      "Training loss: 0.009261727656699425, Validation loss: 0.019910347796388936\n",
      "iterations:  1497\n",
      "Training loss: 0.009252968202224168, Validation loss: 0.019836527271078812\n",
      "iterations:  1498\n",
      "Training loss: 0.009244665820712126, Validation loss: 0.019839275869535877\n",
      "iterations:  1499\n",
      "Training loss: 0.0092363612800454, Validation loss: 0.01980703486279617\n",
      "iterations:  1500\n",
      "Training loss: 0.009228119662792512, Validation loss: 0.01983253272761793\n",
      "iterations:  1501\n",
      "Training loss: 0.009219206297905364, Validation loss: 0.019779153767351745\n",
      "new best w\n",
      "iterations:  1502\n",
      "Training loss: 0.009210723904832336, Validation loss: 0.01977033939517157\n",
      "new best w\n",
      "iterations:  1503\n",
      "Training loss: 0.009203108258623948, Validation loss: 0.019770292515872093\n",
      "new best w\n",
      "iterations:  1504\n",
      "Training loss: 0.00919360260454491, Validation loss: 0.01973969185434659\n",
      "new best w\n",
      "iterations:  1505\n",
      "Training loss: 0.009185383181684232, Validation loss: 0.019726818277600977\n",
      "new best w\n",
      "iterations:  1506\n",
      "Training loss: 0.009177901634960196, Validation loss: 0.019715346448746465\n",
      "new best w\n",
      "iterations:  1507\n",
      "Training loss: 0.009168539331642675, Validation loss: 0.019693900958055035\n",
      "new best w\n",
      "iterations:  1508\n",
      "Training loss: 0.009160039868151651, Validation loss: 0.019699243739860726\n",
      "iterations:  1509\n",
      "Training loss: 0.009152716033752496, Validation loss: 0.019648047804655448\n",
      "new best w\n",
      "iterations:  1510\n",
      "Training loss: 0.009143895888905943, Validation loss: 0.019660580720552497\n",
      "iterations:  1511\n",
      "Training loss: 0.009134516305521085, Validation loss: 0.019619182766916928\n",
      "new best w\n",
      "iterations:  1512\n",
      "Training loss: 0.00912748558969121, Validation loss: 0.01963509825954728\n",
      "iterations:  1513\n",
      "Training loss: 0.009118928281290024, Validation loss: 0.01958878002578764\n",
      "new best w\n",
      "iterations:  1514\n",
      "Training loss: 0.00910942037279421, Validation loss: 0.019602745251834548\n",
      "iterations:  1515\n",
      "Training loss: 0.00910208464379258, Validation loss: 0.019564627706367348\n",
      "new best w\n",
      "iterations:  1516\n",
      "Training loss: 0.009093803616796481, Validation loss: 0.01955681183243288\n",
      "new best w\n",
      "iterations:  1517\n",
      "Training loss: 0.00908472397289803, Validation loss: 0.019527122344246892\n",
      "new best w\n",
      "iterations:  1518\n",
      "Training loss: 0.009076816081021606, Validation loss: 0.019534018461898454\n",
      "iterations:  1519\n",
      "Training loss: 0.009068704554128308, Validation loss: 0.019494297312998798\n",
      "new best w\n",
      "iterations:  1520\n",
      "Training loss: 0.009059851144200144, Validation loss: 0.019497702516025492\n",
      "iterations:  1521\n",
      "Training loss: 0.009051775682841509, Validation loss: 0.019476024964016028\n",
      "new best w\n",
      "iterations:  1522\n",
      "Training loss: 0.00904364481755957, Validation loss: 0.019487145429014675\n",
      "iterations:  1523\n",
      "Training loss: 0.009035128211555719, Validation loss: 0.019409452284688315\n",
      "new best w\n",
      "iterations:  1524\n",
      "Training loss: 0.009027194885345423, Validation loss: 0.019458763841298205\n",
      "iterations:  1525\n",
      "Training loss: 0.009018325781412236, Validation loss: 0.019395204074125785\n",
      "new best w\n",
      "iterations:  1526\n",
      "Training loss: 0.009009851324465891, Validation loss: 0.01939966836900277\n",
      "iterations:  1527\n",
      "Training loss: 0.009002638613240998, Validation loss: 0.019375568194468274\n",
      "new best w\n",
      "iterations:  1528\n",
      "Training loss: 0.008993448939055435, Validation loss: 0.019396248519266742\n",
      "iterations:  1529\n",
      "Training loss: 0.00898500169802611, Validation loss: 0.019320572947028443\n",
      "new best w\n",
      "iterations:  1530\n",
      "Training loss: 0.008977852896137443, Validation loss: 0.019352637103826162\n",
      "iterations:  1531\n",
      "Training loss: 0.008968813389127625, Validation loss: 0.019299224761337254\n",
      "new best w\n",
      "iterations:  1532\n",
      "Training loss: 0.008959950261828385, Validation loss: 0.01931756374526569\n",
      "iterations:  1533\n",
      "Training loss: 0.008953174909086085, Validation loss: 0.019228819348577324\n",
      "new best w\n",
      "iterations:  1534\n",
      "Training loss: 0.008945831256744755, Validation loss: 0.019363386202261818\n",
      "iterations:  1535\n",
      "Training loss: 0.008936578612617157, Validation loss: 0.01915920944315658\n",
      "new best w\n",
      "iterations:  1536\n",
      "Training loss: 0.008929462706863062, Validation loss: 0.019337852253062077\n",
      "iterations:  1537\n",
      "Training loss: 0.008921638871597947, Validation loss: 0.019132725075994794\n",
      "new best w\n",
      "iterations:  1538\n",
      "Training loss: 0.00891214444029678, Validation loss: 0.019306919874099323\n",
      "iterations:  1539\n",
      "Training loss: 0.00890442130052851, Validation loss: 0.01909919799935159\n",
      "new best w\n",
      "iterations:  1540\n",
      "Training loss: 0.008896343805081782, Validation loss: 0.01926261773909155\n",
      "iterations:  1541\n",
      "Training loss: 0.008887528086414843, Validation loss: 0.019071254077492793\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1542\n",
      "Training loss: 0.00888002988843416, Validation loss: 0.01925516115836281\n",
      "iterations:  1543\n",
      "Training loss: 0.008872651631841264, Validation loss: 0.019019721488828596\n",
      "new best w\n",
      "iterations:  1544\n",
      "Training loss: 0.008863452464483527, Validation loss: 0.019209444489944605\n",
      "iterations:  1545\n",
      "Training loss: 0.008854986780686384, Validation loss: 0.01900351637755223\n",
      "new best w\n",
      "iterations:  1546\n",
      "Training loss: 0.00884723906995676, Validation loss: 0.01917890692033615\n",
      "iterations:  1547\n",
      "Training loss: 0.008838914228405596, Validation loss: 0.01896579354552198\n",
      "new best w\n",
      "iterations:  1548\n",
      "Training loss: 0.00883093810632826, Validation loss: 0.019159534284549848\n",
      "iterations:  1549\n",
      "Training loss: 0.008822649033452635, Validation loss: 0.018939637958525237\n",
      "new best w\n",
      "iterations:  1550\n",
      "Training loss: 0.008814011182019637, Validation loss: 0.019105936303144635\n",
      "iterations:  1551\n",
      "Training loss: 0.008806430374092202, Validation loss: 0.018913096112509298\n",
      "new best w\n",
      "iterations:  1552\n",
      "Training loss: 0.008798210364889755, Validation loss: 0.01909959752956033\n",
      "iterations:  1553\n",
      "Training loss: 0.008790198220616217, Validation loss: 0.018860370036810604\n",
      "new best w\n",
      "iterations:  1554\n",
      "Training loss: 0.008782750720045916, Validation loss: 0.019068873801831213\n",
      "iterations:  1555\n",
      "Training loss: 0.00877412310592947, Validation loss: 0.01883615930476922\n",
      "new best w\n",
      "iterations:  1556\n",
      "Training loss: 0.00876548315018918, Validation loss: 0.019030143299924595\n",
      "iterations:  1557\n",
      "Training loss: 0.00875819607953257, Validation loss: 0.01880090213777051\n",
      "new best w\n",
      "iterations:  1558\n",
      "Training loss: 0.008749767614184039, Validation loss: 0.019008411117590017\n",
      "iterations:  1559\n",
      "Training loss: 0.00874051140521355, Validation loss: 0.018780099478373693\n",
      "new best w\n",
      "iterations:  1560\n",
      "Training loss: 0.008733440098805795, Validation loss: 0.01896485589825404\n",
      "iterations:  1561\n",
      "Training loss: 0.008724285396222555, Validation loss: 0.01878046936454714\n",
      "iterations:  1562\n",
      "Training loss: 0.008714819764519634, Validation loss: 0.018876457565242\n",
      "iterations:  1563\n",
      "Training loss: 0.008708190235903933, Validation loss: 0.018743257316889046\n",
      "new best w\n",
      "iterations:  1564\n",
      "Training loss: 0.008700414763601732, Validation loss: 0.0188695864305834\n",
      "iterations:  1565\n",
      "Training loss: 0.008691774767023718, Validation loss: 0.018699903103710006\n",
      "new best w\n",
      "iterations:  1566\n",
      "Training loss: 0.008684425439403417, Validation loss: 0.018869490057255744\n",
      "iterations:  1567\n",
      "Training loss: 0.008677010713790656, Validation loss: 0.018650494642002503\n",
      "new best w\n",
      "iterations:  1568\n",
      "Training loss: 0.008668768483951447, Validation loss: 0.018850332518589464\n",
      "iterations:  1569\n",
      "Training loss: 0.0086603569255609, Validation loss: 0.018616642065514125\n",
      "new best w\n",
      "iterations:  1570\n",
      "Training loss: 0.008652776210887267, Validation loss: 0.018813986739935038\n",
      "iterations:  1571\n",
      "Training loss: 0.008644744112502633, Validation loss: 0.018582252370318488\n",
      "new best w\n",
      "iterations:  1572\n",
      "Training loss: 0.008636211286652127, Validation loss: 0.018788618972186123\n",
      "iterations:  1573\n",
      "Training loss: 0.008628640352763266, Validation loss: 0.018557280241173965\n",
      "new best w\n",
      "iterations:  1574\n",
      "Training loss: 0.008619814697249673, Validation loss: 0.01872793818769024\n",
      "iterations:  1575\n",
      "Training loss: 0.008612225264929337, Validation loss: 0.018533599209715507\n",
      "new best w\n",
      "iterations:  1576\n",
      "Training loss: 0.008604322428304124, Validation loss: 0.018727646059624688\n",
      "iterations:  1577\n",
      "Training loss: 0.008596307037825207, Validation loss: 0.01848588556589886\n",
      "new best w\n",
      "iterations:  1578\n",
      "Training loss: 0.008588444458538439, Validation loss: 0.018688264045637313\n",
      "iterations:  1579\n",
      "Training loss: 0.008578683724513878, Validation loss: 0.01849428181729655\n",
      "iterations:  1580\n",
      "Training loss: 0.008570441829215679, Validation loss: 0.018598144727789513\n",
      "iterations:  1581\n",
      "Training loss: 0.00856402551199999, Validation loss: 0.01845931535494908\n",
      "new best w\n",
      "iterations:  1582\n",
      "Training loss: 0.008555101246994058, Validation loss: 0.018601479698984465\n",
      "iterations:  1583\n",
      "Training loss: 0.008546556858105596, Validation loss: 0.0184441888845746\n",
      "new best w\n",
      "iterations:  1584\n",
      "Training loss: 0.008539600755738435, Validation loss: 0.018553994525551107\n",
      "iterations:  1585\n",
      "Training loss: 0.008530711312499713, Validation loss: 0.018425475987746243\n",
      "new best w\n",
      "iterations:  1586\n",
      "Training loss: 0.008522054425739622, Validation loss: 0.01849362899725706\n",
      "iterations:  1587\n",
      "Training loss: 0.008514874038163757, Validation loss: 0.018414452426228967\n",
      "new best w\n",
      "iterations:  1588\n",
      "Training loss: 0.008507007917799007, Validation loss: 0.018459462834981292\n",
      "iterations:  1589\n",
      "Training loss: 0.008498059117876875, Validation loss: 0.018366490213831438\n",
      "new best w\n",
      "iterations:  1590\n",
      "Training loss: 0.008491212286040708, Validation loss: 0.018437573949744056\n",
      "iterations:  1591\n",
      "Training loss: 0.00848328468810639, Validation loss: 0.018348642735706036\n",
      "new best w\n",
      "iterations:  1592\n",
      "Training loss: 0.008474108326947399, Validation loss: 0.0183884480648134\n",
      "iterations:  1593\n",
      "Training loss: 0.008467003088066097, Validation loss: 0.018332906790252047\n",
      "new best w\n",
      "iterations:  1594\n",
      "Training loss: 0.008459214550150182, Validation loss: 0.01834833206230622\n",
      "iterations:  1595\n",
      "Training loss: 0.008450713014907477, Validation loss: 0.018289538959238273\n",
      "new best w\n",
      "iterations:  1596\n",
      "Training loss: 0.008443301215954286, Validation loss: 0.018348409537964044\n",
      "iterations:  1597\n",
      "Training loss: 0.008435670536932538, Validation loss: 0.018242115564814247\n",
      "new best w\n",
      "iterations:  1598\n",
      "Training loss: 0.00842754144356673, Validation loss: 0.018320817765101135\n",
      "iterations:  1599\n",
      "Training loss: 0.00841932719907623, Validation loss: 0.018222274963270984\n",
      "new best w\n",
      "iterations:  1600\n",
      "Training loss: 0.008411701787355029, Validation loss: 0.018264336198405926\n",
      "iterations:  1601\n",
      "Training loss: 0.008403727352732428, Validation loss: 0.018192391352108574\n",
      "new best w\n",
      "iterations:  1602\n",
      "Training loss: 0.008396065855021156, Validation loss: 0.018270395884367928\n",
      "iterations:  1603\n",
      "Training loss: 0.008388059384105107, Validation loss: 0.018152881826589883\n",
      "new best w\n",
      "iterations:  1604\n",
      "Training loss: 0.008379953789303542, Validation loss: 0.01819792634046214\n",
      "iterations:  1605\n",
      "Training loss: 0.008372218804391469, Validation loss: 0.018147397381477824\n",
      "new best w\n",
      "iterations:  1606\n",
      "Training loss: 0.008364264512368045, Validation loss: 0.018178763243887175\n",
      "iterations:  1607\n",
      "Training loss: 0.008356415981864354, Validation loss: 0.018093885903926597\n",
      "new best w\n",
      "iterations:  1608\n",
      "Training loss: 0.008349189371355908, Validation loss: 0.01815816470078934\n",
      "iterations:  1609\n",
      "Training loss: 0.008340590680659342, Validation loss: 0.018067672839197208\n",
      "new best w\n",
      "iterations:  1610\n",
      "Training loss: 0.008332702097958663, Validation loss: 0.018108393410227214\n",
      "iterations:  1611\n",
      "Training loss: 0.008325574546232218, Validation loss: 0.018056082043815383\n",
      "new best w\n",
      "iterations:  1612\n",
      "Training loss: 0.008317114112759342, Validation loss: 0.01809579542710437\n",
      "iterations:  1613\n",
      "Training loss: 0.008309490072464032, Validation loss: 0.018000448546746053\n",
      "new best w\n",
      "iterations:  1614\n",
      "Training loss: 0.008302204349527569, Validation loss: 0.018058638875320783\n",
      "iterations:  1615\n",
      "Training loss: 0.008293793107846418, Validation loss: 0.01797812170444322\n",
      "new best w\n",
      "iterations:  1616\n",
      "Training loss: 0.008285861788082709, Validation loss: 0.018029397307615123\n",
      "iterations:  1617\n",
      "Training loss: 0.008278640826885581, Validation loss: 0.017945488772820692\n",
      "new best w\n",
      "iterations:  1618\n",
      "Training loss: 0.008270815360418484, Validation loss: 0.018005042142261847\n",
      "iterations:  1619\n",
      "Training loss: 0.008262185747809181, Validation loss: 0.01791020328385069\n",
      "new best w\n",
      "iterations:  1620\n",
      "Training loss: 0.008255227498770475, Validation loss: 0.01796923287880003\n",
      "iterations:  1621\n",
      "Training loss: 0.008247372705045462, Validation loss: 0.01788463683787797\n",
      "new best w\n",
      "iterations:  1622\n",
      "Training loss: 0.008238763289034001, Validation loss: 0.017940747451749568\n",
      "iterations:  1623\n",
      "Training loss: 0.008231920462197792, Validation loss: 0.017857076113017168\n",
      "new best w\n",
      "iterations:  1624\n",
      "Training loss: 0.008224308724602683, Validation loss: 0.017917222571960396\n",
      "iterations:  1625\n",
      "Training loss: 0.008215924163506388, Validation loss: 0.017808873461586366\n",
      "new best w\n",
      "iterations:  1626\n",
      "Training loss: 0.008208750883542814, Validation loss: 0.017887758253905123\n",
      "iterations:  1627\n",
      "Training loss: 0.00820095128155419, Validation loss: 0.017779578052117814\n",
      "new best w\n",
      "iterations:  1628\n",
      "Training loss: 0.008192662355817457, Validation loss: 0.01785320194167438\n",
      "iterations:  1629\n",
      "Training loss: 0.008184947759675688, Validation loss: 0.017771080363021628\n",
      "new best w\n",
      "iterations:  1630\n",
      "Training loss: 0.008177521745392402, Validation loss: 0.01781783305267941\n",
      "iterations:  1631\n",
      "Training loss: 0.008169778965400772, Validation loss: 0.017720883509779652\n",
      "new best w\n",
      "iterations:  1632\n",
      "Training loss: 0.008161728585406725, Validation loss: 0.017793595378653868\n",
      "iterations:  1633\n",
      "Training loss: 0.008154315892076628, Validation loss: 0.01769402825193778\n",
      "new best w\n",
      "iterations:  1634\n",
      "Training loss: 0.008146887861143145, Validation loss: 0.017763586754132234\n",
      "iterations:  1635\n",
      "Training loss: 0.008138882278793334, Validation loss: 0.01766228843513989\n",
      "new best w\n",
      "iterations:  1636\n",
      "Training loss: 0.008131526022054773, Validation loss: 0.017744827318551988\n",
      "iterations:  1637\n",
      "Training loss: 0.008124007306239989, Validation loss: 0.01761112803647347\n",
      "new best w\n",
      "iterations:  1638\n",
      "Training loss: 0.008117520295558052, Validation loss: 0.017770223359060377\n",
      "iterations:  1639\n",
      "Training loss: 0.00810942572001088, Validation loss: 0.01754512359086002\n",
      "new best w\n",
      "iterations:  1640\n",
      "Training loss: 0.008103286694816116, Validation loss: 0.017760911200319755\n",
      "iterations:  1641\n",
      "Training loss: 0.008095663214897054, Validation loss: 0.017495978207418852\n",
      "new best w\n",
      "iterations:  1642\n",
      "Training loss: 0.008087393186807769, Validation loss: 0.01773631378931958\n",
      "iterations:  1643\n",
      "Training loss: 0.008080845215577547, Validation loss: 0.01745149654963086\n",
      "new best w\n",
      "iterations:  1644\n",
      "Training loss: 0.008073356670445874, Validation loss: 0.01772135279921858\n",
      "iterations:  1645\n",
      "Training loss: 0.008064455967490973, Validation loss: 0.017427558341539414\n",
      "new best w\n",
      "iterations:  1646\n",
      "Training loss: 0.008057574796091862, Validation loss: 0.017679646362636088\n",
      "iterations:  1647\n",
      "Training loss: 0.008050460696913455, Validation loss: 0.017392301964596728\n",
      "new best w\n",
      "iterations:  1648\n",
      "Training loss: 0.008041309365994178, Validation loss: 0.01764921939037592\n",
      "iterations:  1649\n",
      "Training loss: 0.008034682798502373, Validation loss: 0.017369712278794937\n",
      "new best w\n",
      "iterations:  1650\n",
      "Training loss: 0.008027051252466584, Validation loss: 0.01761711794872562\n",
      "iterations:  1651\n",
      "Training loss: 0.008017643753723866, Validation loss: 0.017363040522817015\n",
      "new best w\n",
      "iterations:  1652\n",
      "Training loss: 0.008010661892791175, Validation loss: 0.017560002209558587\n",
      "iterations:  1653\n",
      "Training loss: 0.008002037656044082, Validation loss: 0.017348640692573462\n",
      "new best w\n",
      "iterations:  1654\n",
      "Training loss: 0.007993876444082232, Validation loss: 0.017472814781097877\n",
      "iterations:  1655\n",
      "Training loss: 0.007985768231135455, Validation loss: 0.017367751094013312\n",
      "iterations:  1656\n",
      "Training loss: 0.007978474620637837, Validation loss: 0.017420793777040595\n",
      "iterations:  1657\n",
      "Training loss: 0.00797092091206127, Validation loss: 0.017334307008827356\n",
      "new best w\n",
      "iterations:  1658\n",
      "Training loss: 0.007963322109782291, Validation loss: 0.017419284791326085\n",
      "iterations:  1659\n",
      "Training loss: 0.007956006383867814, Validation loss: 0.017294435890442224\n",
      "new best w\n",
      "iterations:  1660\n",
      "Training loss: 0.00794833170171588, Validation loss: 0.017372915756002243\n",
      "iterations:  1661\n",
      "Training loss: 0.007940548712197399, Validation loss: 0.01728213299700228\n",
      "new best w\n",
      "iterations:  1662\n",
      "Training loss: 0.00793308117194091, Validation loss: 0.017340951622925628\n",
      "iterations:  1663\n",
      "Training loss: 0.007926032750362678, Validation loss: 0.01722900298427741\n",
      "new best w\n",
      "iterations:  1664\n",
      "Training loss: 0.007918577947819293, Validation loss: 0.01734232314208515\n",
      "iterations:  1665\n",
      "Training loss: 0.007910340857240114, Validation loss: 0.017204428626802486\n",
      "new best w\n",
      "iterations:  1666\n",
      "Training loss: 0.007903068733182955, Validation loss: 0.01728287644659034\n",
      "iterations:  1667\n",
      "Training loss: 0.007895658450307253, Validation loss: 0.01719160822890058\n",
      "new best w\n",
      "iterations:  1668\n",
      "Training loss: 0.007887598188027917, Validation loss: 0.01725608490814089\n",
      "iterations:  1669\n",
      "Training loss: 0.00788049210059052, Validation loss: 0.01714950498195317\n",
      "new best w\n",
      "iterations:  1670\n",
      "Training loss: 0.00787412743246132, Validation loss: 0.017263598127561654\n",
      "iterations:  1671\n",
      "Training loss: 0.007866468114168293, Validation loss: 0.017073113124714247\n",
      "new best w\n",
      "iterations:  1672\n",
      "Training loss: 0.007860660044637886, Validation loss: 0.017280147221558635\n",
      "iterations:  1673\n",
      "Training loss: 0.007853645504396269, Validation loss: 0.017023933214559115\n",
      "new best w\n",
      "iterations:  1674\n",
      "Training loss: 0.007844934697628097, Validation loss: 0.017258639677664486\n",
      "iterations:  1675\n",
      "Training loss: 0.00783864061151355, Validation loss: 0.016987207279558443\n",
      "new best w\n",
      "iterations:  1676\n",
      "Training loss: 0.00783154180754166, Validation loss: 0.01724039453015012\n",
      "iterations:  1677\n",
      "Training loss: 0.007823695173108044, Validation loss: 0.01695081651739826\n",
      "new best w\n",
      "iterations:  1678\n",
      "Training loss: 0.007816343188391368, Validation loss: 0.01721909679526445\n",
      "iterations:  1679\n",
      "Training loss: 0.0078100538541023435, Validation loss: 0.016905298005975112\n",
      "new best w\n",
      "iterations:  1680\n",
      "Training loss: 0.007801198648313756, Validation loss: 0.017187077466652084\n",
      "iterations:  1681\n",
      "Training loss: 0.007794806558919569, Validation loss: 0.016879955408932848\n",
      "new best w\n",
      "iterations:  1682\n",
      "Training loss: 0.007787120479206881, Validation loss: 0.01716596847358088\n",
      "iterations:  1683\n",
      "Training loss: 0.007778035042584985, Validation loss: 0.016877891053875722\n",
      "new best w\n",
      "iterations:  1684\n",
      "Training loss: 0.007770380743796945, Validation loss: 0.017091586249330937\n",
      "iterations:  1685\n",
      "Training loss: 0.007761944278510026, Validation loss: 0.016877371931563093\n",
      "new best w\n",
      "iterations:  1686\n",
      "Training loss: 0.007754625122163971, Validation loss: 0.017013497208580475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1687\n",
      "Training loss: 0.007746376787403109, Validation loss: 0.016882070521331907\n",
      "iterations:  1688\n",
      "Training loss: 0.007739478533148431, Validation loss: 0.016974654477747056\n",
      "iterations:  1689\n",
      "Training loss: 0.007732299151069255, Validation loss: 0.016836209488838436\n",
      "new best w\n",
      "iterations:  1690\n",
      "Training loss: 0.007724995988344646, Validation loss: 0.016968609451649026\n",
      "iterations:  1691\n",
      "Training loss: 0.007717042135433195, Validation loss: 0.0168067591066799\n",
      "new best w\n",
      "iterations:  1692\n",
      "Training loss: 0.00771175722032072, Validation loss: 0.01696646305763711\n",
      "iterations:  1693\n",
      "Training loss: 0.007704470474167498, Validation loss: 0.016734176739851715\n",
      "new best w\n",
      "iterations:  1694\n",
      "Training loss: 0.0076972739487898055, Validation loss: 0.016966148763631388\n",
      "iterations:  1695\n",
      "Training loss: 0.007690807676978825, Validation loss: 0.01668422429447351\n",
      "new best w\n",
      "iterations:  1696\n",
      "Training loss: 0.007683986016768557, Validation loss: 0.016962973448927537\n",
      "iterations:  1697\n",
      "Training loss: 0.007675872301179003, Validation loss: 0.016649199763312226\n",
      "new best w\n",
      "iterations:  1698\n",
      "Training loss: 0.007668937666429483, Validation loss: 0.016926616535842202\n",
      "iterations:  1699\n",
      "Training loss: 0.0076622982941962235, Validation loss: 0.016615651251654793\n",
      "new best w\n",
      "iterations:  1700\n",
      "Training loss: 0.007653314426626656, Validation loss: 0.01689261826279611\n",
      "iterations:  1701\n",
      "Training loss: 0.0076478015523061445, Validation loss: 0.016589385473830577\n",
      "new best w\n",
      "iterations:  1702\n",
      "Training loss: 0.007640312315985778, Validation loss: 0.016881200027000328\n",
      "iterations:  1703\n",
      "Training loss: 0.0076308353730914925, Validation loss: 0.016578732740021744\n",
      "new best w\n",
      "iterations:  1704\n",
      "Training loss: 0.007624150418487095, Validation loss: 0.016808614079953273\n",
      "iterations:  1705\n",
      "Training loss: 0.007615569517410499, Validation loss: 0.016575699334412042\n",
      "new best w\n",
      "iterations:  1706\n",
      "Training loss: 0.007607487817588343, Validation loss: 0.016737536233640778\n",
      "iterations:  1707\n",
      "Training loss: 0.007599904851511282, Validation loss: 0.016589016219051244\n",
      "iterations:  1708\n",
      "Training loss: 0.007592995559996054, Validation loss: 0.016682942893454632\n",
      "iterations:  1709\n",
      "Training loss: 0.007585572192380982, Validation loss: 0.0165481412048365\n",
      "new best w\n",
      "iterations:  1710\n",
      "Training loss: 0.007578959292227468, Validation loss: 0.016680703480213072\n",
      "iterations:  1711\n",
      "Training loss: 0.00757146562384501, Validation loss: 0.016509602148268347\n",
      "new best w\n",
      "iterations:  1712\n",
      "Training loss: 0.007565828217702745, Validation loss: 0.01668980508520469\n",
      "iterations:  1713\n",
      "Training loss: 0.007558373942046116, Validation loss: 0.016439553629550394\n",
      "new best w\n",
      "iterations:  1714\n",
      "Training loss: 0.007551259995258952, Validation loss: 0.01667035071787983\n",
      "iterations:  1715\n",
      "Training loss: 0.0075451782319440105, Validation loss: 0.016401763534699854\n",
      "new best w\n",
      "iterations:  1716\n",
      "Training loss: 0.007537775956892572, Validation loss: 0.01667021973778294\n",
      "iterations:  1717\n",
      "Training loss: 0.0075314043894595785, Validation loss: 0.01635203553874282\n",
      "new best w\n",
      "iterations:  1718\n",
      "Training loss: 0.007523964041369589, Validation loss: 0.016649160277865248\n",
      "iterations:  1719\n",
      "Training loss: 0.007516583523775125, Validation loss: 0.016325209601311542\n",
      "new best w\n",
      "iterations:  1720\n",
      "Training loss: 0.0075082520993733845, Validation loss: 0.016601573831265114\n",
      "iterations:  1721\n",
      "Training loss: 0.007500240901680598, Validation loss: 0.016351690932906233\n",
      "iterations:  1722\n",
      "Training loss: 0.0074919814958930185, Validation loss: 0.016519780126645463\n",
      "iterations:  1723\n",
      "Training loss: 0.007485326631954923, Validation loss: 0.01632996721102807\n",
      "iterations:  1724\n",
      "Training loss: 0.007478540640024822, Validation loss: 0.016495828585091334\n",
      "iterations:  1725\n",
      "Training loss: 0.0074701936796625, Validation loss: 0.016308765961674875\n",
      "new best w\n",
      "iterations:  1726\n",
      "Training loss: 0.007463277103674426, Validation loss: 0.016441425509258544\n",
      "iterations:  1727\n",
      "Training loss: 0.007456442197720617, Validation loss: 0.016290686413232468\n",
      "new best w\n",
      "iterations:  1728\n",
      "Training loss: 0.007448347198921021, Validation loss: 0.01640875663252112\n",
      "iterations:  1729\n",
      "Training loss: 0.007441844330558556, Validation loss: 0.016260099676187807\n",
      "new best w\n",
      "iterations:  1730\n",
      "Training loss: 0.007436196320258448, Validation loss: 0.016424770149621605\n",
      "iterations:  1731\n",
      "Training loss: 0.0074293607575100775, Validation loss: 0.01618315946313676\n",
      "new best w\n",
      "iterations:  1732\n",
      "Training loss: 0.007422789608840128, Validation loss: 0.016434318465684484\n",
      "iterations:  1733\n",
      "Training loss: 0.007416586052972644, Validation loss: 0.016130873445721512\n",
      "new best w\n",
      "iterations:  1734\n",
      "Training loss: 0.00740813087284169, Validation loss: 0.0164045701751945\n",
      "iterations:  1735\n",
      "Training loss: 0.00740233987491004, Validation loss: 0.016105128943517826\n",
      "new best w\n",
      "iterations:  1736\n",
      "Training loss: 0.007394752410397297, Validation loss: 0.016390096666681148\n",
      "iterations:  1737\n",
      "Training loss: 0.007388974937573447, Validation loss: 0.016064051862506992\n",
      "new best w\n",
      "iterations:  1738\n",
      "Training loss: 0.007380295258342516, Validation loss: 0.01636604372707435\n",
      "iterations:  1739\n",
      "Training loss: 0.007372308290932603, Validation loss: 0.016071695804973047\n",
      "iterations:  1740\n",
      "Training loss: 0.0073638740380980675, Validation loss: 0.016268969656904907\n",
      "iterations:  1741\n",
      "Training loss: 0.007356128055025589, Validation loss: 0.01610041073065294\n",
      "iterations:  1742\n",
      "Training loss: 0.00734904179169118, Validation loss: 0.016205492039153883\n",
      "iterations:  1743\n",
      "Training loss: 0.007342393146438302, Validation loss: 0.016078070655877436\n",
      "iterations:  1744\n",
      "Training loss: 0.0073346982441184205, Validation loss: 0.01618410184605802\n",
      "iterations:  1745\n",
      "Training loss: 0.007328308943879818, Validation loss: 0.016040058640179104\n",
      "new best w\n",
      "iterations:  1746\n",
      "Training loss: 0.007322510021912016, Validation loss: 0.016198066084520034\n",
      "iterations:  1747\n",
      "Training loss: 0.0073159898795109824, Validation loss: 0.0159597044893259\n",
      "new best w\n",
      "iterations:  1748\n",
      "Training loss: 0.007308822763774203, Validation loss: 0.016192780078903155\n",
      "iterations:  1749\n",
      "Training loss: 0.007303810527772759, Validation loss: 0.01590668756438771\n",
      "new best w\n",
      "iterations:  1750\n",
      "Training loss: 0.00729562292884612, Validation loss: 0.016196946498545646\n",
      "iterations:  1751\n",
      "Training loss: 0.0072896102032142295, Validation loss: 0.015874764730010427\n",
      "new best w\n",
      "iterations:  1752\n",
      "Training loss: 0.007281645974080065, Validation loss: 0.016157157226309567\n",
      "iterations:  1753\n",
      "Training loss: 0.007275940529756937, Validation loss: 0.01584449709201085\n",
      "new best w\n",
      "iterations:  1754\n",
      "Training loss: 0.007266884004416262, Validation loss: 0.01612856748134061\n",
      "iterations:  1755\n",
      "Training loss: 0.007260275134563411, Validation loss: 0.01584984124146908\n",
      "iterations:  1756\n",
      "Training loss: 0.007253274148356176, Validation loss: 0.016085893946885504\n",
      "iterations:  1757\n",
      "Training loss: 0.007245802960371619, Validation loss: 0.01581618160657628\n",
      "new best w\n",
      "iterations:  1758\n",
      "Training loss: 0.007240004635033558, Validation loss: 0.016070176289644245\n",
      "iterations:  1759\n",
      "Training loss: 0.007233040218431509, Validation loss: 0.015775116816081732\n",
      "new best w\n",
      "iterations:  1760\n",
      "Training loss: 0.007226065073978149, Validation loss: 0.01605389820955397\n",
      "iterations:  1761\n",
      "Training loss: 0.007220089448392389, Validation loss: 0.015732412373405672\n",
      "new best w\n",
      "iterations:  1762\n",
      "Training loss: 0.007211441709615601, Validation loss: 0.01601839246363455\n",
      "iterations:  1763\n",
      "Training loss: 0.007205981735929171, Validation loss: 0.01570885413323115\n",
      "new best w\n",
      "iterations:  1764\n",
      "Training loss: 0.007198133941777962, Validation loss: 0.016000540767759384\n",
      "iterations:  1765\n",
      "Training loss: 0.007192468582914868, Validation loss: 0.01567656487916154\n",
      "new best w\n",
      "iterations:  1766\n",
      "Training loss: 0.0071837349095515615, Validation loss: 0.015971717911266847\n",
      "iterations:  1767\n",
      "Training loss: 0.007176329627254892, Validation loss: 0.015678763021853205\n",
      "iterations:  1768\n",
      "Training loss: 0.007167635019199181, Validation loss: 0.01587795818512404\n",
      "iterations:  1769\n",
      "Training loss: 0.007160033682411379, Validation loss: 0.015720490601895634\n",
      "iterations:  1770\n",
      "Training loss: 0.0071537209414862316, Validation loss: 0.015834319190343983\n",
      "iterations:  1771\n",
      "Training loss: 0.007147523402939775, Validation loss: 0.015658143720751777\n",
      "new best w\n",
      "iterations:  1772\n",
      "Training loss: 0.007140851180921555, Validation loss: 0.0158525801304015\n",
      "iterations:  1773\n",
      "Training loss: 0.007135054887358997, Validation loss: 0.015591450364461598\n",
      "new best w\n",
      "iterations:  1774\n",
      "Training loss: 0.007128280942443954, Validation loss: 0.01584696224402037\n",
      "iterations:  1775\n",
      "Training loss: 0.0071217983601426105, Validation loss: 0.01554807438357049\n",
      "new best w\n",
      "iterations:  1776\n",
      "Training loss: 0.007113884792956928, Validation loss: 0.015815064229720323\n",
      "iterations:  1777\n",
      "Training loss: 0.00710995344256228, Validation loss: 0.015503296776320188\n",
      "new best w\n",
      "iterations:  1778\n",
      "Training loss: 0.007100988103428411, Validation loss: 0.015819826991673377\n",
      "iterations:  1779\n",
      "Training loss: 0.007093994864210006, Validation loss: 0.015496285242373004\n",
      "new best w\n",
      "iterations:  1780\n",
      "Training loss: 0.007085887855355026, Validation loss: 0.015747232732586686\n",
      "iterations:  1781\n",
      "Training loss: 0.007077722170183639, Validation loss: 0.015518470823104074\n",
      "iterations:  1782\n",
      "Training loss: 0.007072206225744959, Validation loss: 0.015710771891379108\n",
      "iterations:  1783\n",
      "Training loss: 0.007065725522278414, Validation loss: 0.015462305008982496\n",
      "new best w\n",
      "iterations:  1784\n",
      "Training loss: 0.007057590401070533, Validation loss: 0.015687962638166082\n",
      "iterations:  1785\n",
      "Training loss: 0.0070523593723373, Validation loss: 0.015424918566408338\n",
      "new best w\n",
      "iterations:  1786\n",
      "Training loss: 0.007045481664281387, Validation loss: 0.015675855709579925\n",
      "iterations:  1787\n",
      "Training loss: 0.007039139801036669, Validation loss: 0.015386770631526478\n",
      "new best w\n",
      "iterations:  1788\n",
      "Training loss: 0.007031550345762732, Validation loss: 0.015659578816920268\n",
      "iterations:  1789\n",
      "Training loss: 0.007026860051126825, Validation loss: 0.015339116176905481\n",
      "new best w\n",
      "iterations:  1790\n",
      "Training loss: 0.007017037899369909, Validation loss: 0.015632432700531714\n",
      "iterations:  1791\n",
      "Training loss: 0.007009943654158427, Validation loss: 0.015367113981273819\n",
      "iterations:  1792\n",
      "Training loss: 0.007003576099632236, Validation loss: 0.015566435951358388\n",
      "iterations:  1793\n",
      "Training loss: 0.006996018015385913, Validation loss: 0.01534748515338633\n",
      "iterations:  1794\n",
      "Training loss: 0.006990217463817469, Validation loss: 0.015559677782703105\n",
      "iterations:  1795\n",
      "Training loss: 0.006984080505888018, Validation loss: 0.015285970554432218\n",
      "new best w\n",
      "iterations:  1796\n",
      "Training loss: 0.0069767157510247976, Validation loss: 0.01554253125612376\n",
      "iterations:  1797\n",
      "Training loss: 0.006970799901151222, Validation loss: 0.01525259189974651\n",
      "new best w\n",
      "iterations:  1798\n",
      "Training loss: 0.006962496731797122, Validation loss: 0.015500318407214326\n",
      "iterations:  1799\n",
      "Training loss: 0.0069578486170560625, Validation loss: 0.015224433788574155\n",
      "new best w\n",
      "iterations:  1800\n",
      "Training loss: 0.0069500101015285455, Validation loss: 0.015501584381932368\n",
      "iterations:  1801\n",
      "Training loss: 0.006943154702578926, Validation loss: 0.015199928666915375\n",
      "new best w\n",
      "iterations:  1802\n",
      "Training loss: 0.0069351980396177995, Validation loss: 0.015438251009300153\n",
      "iterations:  1803\n",
      "Training loss: 0.0069271121203438426, Validation loss: 0.015231544206692163\n",
      "iterations:  1804\n",
      "Training loss: 0.006921703717357028, Validation loss: 0.015390409487494197\n",
      "iterations:  1805\n",
      "Training loss: 0.006915205608219497, Validation loss: 0.015181465100901423\n",
      "new best w\n",
      "iterations:  1806\n",
      "Training loss: 0.006907727206746905, Validation loss: 0.015376397867557814\n",
      "iterations:  1807\n",
      "Training loss: 0.006903207683227711, Validation loss: 0.01512269980824638\n",
      "new best w\n",
      "iterations:  1808\n",
      "Training loss: 0.006895890248518366, Validation loss: 0.015384595926366798\n",
      "iterations:  1809\n",
      "Training loss: 0.006888202233006129, Validation loss: 0.015109359082395201\n",
      "new best w\n",
      "iterations:  1810\n",
      "Training loss: 0.006880893931944734, Validation loss: 0.015311853993132363\n",
      "iterations:  1811\n",
      "Training loss: 0.006873555034562502, Validation loss: 0.01513163655268456\n",
      "iterations:  1812\n",
      "Training loss: 0.006866363713054196, Validation loss: 0.015238233295259487\n",
      "iterations:  1813\n",
      "Training loss: 0.006859681957768586, Validation loss: 0.015140344175603528\n",
      "iterations:  1814\n",
      "Training loss: 0.006852973444349415, Validation loss: 0.015213614058845276\n",
      "iterations:  1815\n",
      "Training loss: 0.006846819390685089, Validation loss: 0.015081279899894894\n",
      "new best w\n",
      "iterations:  1816\n",
      "Training loss: 0.006840887037678339, Validation loss: 0.015222168932937902\n",
      "iterations:  1817\n",
      "Training loss: 0.00683416413950447, Validation loss: 0.015027250623743299\n",
      "new best w\n",
      "iterations:  1818\n",
      "Training loss: 0.006826792712575893, Validation loss: 0.015188547078532458\n",
      "iterations:  1819\n",
      "Training loss: 0.0068200364188301795, Validation loss: 0.015026744652729962\n",
      "new best w\n",
      "iterations:  1820\n",
      "Training loss: 0.006813005022588217, Validation loss: 0.015121226080164507\n",
      "iterations:  1821\n",
      "Training loss: 0.006805963467995615, Validation loss: 0.015037203092985063\n",
      "iterations:  1822\n",
      "Training loss: 0.0067994357186511555, Validation loss: 0.015071513334525572\n",
      "iterations:  1823\n",
      "Training loss: 0.006793094512604117, Validation loss: 0.015009111296302984\n",
      "new best w\n",
      "iterations:  1824\n",
      "Training loss: 0.00678604648092076, Validation loss: 0.015054728681629293\n",
      "iterations:  1825\n",
      "Training loss: 0.006779895164523213, Validation loss: 0.014979672340622422\n",
      "new best w\n",
      "iterations:  1826\n",
      "Training loss: 0.006773544189354291, Validation loss: 0.015058980423811277\n",
      "iterations:  1827\n",
      "Training loss: 0.0067667077044220765, Validation loss: 0.014928412851601538\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1828\n",
      "Training loss: 0.006759987515700255, Validation loss: 0.015004454539898848\n",
      "iterations:  1829\n",
      "Training loss: 0.0067533499535274025, Validation loss: 0.014937683724917952\n",
      "iterations:  1830\n",
      "Training loss: 0.006746561775033022, Validation loss: 0.014970216343595655\n",
      "iterations:  1831\n",
      "Training loss: 0.006740302895629736, Validation loss: 0.01490251527787968\n",
      "new best w\n",
      "iterations:  1832\n",
      "Training loss: 0.0067335451169437, Validation loss: 0.014946950727517685\n",
      "iterations:  1833\n",
      "Training loss: 0.006726779302631444, Validation loss: 0.014878228465936557\n",
      "new best w\n",
      "iterations:  1834\n",
      "Training loss: 0.0067208938199772, Validation loss: 0.014944551664572103\n",
      "iterations:  1835\n",
      "Training loss: 0.0067153274399495725, Validation loss: 0.01479292324063917\n",
      "new best w\n",
      "iterations:  1836\n",
      "Training loss: 0.006708291407421294, Validation loss: 0.014959590152069055\n",
      "iterations:  1837\n",
      "Training loss: 0.006701896971210477, Validation loss: 0.014776935920215298\n",
      "new best w\n",
      "iterations:  1838\n",
      "Training loss: 0.006694766047636847, Validation loss: 0.014901257169486546\n",
      "iterations:  1839\n",
      "Training loss: 0.006687934177464749, Validation loss: 0.014770394964379013\n",
      "new best w\n",
      "iterations:  1840\n",
      "Training loss: 0.006681575239401872, Validation loss: 0.014857016425460962\n",
      "iterations:  1841\n",
      "Training loss: 0.006674755595701131, Validation loss: 0.014754953536096262\n",
      "new best w\n",
      "iterations:  1842\n",
      "Training loss: 0.006669188201894342, Validation loss: 0.014863881088938024\n",
      "iterations:  1843\n",
      "Training loss: 0.006663519161342573, Validation loss: 0.014678359084371474\n",
      "new best w\n",
      "iterations:  1844\n",
      "Training loss: 0.006655784950912165, Validation loss: 0.014855675044793303\n",
      "iterations:  1845\n",
      "Training loss: 0.006649114540866041, Validation loss: 0.014690059024915436\n",
      "iterations:  1846\n",
      "Training loss: 0.006642666520699312, Validation loss: 0.014775220383357313\n",
      "iterations:  1847\n",
      "Training loss: 0.006635819694950423, Validation loss: 0.014676315109611112\n",
      "new best w\n",
      "iterations:  1848\n",
      "Training loss: 0.006629593164762925, Validation loss: 0.014754161234682853\n",
      "iterations:  1849\n",
      "Training loss: 0.006623173269784625, Validation loss: 0.014643057585551764\n",
      "new best w\n",
      "iterations:  1850\n",
      "Training loss: 0.00661712828561875, Validation loss: 0.014757720967815574\n",
      "iterations:  1851\n",
      "Training loss: 0.006611253770601396, Validation loss: 0.014585889420747025\n",
      "new best w\n",
      "iterations:  1852\n",
      "Training loss: 0.006604302010448603, Validation loss: 0.014733875370359622\n",
      "iterations:  1853\n",
      "Training loss: 0.00659892893837052, Validation loss: 0.014546337139394036\n",
      "new best w\n",
      "iterations:  1854\n",
      "Training loss: 0.006592619421612973, Validation loss: 0.014747019273159028\n",
      "iterations:  1855\n",
      "Training loss: 0.006586472540663469, Validation loss: 0.014505614350233278\n",
      "new best w\n",
      "iterations:  1856\n",
      "Training loss: 0.006578658840167675, Validation loss: 0.014701877656709172\n",
      "iterations:  1857\n",
      "Training loss: 0.0065723306753734355, Validation loss: 0.014510853540482213\n",
      "iterations:  1858\n",
      "Training loss: 0.006567086394509088, Validation loss: 0.014681769737880596\n",
      "iterations:  1859\n",
      "Training loss: 0.0065600029169689475, Validation loss: 0.014464102457529649\n",
      "new best w\n",
      "iterations:  1860\n",
      "Training loss: 0.006553530827283796, Validation loss: 0.014655005335187571\n",
      "iterations:  1861\n",
      "Training loss: 0.006549154621455887, Validation loss: 0.014417693046495244\n",
      "new best w\n",
      "iterations:  1862\n",
      "Training loss: 0.006541506878957227, Validation loss: 0.014661849224646866\n",
      "iterations:  1863\n",
      "Training loss: 0.006535182534832997, Validation loss: 0.014398047921957863\n",
      "new best w\n",
      "iterations:  1864\n",
      "Training loss: 0.006528168537699468, Validation loss: 0.014602088961252799\n",
      "iterations:  1865\n",
      "Training loss: 0.006520704802128157, Validation loss: 0.014410074237937975\n",
      "iterations:  1866\n",
      "Training loss: 0.0065159716339795275, Validation loss: 0.014574995763254278\n",
      "iterations:  1867\n",
      "Training loss: 0.006510023879589166, Validation loss: 0.014348480640169478\n",
      "new best w\n",
      "iterations:  1868\n",
      "Training loss: 0.006502303667473428, Validation loss: 0.01456084242974522\n",
      "iterations:  1869\n",
      "Training loss: 0.006495803876273387, Validation loss: 0.01436085026770557\n",
      "iterations:  1870\n",
      "Training loss: 0.006489108681771862, Validation loss: 0.0144837897891715\n",
      "iterations:  1871\n",
      "Training loss: 0.006482162661647769, Validation loss: 0.014361507503434961\n",
      "iterations:  1872\n",
      "Training loss: 0.0064760890978859155, Validation loss: 0.014438804555702997\n",
      "iterations:  1873\n",
      "Training loss: 0.00646961031748584, Validation loss: 0.014354703644064385\n",
      "iterations:  1874\n",
      "Training loss: 0.0064635232411839015, Validation loss: 0.014431830218302563\n",
      "iterations:  1875\n",
      "Training loss: 0.006458068834706985, Validation loss: 0.014282919058910621\n",
      "new best w\n",
      "iterations:  1876\n",
      "Training loss: 0.006451321897156064, Validation loss: 0.014425230871436028\n",
      "iterations:  1877\n",
      "Training loss: 0.006444206442811649, Validation loss: 0.014290660807256623\n",
      "iterations:  1878\n",
      "Training loss: 0.006438110530974665, Validation loss: 0.014350220982745045\n",
      "iterations:  1879\n",
      "Training loss: 0.0064320414600898155, Validation loss: 0.014270827330542298\n",
      "new best w\n",
      "iterations:  1880\n",
      "Training loss: 0.006425298697659159, Validation loss: 0.014326735241229136\n",
      "iterations:  1881\n",
      "Training loss: 0.006419297294030254, Validation loss: 0.014251842098650751\n",
      "new best w\n",
      "iterations:  1882\n",
      "Training loss: 0.0064134279803158, Validation loss: 0.014325179375153646\n",
      "iterations:  1883\n",
      "Training loss: 0.006406551531052556, Validation loss: 0.014222049268841802\n",
      "new best w\n",
      "iterations:  1884\n",
      "Training loss: 0.006400517888116269, Validation loss: 0.014256682477511448\n",
      "iterations:  1885\n",
      "Training loss: 0.006394111864549502, Validation loss: 0.014227172509330884\n",
      "iterations:  1886\n",
      "Training loss: 0.006387651889561763, Validation loss: 0.014226694990579145\n",
      "iterations:  1887\n",
      "Training loss: 0.006382056162448027, Validation loss: 0.014180897421091196\n",
      "new best w\n",
      "iterations:  1888\n",
      "Training loss: 0.006375237968761681, Validation loss: 0.014215766224384077\n",
      "iterations:  1889\n",
      "Training loss: 0.006369051592722601, Validation loss: 0.014192334169640736\n",
      "iterations:  1890\n",
      "Training loss: 0.00636335021114797, Validation loss: 0.014161333948494684\n",
      "new best w\n",
      "iterations:  1891\n",
      "Training loss: 0.006356539987759211, Validation loss: 0.014158983466731634\n",
      "new best w\n",
      "iterations:  1892\n",
      "Training loss: 0.0063506439500036, Validation loss: 0.014134559702398821\n",
      "new best w\n",
      "iterations:  1893\n",
      "Training loss: 0.0063446673768936335, Validation loss: 0.014135310361703037\n",
      "iterations:  1894\n",
      "Training loss: 0.006337646755878378, Validation loss: 0.014109327232561236\n",
      "new best w\n",
      "iterations:  1895\n",
      "Training loss: 0.006332495055576946, Validation loss: 0.014150553663860154\n",
      "iterations:  1896\n",
      "Training loss: 0.006327368960073773, Validation loss: 0.01401633989943\n",
      "new best w\n",
      "iterations:  1897\n",
      "Training loss: 0.006320069706836264, Validation loss: 0.014161918099550752\n",
      "iterations:  1898\n",
      "Training loss: 0.006314025071410663, Validation loss: 0.014018371567590096\n",
      "iterations:  1899\n",
      "Training loss: 0.006307843250405759, Validation loss: 0.014096090623214106\n",
      "iterations:  1900\n",
      "Training loss: 0.006301101637511134, Validation loss: 0.013997928538898915\n",
      "new best w\n",
      "iterations:  1901\n",
      "Training loss: 0.006295628540889383, Validation loss: 0.014070361773538686\n",
      "iterations:  1902\n",
      "Training loss: 0.006289741164333514, Validation loss: 0.013966584065866736\n",
      "new best w\n",
      "iterations:  1903\n",
      "Training loss: 0.006283245409773264, Validation loss: 0.014078169005958588\n",
      "iterations:  1904\n",
      "Training loss: 0.006277152374865806, Validation loss: 0.013940446628634237\n",
      "new best w\n",
      "iterations:  1905\n",
      "Training loss: 0.0062707623563029195, Validation loss: 0.014010265005306777\n",
      "iterations:  1906\n",
      "Training loss: 0.006264053446813043, Validation loss: 0.013954080175094188\n",
      "iterations:  1907\n",
      "Training loss: 0.006258221874769543, Validation loss: 0.013960069219577691\n",
      "iterations:  1908\n",
      "Training loss: 0.006252410561570928, Validation loss: 0.013924531569065418\n",
      "new best w\n",
      "iterations:  1909\n",
      "Training loss: 0.006245904106360234, Validation loss: 0.01394681310326954\n",
      "iterations:  1910\n",
      "Training loss: 0.006240081150602349, Validation loss: 0.013923277008905105\n",
      "new best w\n",
      "iterations:  1911\n",
      "Training loss: 0.006233930823773505, Validation loss: 0.013908956652913227\n",
      "new best w\n",
      "iterations:  1912\n",
      "Training loss: 0.006227661363353327, Validation loss: 0.013894332537003404\n",
      "new best w\n",
      "iterations:  1913\n",
      "Training loss: 0.006222153153339912, Validation loss: 0.013859256449739149\n",
      "new best w\n",
      "iterations:  1914\n",
      "Training loss: 0.00621550046871599, Validation loss: 0.013889452793418727\n",
      "iterations:  1915\n",
      "Training loss: 0.006209556532138134, Validation loss: 0.013841646690672827\n",
      "new best w\n",
      "iterations:  1916\n",
      "Training loss: 0.006204141715960323, Validation loss: 0.013880954930146547\n",
      "iterations:  1917\n",
      "Training loss: 0.0061973878123865464, Validation loss: 0.013806017310648025\n",
      "new best w\n",
      "iterations:  1918\n",
      "Training loss: 0.0061915306731408735, Validation loss: 0.01383335878848375\n",
      "iterations:  1919\n",
      "Training loss: 0.006185682694286952, Validation loss: 0.013796386912352784\n",
      "new best w\n",
      "iterations:  1920\n",
      "Training loss: 0.006178830974726682, Validation loss: 0.013798570897451182\n",
      "iterations:  1921\n",
      "Training loss: 0.006173543526533202, Validation loss: 0.013768481718784411\n",
      "new best w\n",
      "iterations:  1922\n",
      "Training loss: 0.006167921758857179, Validation loss: 0.013772765375761133\n",
      "iterations:  1923\n",
      "Training loss: 0.006161039073347782, Validation loss: 0.013773345990398835\n",
      "iterations:  1924\n",
      "Training loss: 0.006155484793436928, Validation loss: 0.013742045258475943\n",
      "new best w\n",
      "iterations:  1925\n",
      "Training loss: 0.006149415190406005, Validation loss: 0.01373795640384203\n",
      "new best w\n",
      "iterations:  1926\n",
      "Training loss: 0.006143093814541183, Validation loss: 0.013707594653030297\n",
      "new best w\n",
      "iterations:  1927\n",
      "Training loss: 0.006137419520628558, Validation loss: 0.013721842694616371\n",
      "iterations:  1928\n",
      "Training loss: 0.006131439650570764, Validation loss: 0.013710014481424487\n",
      "iterations:  1929\n",
      "Training loss: 0.006125104058093891, Validation loss: 0.013688682295863331\n",
      "new best w\n",
      "iterations:  1930\n",
      "Training loss: 0.006119514745257181, Validation loss: 0.013668561961030857\n",
      "new best w\n",
      "iterations:  1931\n",
      "Training loss: 0.006113481865072768, Validation loss: 0.013650451954900274\n",
      "new best w\n",
      "iterations:  1932\n",
      "Training loss: 0.006107286199911528, Validation loss: 0.013669052602448934\n",
      "iterations:  1933\n",
      "Training loss: 0.006101820682966573, Validation loss: 0.013612379565898008\n",
      "new best w\n",
      "iterations:  1934\n",
      "Training loss: 0.006095707961372623, Validation loss: 0.013672813660886084\n",
      "iterations:  1935\n",
      "Training loss: 0.006089734167877622, Validation loss: 0.013583742036117664\n",
      "new best w\n",
      "iterations:  1936\n",
      "Training loss: 0.006083998543016277, Validation loss: 0.013605347781268384\n",
      "iterations:  1937\n",
      "Training loss: 0.006077375746664929, Validation loss: 0.01359097379014228\n",
      "iterations:  1938\n",
      "Training loss: 0.0060716738283707605, Validation loss: 0.013576224672857884\n",
      "new best w\n",
      "iterations:  1939\n",
      "Training loss: 0.006066520660246366, Validation loss: 0.013539035311371008\n",
      "new best w\n",
      "iterations:  1940\n",
      "Training loss: 0.006059837917377516, Validation loss: 0.013575858308854364\n",
      "iterations:  1941\n",
      "Training loss: 0.0060542424960498544, Validation loss: 0.013541124205095895\n",
      "iterations:  1942\n",
      "Training loss: 0.006048289893905483, Validation loss: 0.013538922354947538\n",
      "new best w\n",
      "iterations:  1943\n",
      "Training loss: 0.006041820808268966, Validation loss: 0.013510711212515191\n",
      "new best w\n",
      "iterations:  1944\n",
      "Training loss: 0.006036580527768112, Validation loss: 0.013493272860727379\n",
      "new best w\n",
      "iterations:  1945\n",
      "Training loss: 0.006030680588789241, Validation loss: 0.013501719013022578\n",
      "iterations:  1946\n",
      "Training loss: 0.006024370882575553, Validation loss: 0.013492915344688688\n",
      "new best w\n",
      "iterations:  1947\n",
      "Training loss: 0.006018885564838521, Validation loss: 0.013466608215659278\n",
      "new best w\n",
      "iterations:  1948\n",
      "Training loss: 0.0060128442599051615, Validation loss: 0.013455456594302534\n",
      "new best w\n",
      "iterations:  1949\n",
      "Training loss: 0.00600706493412911, Validation loss: 0.013439966664550409\n",
      "new best w\n",
      "iterations:  1950\n",
      "Training loss: 0.006001446926271011, Validation loss: 0.013435142266538067\n",
      "new best w\n",
      "iterations:  1951\n",
      "Training loss: 0.005995118060845216, Validation loss: 0.013440915296477979\n",
      "iterations:  1952\n",
      "Training loss: 0.005989436278768587, Validation loss: 0.013411420294865872\n",
      "new best w\n",
      "iterations:  1953\n",
      "Training loss: 0.005983912727978085, Validation loss: 0.013390167708987174\n",
      "new best w\n",
      "iterations:  1954\n",
      "Training loss: 0.005977525484520534, Validation loss: 0.013378273906338917\n",
      "new best w\n",
      "iterations:  1955\n",
      "Training loss: 0.005972022160285002, Validation loss: 0.013388601340271904\n",
      "iterations:  1956\n",
      "Training loss: 0.005966614382376226, Validation loss: 0.013363615848642202\n",
      "new best w\n",
      "iterations:  1957\n",
      "Training loss: 0.005960134292964773, Validation loss: 0.013361077911189937\n",
      "new best w\n",
      "iterations:  1958\n",
      "Training loss: 0.005954742144284975, Validation loss: 0.013327368879931583\n",
      "new best w\n",
      "iterations:  1959\n",
      "Training loss: 0.005948948204415288, Validation loss: 0.013325218054674926\n",
      "new best w\n",
      "iterations:  1960\n",
      "Training loss: 0.005942584113690101, Validation loss: 0.013323764754897757\n",
      "new best w\n",
      "iterations:  1961\n",
      "Training loss: 0.005937312955540413, Validation loss: 0.013316572269854984\n",
      "new best w\n",
      "iterations:  1962\n",
      "Training loss: 0.005931409026463536, Validation loss: 0.013291649981008357\n",
      "new best w\n",
      "iterations:  1963\n",
      "Training loss: 0.0059255736774200185, Validation loss: 0.013288697154455317\n",
      "new best w\n",
      "iterations:  1964\n",
      "Training loss: 0.00592023257178066, Validation loss: 0.013244736355633368\n",
      "new best w\n",
      "iterations:  1965\n",
      "Training loss: 0.005913775031177185, Validation loss: 0.013280612320507113\n",
      "iterations:  1966\n",
      "Training loss: 0.005908385173301172, Validation loss: 0.013227937354131141\n",
      "new best w\n",
      "iterations:  1967\n",
      "Training loss: 0.005903104652890008, Validation loss: 0.013271935577016742\n",
      "iterations:  1968\n",
      "Training loss: 0.005896550239123681, Validation loss: 0.0131928352055138\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1969\n",
      "Training loss: 0.00589128313164476, Validation loss: 0.013220803283906093\n",
      "iterations:  1970\n",
      "Training loss: 0.00588541653125031, Validation loss: 0.013192857397529059\n",
      "iterations:  1971\n",
      "Training loss: 0.005879396828652692, Validation loss: 0.013216951593326942\n",
      "iterations:  1972\n",
      "Training loss: 0.005874035066370484, Validation loss: 0.01316275670213037\n",
      "new best w\n",
      "iterations:  1973\n",
      "Training loss: 0.005868328689710655, Validation loss: 0.01317616528422023\n",
      "iterations:  1974\n",
      "Training loss: 0.005862328509835036, Validation loss: 0.01313234108685576\n",
      "new best w\n",
      "iterations:  1975\n",
      "Training loss: 0.005856851182244366, Validation loss: 0.013157492479440263\n",
      "iterations:  1976\n",
      "Training loss: 0.005851081825979597, Validation loss: 0.013108159513029159\n",
      "new best w\n",
      "iterations:  1977\n",
      "Training loss: 0.005845689864329592, Validation loss: 0.013171657128215969\n",
      "iterations:  1978\n",
      "Training loss: 0.005840179731889593, Validation loss: 0.01306092129274387\n",
      "new best w\n",
      "iterations:  1979\n",
      "Training loss: 0.0058337296343386165, Validation loss: 0.013117668422956336\n",
      "iterations:  1980\n",
      "Training loss: 0.005828416548481457, Validation loss: 0.013069556322405518\n",
      "iterations:  1981\n",
      "Training loss: 0.005822896730608803, Validation loss: 0.013104504355681376\n",
      "iterations:  1982\n",
      "Training loss: 0.005816684121843983, Validation loss: 0.013047194747461304\n",
      "new best w\n",
      "iterations:  1983\n",
      "Training loss: 0.005811475447231167, Validation loss: 0.013058439192033255\n",
      "iterations:  1984\n",
      "Training loss: 0.005805865169846399, Validation loss: 0.01301825242695911\n",
      "new best w\n",
      "iterations:  1985\n",
      "Training loss: 0.005799728570239216, Validation loss: 0.013046588604228127\n",
      "iterations:  1986\n",
      "Training loss: 0.005794546825033763, Validation loss: 0.013016578439551656\n",
      "new best w\n",
      "iterations:  1987\n",
      "Training loss: 0.005788593885518983, Validation loss: 0.013008204993057396\n",
      "new best w\n",
      "iterations:  1988\n",
      "Training loss: 0.005783076630779579, Validation loss: 0.012994301440203087\n",
      "new best w\n",
      "iterations:  1989\n",
      "Training loss: 0.005777876959080826, Validation loss: 0.012958718587036044\n",
      "new best w\n",
      "iterations:  1990\n",
      "Training loss: 0.005771474443694108, Validation loss: 0.012988452402395049\n",
      "iterations:  1991\n",
      "Training loss: 0.005766397302023634, Validation loss: 0.012962154103515616\n",
      "iterations:  1992\n",
      "Training loss: 0.005760720444557585, Validation loss: 0.012955880959633551\n",
      "new best w\n",
      "iterations:  1993\n",
      "Training loss: 0.005754647694431809, Validation loss: 0.012930540569256471\n",
      "new best w\n",
      "iterations:  1994\n",
      "Training loss: 0.00574967685203828, Validation loss: 0.012908057407823267\n",
      "new best w\n",
      "iterations:  1995\n",
      "Training loss: 0.005743906614433421, Validation loss: 0.012922892279511489\n",
      "iterations:  1996\n",
      "Training loss: 0.005738208504962663, Validation loss: 0.012919537933112166\n",
      "iterations:  1997\n",
      "Training loss: 0.005732930827778334, Validation loss: 0.012879074348452688\n",
      "new best w\n",
      "iterations:  1998\n",
      "Training loss: 0.005726742433933084, Validation loss: 0.012885480188649398\n",
      "iterations:  1999\n",
      "Training loss: 0.005721669174589327, Validation loss: 0.012852535475941684\n",
      "new best w\n",
      "iterations:  2000\n",
      "Training loss: 0.005716514226745191, Validation loss: 0.012859326534903797\n",
      "iterations:  2001\n",
      "Training loss: 0.005710226922299429, Validation loss: 0.012864349320303847\n",
      "iterations:  2002\n",
      "Training loss: 0.005704932317266325, Validation loss: 0.012824982376699\n",
      "new best w\n",
      "iterations:  2003\n",
      "Training loss: 0.00569931558092788, Validation loss: 0.012828460783859522\n",
      "iterations:  2004\n",
      "Training loss: 0.005693671468678947, Validation loss: 0.012842474433547038\n",
      "iterations:  2005\n",
      "Training loss: 0.005688573467720171, Validation loss: 0.012776187490491666\n",
      "new best w\n",
      "iterations:  2006\n",
      "Training loss: 0.005682716426095901, Validation loss: 0.012794576713596836\n",
      "iterations:  2007\n",
      "Training loss: 0.005677094750115732, Validation loss: 0.012779120486973088\n",
      "iterations:  2008\n",
      "Training loss: 0.005672066054274424, Validation loss: 0.01278090971814905\n",
      "iterations:  2009\n",
      "Training loss: 0.005665854395018055, Validation loss: 0.012754200397457092\n",
      "new best w\n",
      "iterations:  2010\n",
      "Training loss: 0.005660882476031206, Validation loss: 0.012749296237079055\n",
      "new best w\n",
      "iterations:  2011\n",
      "Training loss: 0.0056556940911077465, Validation loss: 0.01270578703290661\n",
      "new best w\n",
      "iterations:  2012\n",
      "Training loss: 0.00564953257278827, Validation loss: 0.01274911641723894\n",
      "iterations:  2013\n",
      "Training loss: 0.005644422704253464, Validation loss: 0.01270522855731109\n",
      "new best w\n",
      "iterations:  2014\n",
      "Training loss: 0.005638687709392528, Validation loss: 0.012710632254803752\n",
      "iterations:  2015\n",
      "Training loss: 0.005633073781297333, Validation loss: 0.01268515987265936\n",
      "new best w\n",
      "iterations:  2016\n",
      "Training loss: 0.005628211832505923, Validation loss: 0.012658764784376077\n",
      "new best w\n",
      "iterations:  2017\n",
      "Training loss: 0.005622101565444395, Validation loss: 0.012681354147069127\n",
      "iterations:  2018\n",
      "Training loss: 0.005616985720237156, Validation loss: 0.01266464974009284\n",
      "iterations:  2019\n",
      "Training loss: 0.005611750223459471, Validation loss: 0.01263064790966888\n",
      "new best w\n",
      "iterations:  2020\n",
      "Training loss: 0.005605592019729021, Validation loss: 0.012640558046842687\n",
      "iterations:  2021\n",
      "Training loss: 0.005600672698020566, Validation loss: 0.012642967458897817\n",
      "iterations:  2022\n",
      "Training loss: 0.005595093064040385, Validation loss: 0.012594671217533927\n",
      "new best w\n",
      "iterations:  2023\n",
      "Training loss: 0.00558921969443044, Validation loss: 0.01260001358142845\n",
      "iterations:  2024\n",
      "Training loss: 0.0055840900006888265, Validation loss: 0.012585523616986321\n",
      "new best w\n",
      "iterations:  2025\n",
      "Training loss: 0.005578202330694628, Validation loss: 0.012598136850386812\n",
      "iterations:  2026\n",
      "Training loss: 0.00557294673616893, Validation loss: 0.012563244981257165\n",
      "new best w\n",
      "iterations:  2027\n",
      "Training loss: 0.0055678406668323655, Validation loss: 0.012545217171353923\n",
      "new best w\n",
      "iterations:  2028\n",
      "Training loss: 0.005561852185745754, Validation loss: 0.012538858215826131\n",
      "new best w\n",
      "iterations:  2029\n",
      "Training loss: 0.0055566765605744636, Validation loss: 0.01253503468151162\n",
      "new best w\n",
      "iterations:  2030\n",
      "Training loss: 0.005551187632548118, Validation loss: 0.012537098950993008\n",
      "iterations:  2031\n",
      "Training loss: 0.005545246362060485, Validation loss: 0.012505477212360847\n",
      "new best w\n",
      "iterations:  2032\n",
      "Training loss: 0.005540303749786222, Validation loss: 0.012493718006230344\n",
      "new best w\n",
      "iterations:  2033\n",
      "Training loss: 0.005534625534160684, Validation loss: 0.012519729226598586\n",
      "iterations:  2034\n",
      "Training loss: 0.005529503859204027, Validation loss: 0.012457329620996467\n",
      "new best w\n",
      "iterations:  2035\n",
      "Training loss: 0.005524314078795849, Validation loss: 0.012458928377978868\n",
      "iterations:  2036\n",
      "Training loss: 0.005518152714883232, Validation loss: 0.012467198638952837\n",
      "iterations:  2037\n",
      "Training loss: 0.0055131811255264675, Validation loss: 0.01245168908300077\n",
      "new best w\n",
      "iterations:  2038\n",
      "Training loss: 0.005507584275160991, Validation loss: 0.01243372008456634\n",
      "new best w\n",
      "iterations:  2039\n",
      "Training loss: 0.0055020852008894885, Validation loss: 0.012427033781319314\n",
      "new best w\n",
      "iterations:  2040\n",
      "Training loss: 0.005497207582717045, Validation loss: 0.012385867923162305\n",
      "new best w\n",
      "iterations:  2041\n",
      "Training loss: 0.005491156563749849, Validation loss: 0.01242064032972706\n",
      "iterations:  2042\n",
      "Training loss: 0.005486123259656873, Validation loss: 0.012393609983881018\n",
      "iterations:  2043\n",
      "Training loss: 0.005480874056638565, Validation loss: 0.012371434912120383\n",
      "new best w\n",
      "iterations:  2044\n",
      "Training loss: 0.005475008371046036, Validation loss: 0.012370869524948518\n",
      "new best w\n",
      "iterations:  2045\n",
      "Training loss: 0.0054701976820357625, Validation loss: 0.0123815021053117\n",
      "iterations:  2046\n",
      "Training loss: 0.005464611090021632, Validation loss: 0.012319440843452583\n",
      "new best w\n",
      "iterations:  2047\n",
      "Training loss: 0.00545925401566704, Validation loss: 0.01234926831390851\n",
      "iterations:  2048\n",
      "Training loss: 0.005454174323597695, Validation loss: 0.012307064696989937\n",
      "new best w\n",
      "iterations:  2049\n",
      "Training loss: 0.005448262815050115, Validation loss: 0.01234344867035374\n",
      "iterations:  2050\n",
      "Training loss: 0.005443242062356463, Validation loss: 0.012284907507404574\n",
      "new best w\n",
      "iterations:  2051\n",
      "Training loss: 0.005437848890619903, Validation loss: 0.012304365158970331\n",
      "iterations:  2052\n",
      "Training loss: 0.005432352780165739, Validation loss: 0.012255101733631448\n",
      "new best w\n",
      "iterations:  2053\n",
      "Training loss: 0.005428030164127384, Validation loss: 0.012324482823668725\n",
      "iterations:  2054\n",
      "Training loss: 0.005422296397950017, Validation loss: 0.01220103291330171\n",
      "new best w\n",
      "iterations:  2055\n",
      "Training loss: 0.005416929513527128, Validation loss: 0.01230688828910157\n",
      "iterations:  2056\n",
      "Training loss: 0.005412512552475021, Validation loss: 0.01217533563372265\n",
      "new best w\n",
      "iterations:  2057\n",
      "Training loss: 0.005406521870988052, Validation loss: 0.012310641872681785\n",
      "iterations:  2058\n",
      "Training loss: 0.005401575621912842, Validation loss: 0.012144450636559705\n",
      "new best w\n",
      "iterations:  2059\n",
      "Training loss: 0.0053970828613329035, Validation loss: 0.012300410923151147\n",
      "iterations:  2060\n",
      "Training loss: 0.005391802733705327, Validation loss: 0.01208648752611687\n",
      "new best w\n",
      "iterations:  2061\n",
      "Training loss: 0.005386535675905211, Validation loss: 0.012288777066737505\n",
      "iterations:  2062\n",
      "Training loss: 0.005380421220412455, Validation loss: 0.01209288433151745\n",
      "iterations:  2063\n",
      "Training loss: 0.005375587406793706, Validation loss: 0.012255873680514628\n",
      "iterations:  2064\n",
      "Training loss: 0.0053721353237728845, Validation loss: 0.012031677278355173\n",
      "new best w\n",
      "iterations:  2065\n",
      "Training loss: 0.0053678901895760885, Validation loss: 0.012309814753520134\n",
      "iterations:  2066\n",
      "Training loss: 0.005363174084779907, Validation loss: 0.01196919499267667\n",
      "new best w\n",
      "iterations:  2067\n",
      "Training loss: 0.0053580247315776494, Validation loss: 0.012303719359813076\n",
      "iterations:  2068\n",
      "Training loss: 0.00535334536717904, Validation loss: 0.011938173452846635\n",
      "new best w\n",
      "iterations:  2069\n",
      "Training loss: 0.005349864817298591, Validation loss: 0.01231096223304752\n",
      "iterations:  2070\n",
      "Training loss: 0.005344049867336184, Validation loss: 0.011896167138799856\n",
      "new best w\n",
      "iterations:  2071\n",
      "Training loss: 0.005338664146232551, Validation loss: 0.012285880732125963\n",
      "iterations:  2072\n",
      "Training loss: 0.005335717929961554, Validation loss: 0.011863790478864326\n",
      "new best w\n",
      "iterations:  2073\n",
      "Training loss: 0.005330864651249697, Validation loss: 0.012311676985425653\n",
      "iterations:  2074\n",
      "Training loss: 0.005326030528346754, Validation loss: 0.011824289102191552\n",
      "new best w\n",
      "iterations:  2075\n",
      "Training loss: 0.005322059686707087, Validation loss: 0.012311553051415682\n",
      "iterations:  2076\n",
      "Training loss: 0.0053164112158894895, Validation loss: 0.011792422096084283\n",
      "new best w\n",
      "iterations:  2077\n",
      "Training loss: 0.0053136181776857775, Validation loss: 0.012310244753031803\n",
      "iterations:  2078\n",
      "Training loss: 0.005305544955993138, Validation loss: 0.011766757739690343\n",
      "new best w\n",
      "iterations:  2079\n",
      "Training loss: 0.005299924701039111, Validation loss: 0.012243604617051606\n",
      "iterations:  2080\n",
      "Training loss: 0.005296276296223799, Validation loss: 0.011752871338193135\n",
      "new best w\n",
      "iterations:  2081\n",
      "Training loss: 0.005291608532989548, Validation loss: 0.012257717098249068\n",
      "iterations:  2082\n",
      "Training loss: 0.005287099937381235, Validation loss: 0.011716364762874769\n",
      "new best w\n",
      "iterations:  2083\n",
      "Training loss: 0.005283180114181152, Validation loss: 0.012260553737944479\n",
      "iterations:  2084\n",
      "Training loss: 0.0052772221077750725, Validation loss: 0.011686159851356584\n",
      "new best w\n",
      "iterations:  2085\n",
      "Training loss: 0.005274240183667871, Validation loss: 0.012251987409300457\n",
      "iterations:  2086\n",
      "Training loss: 0.005265967593066508, Validation loss: 0.011666706873792735\n",
      "new best w\n",
      "iterations:  2087\n",
      "Training loss: 0.005262402686268892, Validation loss: 0.0122036788150869\n",
      "iterations:  2088\n",
      "Training loss: 0.005256520981909381, Validation loss: 0.01164686303161283\n",
      "new best w\n",
      "iterations:  2089\n",
      "Training loss: 0.005253617732505633, Validation loss: 0.012207821537710621\n",
      "iterations:  2090\n",
      "Training loss: 0.0052470647126054105, Validation loss: 0.011618082004298414\n",
      "new best w\n",
      "iterations:  2091\n",
      "Training loss: 0.005240787204385412, Validation loss: 0.012160438427409312\n",
      "iterations:  2092\n",
      "Training loss: 0.005236511559953025, Validation loss: 0.011608376995131573\n",
      "new best w\n",
      "iterations:  2093\n",
      "Training loss: 0.0052323684234248345, Validation loss: 0.012160749789541212\n",
      "iterations:  2094\n",
      "Training loss: 0.005225611838116496, Validation loss: 0.01158266636470692\n",
      "new best w\n",
      "iterations:  2095\n",
      "Training loss: 0.005223784684325156, Validation loss: 0.012146207391671042\n",
      "iterations:  2096\n",
      "Training loss: 0.005217007819599661, Validation loss: 0.011549542606735628\n",
      "new best w\n",
      "iterations:  2097\n",
      "Training loss: 0.005214778960315791, Validation loss: 0.012145816725392736\n",
      "iterations:  2098\n",
      "Training loss: 0.0052076753224239835, Validation loss: 0.01152048693506168\n",
      "new best w\n",
      "iterations:  2099\n",
      "Training loss: 0.005205127858742434, Validation loss: 0.012137744118881175\n",
      "iterations:  2100\n",
      "Training loss: 0.0051971661333927045, Validation loss: 0.011504820116008469\n",
      "new best w\n",
      "iterations:  2101\n",
      "Training loss: 0.0051902303979951695, Validation loss: 0.01206331246873304\n",
      "iterations:  2102\n",
      "Training loss: 0.005184352368890845, Validation loss: 0.011511720468439256\n",
      "iterations:  2103\n",
      "Training loss: 0.005180869096129501, Validation loss: 0.01204231109049682\n",
      "iterations:  2104\n",
      "Training loss: 0.0051741533061882405, Validation loss: 0.011492856998192419\n",
      "new best w\n",
      "iterations:  2105\n",
      "Training loss: 0.005172173182250899, Validation loss: 0.012034040296635054\n",
      "iterations:  2106\n",
      "Training loss: 0.005166046583160903, Validation loss: 0.01145527344648271\n",
      "new best w\n",
      "iterations:  2107\n",
      "Training loss: 0.005163792550148581, Validation loss: 0.012039152256208982\n",
      "iterations:  2108\n",
      "Training loss: 0.005156958290668796, Validation loss: 0.011426115183399312\n",
      "new best w\n",
      "iterations:  2109\n",
      "Training loss: 0.005152247148242048, Validation loss: 0.012004413830085105\n",
      "iterations:  2110\n",
      "Training loss: 0.005146886997412887, Validation loss: 0.011410887111429145\n",
      "new best w\n",
      "iterations:  2111\n",
      "Training loss: 0.005143971575116531, Validation loss: 0.012002455525555986\n",
      "iterations:  2112\n",
      "Training loss: 0.005136277528665035, Validation loss: 0.011386075985900926\n",
      "new best w\n",
      "iterations:  2113\n",
      "Training loss: 0.005131542547952388, Validation loss: 0.01194563459806084\n",
      "iterations:  2114\n",
      "Training loss: 0.005127402845796321, Validation loss: 0.011366282650371953\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2115\n",
      "Training loss: 0.0051235509273584254, Validation loss: 0.011956928899268814\n",
      "iterations:  2116\n",
      "Training loss: 0.005118205720470702, Validation loss: 0.011338441117730734\n",
      "new best w\n",
      "iterations:  2117\n",
      "Training loss: 0.005114024368946492, Validation loss: 0.011948771664179458\n",
      "iterations:  2118\n",
      "Training loss: 0.005108132265984613, Validation loss: 0.011319452577166223\n",
      "new best w\n",
      "iterations:  2119\n",
      "Training loss: 0.0051048599432748235, Validation loss: 0.011933742267701752\n",
      "iterations:  2120\n",
      "Training loss: 0.005097031256147634, Validation loss: 0.011302414034849639\n",
      "new best w\n",
      "iterations:  2121\n",
      "Training loss: 0.005094963163344232, Validation loss: 0.011904564453359894\n",
      "iterations:  2122\n",
      "Training loss: 0.00508656726318942, Validation loss: 0.011288735428795759\n",
      "new best w\n",
      "iterations:  2123\n",
      "Training loss: 0.005086047580428058, Validation loss: 0.011889298772423841\n",
      "iterations:  2124\n",
      "Training loss: 0.0050786330454099474, Validation loss: 0.011254698065093952\n",
      "new best w\n",
      "iterations:  2125\n",
      "Training loss: 0.005072363337109115, Validation loss: 0.011836049713285158\n",
      "iterations:  2126\n",
      "Training loss: 0.0050681322378180104, Validation loss: 0.011249133004932823\n",
      "new best w\n",
      "iterations:  2127\n",
      "Training loss: 0.005064331156863473, Validation loss: 0.011838761424440233\n",
      "iterations:  2128\n",
      "Training loss: 0.005059481428383741, Validation loss: 0.011218327826221284\n",
      "new best w\n",
      "iterations:  2129\n",
      "Training loss: 0.005056186483647625, Validation loss: 0.011836853053197087\n",
      "iterations:  2130\n",
      "Training loss: 0.005049551588711057, Validation loss: 0.01119019792814692\n",
      "new best w\n",
      "iterations:  2131\n",
      "Training loss: 0.005048350910018687, Validation loss: 0.011825827747008914\n",
      "iterations:  2132\n",
      "Training loss: 0.0050413711530956406, Validation loss: 0.011159478298282895\n",
      "new best w\n",
      "iterations:  2133\n",
      "Training loss: 0.005039394094564634, Validation loss: 0.011822728012244458\n",
      "iterations:  2134\n",
      "Training loss: 0.0050328785982864345, Validation loss: 0.01113480521217915\n",
      "new best w\n",
      "iterations:  2135\n",
      "Training loss: 0.005030471112874779, Validation loss: 0.01181710536175911\n",
      "iterations:  2136\n",
      "Training loss: 0.005023438148978218, Validation loss: 0.011112869997591339\n",
      "new best w\n",
      "iterations:  2137\n",
      "Training loss: 0.005018455405048015, Validation loss: 0.011774977745348494\n",
      "iterations:  2138\n",
      "Training loss: 0.005013121137173333, Validation loss: 0.011102174163505268\n",
      "new best w\n",
      "iterations:  2139\n",
      "Training loss: 0.005010049368666424, Validation loss: 0.011766934397490582\n",
      "iterations:  2140\n",
      "Training loss: 0.005002479000468522, Validation loss: 0.01108207387186343\n",
      "new best w\n",
      "iterations:  2141\n",
      "Training loss: 0.005001269198727853, Validation loss: 0.011746609661666145\n",
      "iterations:  2142\n",
      "Training loss: 0.00499433599957701, Validation loss: 0.01105281764077327\n",
      "new best w\n",
      "iterations:  2143\n",
      "Training loss: 0.004992348760850944, Validation loss: 0.01174176924538408\n",
      "iterations:  2144\n",
      "Training loss: 0.004984963800182696, Validation loss: 0.011031529918893428\n",
      "new best w\n",
      "iterations:  2145\n",
      "Training loss: 0.0049784371195914015, Validation loss: 0.01168476192210825\n",
      "iterations:  2146\n",
      "Training loss: 0.0049740891368408365, Validation loss: 0.011027813512670358\n",
      "new best w\n",
      "iterations:  2147\n",
      "Training loss: 0.004970114876753945, Validation loss: 0.011677100235424124\n",
      "iterations:  2148\n",
      "Training loss: 0.004963617367791065, Validation loss: 0.011009431267268853\n",
      "new best w\n",
      "iterations:  2149\n",
      "Training loss: 0.004960607327565953, Validation loss: 0.011650363077441713\n",
      "iterations:  2150\n",
      "Training loss: 0.004953649812241454, Validation loss: 0.010994904326895623\n",
      "new best w\n",
      "iterations:  2151\n",
      "Training loss: 0.004952264967491986, Validation loss: 0.011637493505609374\n",
      "iterations:  2152\n",
      "Training loss: 0.0049461116917037095, Validation loss: 0.010960392167683318\n",
      "new best w\n",
      "iterations:  2153\n",
      "Training loss: 0.004943965086314981, Validation loss: 0.011638590727366392\n",
      "iterations:  2154\n",
      "Training loss: 0.0049373394954332185, Validation loss: 0.010935264161827095\n",
      "new best w\n",
      "iterations:  2155\n",
      "Training loss: 0.004934763125186616, Validation loss: 0.011627850108704126\n",
      "iterations:  2156\n",
      "Training loss: 0.004927723213436582, Validation loss: 0.010917398833247145\n",
      "new best w\n",
      "iterations:  2157\n",
      "Training loss: 0.004925586321079258, Validation loss: 0.011610698316448264\n",
      "iterations:  2158\n",
      "Training loss: 0.004917019767368604, Validation loss: 0.010901332520623173\n",
      "new best w\n",
      "iterations:  2159\n",
      "Training loss: 0.0049156831253612705, Validation loss: 0.01157983324888737\n",
      "iterations:  2160\n",
      "Training loss: 0.0049069270660808, Validation loss: 0.010887512740360556\n",
      "new best w\n",
      "iterations:  2161\n",
      "Training loss: 0.00490733272353425, Validation loss: 0.011565024174521488\n",
      "iterations:  2162\n",
      "Training loss: 0.004899435467356888, Validation loss: 0.010853583214246763\n",
      "new best w\n",
      "iterations:  2163\n",
      "Training loss: 0.004898965381746996, Validation loss: 0.01156489043301463\n",
      "iterations:  2164\n",
      "Training loss: 0.004891662498013285, Validation loss: 0.010826331875220578\n",
      "new best w\n",
      "iterations:  2165\n",
      "Training loss: 0.004890570180813521, Validation loss: 0.011561954048330008\n",
      "iterations:  2166\n",
      "Training loss: 0.004883092142563271, Validation loss: 0.010800782882920728\n",
      "new best w\n",
      "iterations:  2167\n",
      "Training loss: 0.004876529069099071, Validation loss: 0.011499608836120162\n",
      "iterations:  2168\n",
      "Training loss: 0.004872259186976403, Validation loss: 0.010797602472508012\n",
      "new best w\n",
      "iterations:  2169\n",
      "Training loss: 0.004868693894246705, Validation loss: 0.011497214111329587\n",
      "iterations:  2170\n",
      "Training loss: 0.00486223141995067, Validation loss: 0.010775740717284796\n",
      "new best w\n",
      "iterations:  2171\n",
      "Training loss: 0.004860480324572931, Validation loss: 0.011480147852663972\n",
      "iterations:  2172\n",
      "Training loss: 0.004854871839305051, Validation loss: 0.010744295171144672\n",
      "new best w\n",
      "iterations:  2173\n",
      "Training loss: 0.004852214615888959, Validation loss: 0.011479983251650536\n",
      "iterations:  2174\n",
      "Training loss: 0.004846929799826414, Validation loss: 0.010719060054582083\n",
      "new best w\n",
      "iterations:  2175\n",
      "Training loss: 0.00484368313778619, Validation loss: 0.011474934887586125\n",
      "iterations:  2176\n",
      "Training loss: 0.004837888494817042, Validation loss: 0.010698086375699651\n",
      "new best w\n",
      "iterations:  2177\n",
      "Training loss: 0.004834940606810979, Validation loss: 0.011462012776341906\n",
      "iterations:  2178\n",
      "Training loss: 0.0048290536219682, Validation loss: 0.010676160359986763\n",
      "new best w\n",
      "iterations:  2179\n",
      "Training loss: 0.004826385920897627, Validation loss: 0.011449337614099515\n",
      "iterations:  2180\n",
      "Training loss: 0.00481856933666308, Validation loss: 0.010658873282066552\n",
      "new best w\n",
      "iterations:  2181\n",
      "Training loss: 0.004817639528728441, Validation loss: 0.011425603171382113\n",
      "iterations:  2182\n",
      "Training loss: 0.004810962891979006, Validation loss: 0.010631004832462243\n",
      "new best w\n",
      "iterations:  2183\n",
      "Training loss: 0.004809099574932734, Validation loss: 0.011420496083140042\n",
      "iterations:  2184\n",
      "Training loss: 0.004803254642596955, Validation loss: 0.010605742188942295\n",
      "new best w\n",
      "iterations:  2185\n",
      "Training loss: 0.004800731070965205, Validation loss: 0.01141537579507197\n",
      "iterations:  2186\n",
      "Training loss: 0.004794167544684559, Validation loss: 0.010586298315655341\n",
      "new best w\n",
      "iterations:  2187\n",
      "Training loss: 0.004791882531869926, Validation loss: 0.011400203968402816\n",
      "iterations:  2188\n",
      "Training loss: 0.004785191391948418, Validation loss: 0.010566461078725007\n",
      "new best w\n",
      "iterations:  2189\n",
      "Training loss: 0.004783109731351265, Validation loss: 0.011384592000139002\n",
      "iterations:  2190\n",
      "Training loss: 0.0047746599418723185, Validation loss: 0.010550429539197112\n",
      "new best w\n",
      "iterations:  2191\n",
      "Training loss: 0.004774301118222588, Validation loss: 0.011359092268125887\n",
      "iterations:  2192\n",
      "Training loss: 0.00476699756803298, Validation loss: 0.01052421554062511\n",
      "new best w\n",
      "iterations:  2193\n",
      "Training loss: 0.004765646493899219, Validation loss: 0.01135180634704493\n",
      "iterations:  2194\n",
      "Training loss: 0.004758937642154655, Validation loss: 0.010502558646211838\n",
      "new best w\n",
      "iterations:  2195\n",
      "Training loss: 0.004756875250330532, Validation loss: 0.01134199939141729\n",
      "iterations:  2196\n",
      "Training loss: 0.004749842466356588, Validation loss: 0.010484252238386785\n",
      "new best w\n",
      "iterations:  2197\n",
      "Training loss: 0.004747974278885429, Validation loss: 0.01132548186303176\n",
      "iterations:  2198\n",
      "Training loss: 0.004741076080936158, Validation loss: 0.010464327543095262\n",
      "new best w\n",
      "iterations:  2199\n",
      "Training loss: 0.004739379162931931, Validation loss: 0.011310987058409798\n",
      "iterations:  2200\n",
      "Training loss: 0.004730850187816764, Validation loss: 0.010448360908883625\n",
      "new best w\n",
      "iterations:  2201\n",
      "Training loss: 0.004730856482943458, Validation loss: 0.011285662687671534\n",
      "iterations:  2202\n",
      "Training loss: 0.0047236944905875, Validation loss: 0.010421443746491718\n",
      "new best w\n",
      "iterations:  2203\n",
      "Training loss: 0.004722659498322458, Validation loss: 0.01127991726683283\n",
      "iterations:  2204\n",
      "Training loss: 0.004716433312315766, Validation loss: 0.010398033296795534\n",
      "new best w\n",
      "iterations:  2205\n",
      "Training loss: 0.004714394874769804, Validation loss: 0.011273652875298263\n",
      "iterations:  2206\n",
      "Training loss: 0.0047074849156062346, Validation loss: 0.01038051971946652\n",
      "new best w\n",
      "iterations:  2207\n",
      "Training loss: 0.004705777503085808, Validation loss: 0.01125636192443536\n",
      "iterations:  2208\n",
      "Training loss: 0.004698932473707696, Validation loss: 0.010361244383653118\n",
      "new best w\n",
      "iterations:  2209\n",
      "Training loss: 0.004697161890407544, Validation loss: 0.011240387715499844\n",
      "iterations:  2210\n",
      "Training loss: 0.004688782263081049, Validation loss: 0.010345992581254371\n",
      "new best w\n",
      "iterations:  2211\n",
      "Training loss: 0.00468877723979457, Validation loss: 0.011214504736898013\n",
      "iterations:  2212\n",
      "Training loss: 0.004681581774472586, Validation loss: 0.010320409724619936\n",
      "new best w\n",
      "iterations:  2213\n",
      "Training loss: 0.004680503782753171, Validation loss: 0.011207187748976562\n",
      "iterations:  2214\n",
      "Training loss: 0.004674182284364382, Validation loss: 0.01029888566813403\n",
      "new best w\n",
      "iterations:  2215\n",
      "Training loss: 0.004672020568870839, Validation loss: 0.011198631853922619\n",
      "iterations:  2216\n",
      "Training loss: 0.0046652853883626414, Validation loss: 0.010282009461663428\n",
      "new best w\n",
      "iterations:  2217\n",
      "Training loss: 0.004663429665999503, Validation loss: 0.011180812180291402\n",
      "iterations:  2218\n",
      "Training loss: 0.004656958930463044, Validation loss: 0.010262335263673445\n",
      "new best w\n",
      "iterations:  2219\n",
      "Training loss: 0.004654938475981576, Validation loss: 0.011165753129035002\n",
      "iterations:  2220\n",
      "Training loss: 0.004648908554729542, Validation loss: 0.010242209924864121\n",
      "new best w\n",
      "iterations:  2221\n",
      "Training loss: 0.004646843866267245, Validation loss: 0.011152310554600968\n",
      "iterations:  2222\n",
      "Training loss: 0.004639522070741722, Validation loss: 0.010225736433515096\n",
      "new best w\n",
      "iterations:  2223\n",
      "Training loss: 0.004638202278480376, Validation loss: 0.011130703989449353\n",
      "iterations:  2224\n",
      "Training loss: 0.004632854250710285, Validation loss: 0.010199117560310996\n",
      "new best w\n",
      "iterations:  2225\n",
      "Training loss: 0.004630438298189526, Validation loss: 0.01112660827328819\n",
      "iterations:  2226\n",
      "Training loss: 0.004625423129620432, Validation loss: 0.010178271867245198\n",
      "new best w\n",
      "iterations:  2227\n",
      "Training loss: 0.004622217216576397, Validation loss: 0.011116920489425967\n",
      "iterations:  2228\n",
      "Training loss: 0.004616782934365965, Validation loss: 0.01016145605445077\n",
      "new best w\n",
      "iterations:  2229\n",
      "Training loss: 0.004613628944105915, Validation loss: 0.011099373098722555\n",
      "iterations:  2230\n",
      "Training loss: 0.004608473736395029, Validation loss: 0.010142815836509667\n",
      "new best w\n",
      "iterations:  2231\n",
      "Training loss: 0.004605307011685819, Validation loss: 0.011083336963065769\n",
      "iterations:  2232\n",
      "Training loss: 0.004598580338646407, Validation loss: 0.010127912701566665\n",
      "new best w\n",
      "iterations:  2233\n",
      "Training loss: 0.004596891932695703, Validation loss: 0.011057570086157605\n",
      "iterations:  2234\n",
      "Training loss: 0.004591663189935094, Validation loss: 0.010102791652434018\n",
      "new best w\n",
      "iterations:  2235\n",
      "Training loss: 0.0045888583095438605, Validation loss: 0.01105023250928445\n",
      "iterations:  2236\n",
      "Training loss: 0.004584366833474875, Validation loss: 0.010082103128073413\n",
      "new best w\n",
      "iterations:  2237\n",
      "Training loss: 0.004580684424647149, Validation loss: 0.011040745328552518\n",
      "iterations:  2238\n",
      "Training loss: 0.004576007858147729, Validation loss: 0.010064644985035408\n",
      "new best w\n",
      "iterations:  2239\n",
      "Training loss: 0.004572343963166378, Validation loss: 0.011024684749644803\n",
      "iterations:  2240\n",
      "Training loss: 0.004567951263037204, Validation loss: 0.010045502380904372\n",
      "new best w\n",
      "iterations:  2241\n",
      "Training loss: 0.004564114220325618, Validation loss: 0.0110098849323658\n",
      "iterations:  2242\n",
      "Training loss: 0.004558198812723766, Validation loss: 0.010030650063450466\n",
      "new best w\n",
      "iterations:  2243\n",
      "Training loss: 0.004555962305220455, Validation loss: 0.01098448877037177\n",
      "iterations:  2244\n",
      "Training loss: 0.004551699727777657, Validation loss: 0.010004253328559549\n",
      "new best w\n",
      "iterations:  2245\n",
      "Training loss: 0.004548341982791527, Validation loss: 0.010979662223400997\n",
      "iterations:  2246\n",
      "Training loss: 0.0045451228628895675, Validation loss: 0.009981346122974877\n",
      "new best w\n",
      "iterations:  2247\n",
      "Training loss: 0.004540599416992188, Validation loss: 0.010974614640665962\n",
      "iterations:  2248\n",
      "Training loss: 0.004536790189609825, Validation loss: 0.009964164355170852\n",
      "new best w\n",
      "iterations:  2249\n",
      "Training loss: 0.004532470702842474, Validation loss: 0.010958510360534062\n",
      "iterations:  2250\n",
      "Training loss: 0.004528843397776431, Validation loss: 0.009945112113803532\n",
      "new best w\n",
      "iterations:  2251\n",
      "Training loss: 0.0045242870083973714, Validation loss: 0.010943689538851572\n",
      "iterations:  2252\n",
      "Training loss: 0.004521258215139515, Validation loss: 0.009925259441354889\n",
      "new best w\n",
      "iterations:  2253\n",
      "Training loss: 0.004516542645454278, Validation loss: 0.010931159093821629\n",
      "iterations:  2254\n",
      "Training loss: 0.004512237857237312, Validation loss: 0.009909411945169144\n",
      "new best w\n",
      "iterations:  2255\n",
      "Training loss: 0.004508189457472215, Validation loss: 0.010909892672085506\n",
      "iterations:  2256\n",
      "Training loss: 0.004505501350632667, Validation loss: 0.0098861302187059\n",
      "new best w\n",
      "iterations:  2257\n",
      "Training loss: 0.004500321089973532, Validation loss: 0.010902112226288311\n",
      "iterations:  2258\n",
      "Training loss: 0.004498420509945119, Validation loss: 0.009866254367747048\n",
      "new best w\n",
      "iterations:  2259\n",
      "Training loss: 0.0044923773193839885, Validation loss: 0.010892485074163388\n",
      "iterations:  2260\n",
      "Training loss: 0.004490291008140453, Validation loss: 0.009849347420966128\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2261\n",
      "Training loss: 0.004484213329406575, Validation loss: 0.01087648404237648\n",
      "iterations:  2262\n",
      "Training loss: 0.0044824784690505135, Validation loss: 0.009830670298629737\n",
      "new best w\n",
      "iterations:  2263\n",
      "Training loss: 0.004476241948786816, Validation loss: 0.010861835704249236\n",
      "iterations:  2264\n",
      "Training loss: 0.004473092625066687, Validation loss: 0.009815589303979655\n",
      "new best w\n",
      "iterations:  2265\n",
      "Training loss: 0.004468275890329206, Validation loss: 0.010837740307737381\n",
      "iterations:  2266\n",
      "Training loss: 0.004467069950802341, Validation loss: 0.009788837542825021\n",
      "new best w\n",
      "iterations:  2267\n",
      "Training loss: 0.00446110880714566, Validation loss: 0.010834619745250773\n",
      "iterations:  2268\n",
      "Training loss: 0.004460582520694506, Validation loss: 0.009767411907676557\n",
      "new best w\n",
      "iterations:  2269\n",
      "Training loss: 0.004453479245369213, Validation loss: 0.0108283091501287\n",
      "iterations:  2270\n",
      "Training loss: 0.004452349608400141, Validation loss: 0.009751408492645844\n",
      "new best w\n",
      "iterations:  2271\n",
      "Training loss: 0.004445400545129506, Validation loss: 0.010811349664369637\n",
      "iterations:  2272\n",
      "Training loss: 0.004444779565191425, Validation loss: 0.009732233929499647\n",
      "new best w\n",
      "iterations:  2273\n",
      "Training loss: 0.004437540983536711, Validation loss: 0.01079778870993992\n",
      "iterations:  2274\n",
      "Training loss: 0.004437425302955573, Validation loss: 0.009712937687018832\n",
      "new best w\n",
      "iterations:  2275\n",
      "Training loss: 0.004430021518898721, Validation loss: 0.010785342067009717\n",
      "iterations:  2276\n",
      "Training loss: 0.004428703312008054, Validation loss: 0.009697257201368669\n",
      "new best w\n",
      "iterations:  2277\n",
      "Training loss: 0.004421848313103793, Validation loss: 0.01076483914475326\n",
      "iterations:  2278\n",
      "Training loss: 0.004422284543820194, Validation loss: 0.009674213960434849\n",
      "new best w\n",
      "iterations:  2279\n",
      "Training loss: 0.004414285583256793, Validation loss: 0.01075772797233619\n",
      "iterations:  2280\n",
      "Training loss: 0.004415525212829038, Validation loss: 0.009654561246130407\n",
      "new best w\n",
      "iterations:  2281\n",
      "Training loss: 0.004406662752537342, Validation loss: 0.010748858136487276\n",
      "iterations:  2282\n",
      "Training loss: 0.004407705272823089, Validation loss: 0.0096378341596572\n",
      "new best w\n",
      "iterations:  2283\n",
      "Training loss: 0.004398763702953391, Validation loss: 0.01073365521864576\n",
      "iterations:  2284\n",
      "Training loss: 0.0044003424974618345, Validation loss: 0.009618750448365986\n",
      "new best w\n",
      "iterations:  2285\n",
      "Training loss: 0.004391239445737465, Validation loss: 0.010720838263599408\n",
      "iterations:  2286\n",
      "Training loss: 0.004391153567107714, Validation loss: 0.009604243909562077\n",
      "new best w\n",
      "iterations:  2287\n",
      "Training loss: 0.004383337111138693, Validation loss: 0.010696817223250832\n",
      "iterations:  2288\n",
      "Training loss: 0.0043854056628853276, Validation loss: 0.009578157374357164\n",
      "new best w\n",
      "iterations:  2289\n",
      "Training loss: 0.004376430620160246, Validation loss: 0.010693901963538577\n",
      "iterations:  2290\n",
      "Training loss: 0.004379269290554432, Validation loss: 0.009556974979019644\n",
      "new best w\n",
      "iterations:  2291\n",
      "Training loss: 0.004369164745385303, Validation loss: 0.010688509752255654\n",
      "iterations:  2292\n",
      "Training loss: 0.004371436700535475, Validation loss: 0.009540798048521695\n",
      "new best w\n",
      "iterations:  2293\n",
      "Training loss: 0.0043614296366858325, Validation loss: 0.010673069181843409\n",
      "iterations:  2294\n",
      "Training loss: 0.004364112962814926, Validation loss: 0.009522093888173518\n",
      "new best w\n",
      "iterations:  2295\n",
      "Training loss: 0.004353799479380071, Validation loss: 0.010659806901052966\n",
      "iterations:  2296\n",
      "Training loss: 0.004355006173499055, Validation loss: 0.009507889403842005\n",
      "new best w\n",
      "iterations:  2297\n",
      "Training loss: 0.004346107251800797, Validation loss: 0.010635861037720256\n",
      "iterations:  2298\n",
      "Training loss: 0.004349103043651431, Validation loss: 0.009483234237068029\n",
      "new best w\n",
      "iterations:  2299\n",
      "Training loss: 0.004339051973982999, Validation loss: 0.010631044223972442\n",
      "iterations:  2300\n",
      "Training loss: 0.0043430350211666425, Validation loss: 0.009462591373965523\n",
      "new best w\n",
      "iterations:  2301\n",
      "Training loss: 0.004331771721513246, Validation loss: 0.010625469383282763\n",
      "iterations:  2302\n",
      "Training loss: 0.004336519477985519, Validation loss: 0.009443926208752187\n",
      "new best w\n",
      "iterations:  2303\n",
      "Training loss: 0.004324436222860581, Validation loss: 0.01061712637156785\n",
      "iterations:  2304\n",
      "Training loss: 0.004329125472013, Validation loss: 0.009427059997558336\n",
      "new best w\n",
      "iterations:  2305\n",
      "Training loss: 0.004316938804167569, Validation loss: 0.010603627174088902\n",
      "iterations:  2306\n",
      "Training loss: 0.004314046553341427, Validation loss: 0.00943811645822344\n",
      "iterations:  2307\n",
      "Training loss: 0.004305613004954203, Validation loss: 0.010553317925608005\n",
      "iterations:  2308\n",
      "Training loss: 0.004306386010632079, Validation loss: 0.00941946073966463\n",
      "new best w\n",
      "iterations:  2309\n",
      "Training loss: 0.004298943375432943, Validation loss: 0.010539060542647903\n",
      "iterations:  2310\n",
      "Training loss: 0.004301745713289093, Validation loss: 0.009390484604631539\n",
      "new best w\n",
      "iterations:  2311\n",
      "Training loss: 0.004292998601331836, Validation loss: 0.01054210416685102\n",
      "iterations:  2312\n",
      "Training loss: 0.004297009250977865, Validation loss: 0.009365334579290046\n",
      "new best w\n",
      "iterations:  2313\n",
      "Training loss: 0.004286978594076207, Validation loss: 0.010544713446445364\n",
      "iterations:  2314\n",
      "Training loss: 0.00429044747722742, Validation loss: 0.009345723268417896\n",
      "new best w\n",
      "iterations:  2315\n",
      "Training loss: 0.004280325584196889, Validation loss: 0.010536311067242307\n",
      "iterations:  2316\n",
      "Training loss: 0.004284270982981449, Validation loss: 0.009324107749476824\n",
      "new best w\n",
      "iterations:  2317\n",
      "Training loss: 0.004273664063359258, Validation loss: 0.010529174444184813\n",
      "iterations:  2318\n",
      "Training loss: 0.004278297961225444, Validation loss: 0.00930251280518816\n",
      "new best w\n",
      "iterations:  2319\n",
      "Training loss: 0.004267303290417108, Validation loss: 0.010523039886600544\n",
      "iterations:  2320\n",
      "Training loss: 0.0042708176558693646, Validation loss: 0.009284945016186548\n",
      "new best w\n",
      "iterations:  2321\n",
      "Training loss: 0.00426028303128872, Validation loss: 0.010508144699660067\n",
      "iterations:  2322\n",
      "Training loss: 0.00425920429913411, Validation loss: 0.009282651055735069\n",
      "new best w\n",
      "iterations:  2323\n",
      "Training loss: 0.004250174301707976, Validation loss: 0.010475317846122755\n",
      "iterations:  2324\n",
      "Training loss: 0.004254578096713913, Validation loss: 0.009258524712373883\n",
      "new best w\n",
      "iterations:  2325\n",
      "Training loss: 0.004244453325659397, Validation loss: 0.010478188035048207\n",
      "iterations:  2326\n",
      "Training loss: 0.0042443326641422195, Validation loss: 0.00925270084536396\n",
      "new best w\n",
      "iterations:  2327\n",
      "Training loss: 0.004235798512716369, Validation loss: 0.01045210055253174\n",
      "iterations:  2328\n",
      "Training loss: 0.004239323128483552, Validation loss: 0.009228159937877355\n",
      "new best w\n",
      "iterations:  2329\n",
      "Training loss: 0.0042301730013225404, Validation loss: 0.010452179267246309\n",
      "iterations:  2330\n",
      "Training loss: 0.004234619252064613, Validation loss: 0.009202907338320587\n",
      "new best w\n",
      "iterations:  2331\n",
      "Training loss: 0.004224935281441646, Validation loss: 0.010453525765342435\n",
      "iterations:  2332\n",
      "Training loss: 0.004228219993919644, Validation loss: 0.009182356140816495\n",
      "new best w\n",
      "iterations:  2333\n",
      "Training loss: 0.0042187367838790275, Validation loss: 0.01044470755359236\n",
      "iterations:  2334\n",
      "Training loss: 0.004224389667923604, Validation loss: 0.009154198359419292\n",
      "new best w\n",
      "iterations:  2335\n",
      "Training loss: 0.004213535486399418, Validation loss: 0.010450400751327006\n",
      "iterations:  2336\n",
      "Training loss: 0.004220176377257981, Validation loss: 0.00913008255778685\n",
      "new best w\n",
      "iterations:  2337\n",
      "Training loss: 0.004207982382247852, Validation loss: 0.010453702947721195\n",
      "iterations:  2338\n",
      "Training loss: 0.004213903907487072, Validation loss: 0.009111667832085902\n",
      "new best w\n",
      "iterations:  2339\n",
      "Training loss: 0.004201624728195171, Validation loss: 0.01044502107980346\n",
      "iterations:  2340\n",
      "Training loss: 0.004199075606917316, Validation loss: 0.009118860830104707\n",
      "iterations:  2341\n",
      "Training loss: 0.004189880030077045, Validation loss: 0.010395968894063756\n",
      "iterations:  2342\n",
      "Training loss: 0.004194809413987753, Validation loss: 0.00909417152630235\n",
      "new best w\n",
      "iterations:  2343\n",
      "Training loss: 0.004185044385937687, Validation loss: 0.010399431690111774\n",
      "iterations:  2344\n",
      "Training loss: 0.004188960527019326, Validation loss: 0.009073026856026126\n",
      "new best w\n",
      "iterations:  2345\n",
      "Training loss: 0.004179303818048086, Validation loss: 0.010393222826462543\n",
      "iterations:  2346\n",
      "Training loss: 0.0041853788937084024, Validation loss: 0.009045257852776895\n",
      "new best w\n",
      "iterations:  2347\n",
      "Training loss: 0.00417427823253333, Validation loss: 0.010399127846285291\n",
      "iterations:  2348\n",
      "Training loss: 0.004181542336241878, Validation loss: 0.009021155425108285\n",
      "new best w\n",
      "iterations:  2349\n",
      "Training loss: 0.0041691533330685, Validation loss: 0.010403592561612484\n",
      "iterations:  2350\n",
      "Training loss: 0.0041689077537776335, Validation loss: 0.009022206021983895\n",
      "iterations:  2351\n",
      "Training loss: 0.004158903795478015, Validation loss: 0.010365600156331567\n",
      "iterations:  2352\n",
      "Training loss: 0.0041646994725021555, Validation loss: 0.00899798796780748\n",
      "new best w\n",
      "iterations:  2353\n",
      "Training loss: 0.00415409521966181, Validation loss: 0.010368254446859234\n",
      "iterations:  2354\n",
      "Training loss: 0.004156024200459801, Validation loss: 0.0089868320427473\n",
      "new best w\n",
      "iterations:  2355\n",
      "Training loss: 0.004146516085799102, Validation loss: 0.010350070589766837\n",
      "iterations:  2356\n",
      "Training loss: 0.004150937547277104, Validation loss: 0.00896491278021806\n",
      "new best w\n",
      "iterations:  2357\n",
      "Training loss: 0.004141496199860491, Validation loss: 0.010347774672124119\n",
      "iterations:  2358\n",
      "Training loss: 0.004148253505836496, Validation loss: 0.00893589566510235\n",
      "new best w\n",
      "iterations:  2359\n",
      "Training loss: 0.0041372958363521785, Validation loss: 0.010358000187217945\n",
      "iterations:  2360\n",
      "Training loss: 0.004145219829136089, Validation loss: 0.008910830713375324\n",
      "new best w\n",
      "iterations:  2361\n",
      "Training loss: 0.0041327346616574605, Validation loss: 0.010366124848604363\n",
      "iterations:  2362\n",
      "Training loss: 0.004140858963762623, Validation loss: 0.008889382887411456\n",
      "new best w\n",
      "iterations:  2363\n",
      "Training loss: 0.004128231018396396, Validation loss: 0.010367355469184516\n",
      "iterations:  2364\n",
      "Training loss: 0.004126782277238242, Validation loss: 0.00889201410857115\n",
      "iterations:  2365\n",
      "Training loss: 0.0041168986275656365, Validation loss: 0.010323478721261534\n",
      "iterations:  2366\n",
      "Training loss: 0.00412457047870079, Validation loss: 0.008865196993756302\n",
      "new best w\n",
      "iterations:  2367\n",
      "Training loss: 0.004113240014101143, Validation loss: 0.010336135704821938\n",
      "iterations:  2368\n",
      "Training loss: 0.004118812706587118, Validation loss: 0.008846845397884055\n",
      "new best w\n",
      "iterations:  2369\n",
      "Training loss: 0.00410818817853399, Validation loss: 0.010328999970982735\n",
      "iterations:  2370\n",
      "Training loss: 0.004116524856315145, Validation loss: 0.008817508377198736\n",
      "new best w\n",
      "iterations:  2371\n",
      "Training loss: 0.004104544328360254, Validation loss: 0.010339829849103806\n",
      "iterations:  2372\n",
      "Training loss: 0.004106398489529566, Validation loss: 0.00881130352632548\n",
      "new best w\n",
      "iterations:  2373\n",
      "Training loss: 0.004095370259896961, Validation loss: 0.010315401596219931\n",
      "iterations:  2374\n",
      "Training loss: 0.004098938072747954, Validation loss: 0.008799868320964731\n",
      "new best w\n",
      "iterations:  2375\n",
      "Training loss: 0.004088383117474257, Validation loss: 0.010303882335916583\n",
      "iterations:  2376\n",
      "Training loss: 0.004095946759608182, Validation loss: 0.00877649304998074\n",
      "new best w\n",
      "iterations:  2377\n",
      "Training loss: 0.0040847030371359535, Validation loss: 0.010311882471220777\n",
      "iterations:  2378\n",
      "Training loss: 0.004093137744231352, Validation loss: 0.008751439191532566\n",
      "new best w\n",
      "iterations:  2379\n",
      "Training loss: 0.004081030045355357, Validation loss: 0.0103195797804805\n",
      "iterations:  2380\n",
      "Training loss: 0.0040882720030836085, Validation loss: 0.00873098855412651\n",
      "new best w\n",
      "iterations:  2381\n",
      "Training loss: 0.004076909722980587, Validation loss: 0.010315867779201956\n",
      "iterations:  2382\n",
      "Training loss: 0.004076177898360509, Validation loss: 0.008725198483632911\n",
      "new best w\n",
      "iterations:  2383\n",
      "Training loss: 0.004066685791005467, Validation loss: 0.010282662743636355\n",
      "iterations:  2384\n",
      "Training loss: 0.004075481634973563, Validation loss: 0.008698125719616465\n",
      "new best w\n",
      "iterations:  2385\n",
      "Training loss: 0.004064279550426806, Validation loss: 0.010302432674420945\n",
      "iterations:  2386\n",
      "Training loss: 0.004074285255979783, Validation loss: 0.008672647337329639\n",
      "new best w\n",
      "iterations:  2387\n",
      "Training loss: 0.004061623706315931, Validation loss: 0.010318662844268675\n",
      "iterations:  2388\n",
      "Training loss: 0.004071715320812631, Validation loss: 0.008650093711719212\n",
      "new best w\n",
      "iterations:  2389\n",
      "Training loss: 0.004058555074678563, Validation loss: 0.010327041076853893\n",
      "iterations:  2390\n",
      "Training loss: 0.004061489869801978, Validation loss: 0.00864219337446993\n",
      "new best w\n",
      "iterations:  2391\n",
      "Training loss: 0.004049944999960084, Validation loss: 0.01030226498763177\n",
      "iterations:  2392\n",
      "Training loss: 0.004054453412978755, Validation loss: 0.008628623151797908\n",
      "new best w\n",
      "iterations:  2393\n",
      "Training loss: 0.0040437171388701745, Validation loss: 0.010291800064269494\n",
      "iterations:  2394\n",
      "Training loss: 0.004051839529669806, Validation loss: 0.008606200191340077\n",
      "new best w\n",
      "iterations:  2395\n",
      "Training loss: 0.00404126616571385, Validation loss: 0.010302105678301044\n",
      "iterations:  2396\n",
      "Training loss: 0.004052517752871167, Validation loss: 0.008575802816464386\n",
      "new best w\n",
      "iterations:  2397\n",
      "Training loss: 0.004040146078442291, Validation loss: 0.010326148962935953\n",
      "iterations:  2398\n",
      "Training loss: 0.004052683873743118, Validation loss: 0.008549832662963119\n",
      "new best w\n",
      "iterations:  2399\n",
      "Training loss: 0.004038794240658471, Validation loss: 0.010347186622030118\n",
      "iterations:  2400\n",
      "Training loss: 0.00403912291560614, Validation loss: 0.008549673809491972\n",
      "new best w\n",
      "iterations:  2401\n",
      "Training loss: 0.004027796355431216, Validation loss: 0.010308096151029306\n",
      "iterations:  2402\n",
      "Training loss: 0.004038875654125422, Validation loss: 0.008524628107251266\n",
      "new best w\n",
      "iterations:  2403\n",
      "Training loss: 0.004026490982092196, Validation loss: 0.010326990822932482\n",
      "iterations:  2404\n",
      "Training loss: 0.004031011171079482, Validation loss: 0.00851260322086271\n",
      "new best w\n",
      "iterations:  2405\n",
      "Training loss: 0.004019459173966425, Validation loss: 0.010313518135861486\n",
      "iterations:  2406\n",
      "Training loss: 0.004022728312816707, Validation loss: 0.008503871038868838\n",
      "new best w\n",
      "iterations:  2407\n",
      "Training loss: 0.0040133032984732975, Validation loss: 0.0102966089667743\n",
      "iterations:  2408\n",
      "Training loss: 0.004025211041209517, Validation loss: 0.008471788530637146\n",
      "new best w\n",
      "iterations:  2409\n",
      "Training loss: 0.004013866452834165, Validation loss: 0.010328327753831251\n",
      "iterations:  2410\n",
      "Training loss: 0.004027253388450534, Validation loss: 0.008444519923517805\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2411\n",
      "Training loss: 0.004013917751812599, Validation loss: 0.010357406612831275\n",
      "iterations:  2412\n",
      "Training loss: 0.004028755570405675, Validation loss: 0.008419867472516272\n",
      "new best w\n",
      "iterations:  2413\n",
      "Training loss: 0.0040143183049201545, Validation loss: 0.01038459094825773\n",
      "iterations:  2414\n",
      "Training loss: 0.00401637259930897, Validation loss: 0.008415905648679327\n",
      "new best w\n",
      "iterations:  2415\n",
      "Training loss: 0.004003827537237075, Validation loss: 0.010350459981638402\n",
      "iterations:  2416\n",
      "Training loss: 0.00401756801562976, Validation loss: 0.00839208633881092\n",
      "new best w\n",
      "iterations:  2417\n",
      "Training loss: 0.004003958621439659, Validation loss: 0.010374473122369518\n",
      "iterations:  2418\n",
      "Training loss: 0.004007298741252008, Validation loss: 0.008384519510586542\n",
      "new best w\n",
      "iterations:  2419\n",
      "Training loss: 0.0039968139812414675, Validation loss: 0.010350413220908652\n",
      "iterations:  2420\n",
      "Training loss: 0.0040046373649377555, Validation loss: 0.008362178794197556\n",
      "new best w\n",
      "iterations:  2421\n",
      "Training loss: 0.003993530447865594, Validation loss: 0.010358303933273645\n",
      "iterations:  2422\n",
      "Training loss: 0.004008630546088857, Validation loss: 0.008335795795688904\n",
      "new best w\n",
      "iterations:  2423\n",
      "Training loss: 0.003995737908046622, Validation loss: 0.010396673599526823\n",
      "iterations:  2424\n",
      "Training loss: 0.004012908006694103, Validation loss: 0.008310079831244108\n",
      "new best w\n",
      "iterations:  2425\n",
      "Training loss: 0.003997792021913131, Validation loss: 0.01043285235835105\n",
      "iterations:  2426\n",
      "Training loss: 0.004000691136396445, Validation loss: 0.008305732124000181\n",
      "new best w\n",
      "iterations:  2427\n",
      "Training loss: 0.003988029894366763, Validation loss: 0.010401647118461135\n",
      "iterations:  2428\n",
      "Training loss: 0.004003963908912089, Validation loss: 0.008281766097372138\n",
      "new best w\n",
      "iterations:  2429\n",
      "Training loss: 0.003990252890819273, Validation loss: 0.010434704314946553\n",
      "iterations:  2430\n",
      "Training loss: 0.003994644807231918, Validation loss: 0.00827247864396125\n",
      "new best w\n",
      "iterations:  2431\n",
      "Training loss: 0.003983468007593492, Validation loss: 0.010412804286609864\n",
      "iterations:  2432\n",
      "Training loss: 0.003993360608540774, Validation loss: 0.008250301329679192\n",
      "new best w\n",
      "iterations:  2433\n",
      "Training loss: 0.003982079235142079, Validation loss: 0.010428571132754529\n",
      "iterations:  2434\n",
      "Training loss: 0.00399926043202081, Validation loss: 0.008224796076620102\n",
      "new best w\n",
      "iterations:  2435\n",
      "Training loss: 0.003985563001418186, Validation loss: 0.010471368067647147\n",
      "iterations:  2436\n",
      "Training loss: 0.00400540027070802, Validation loss: 0.008201151820340377\n",
      "new best w\n",
      "iterations:  2437\n",
      "Training loss: 0.00399013074890972, Validation loss: 0.010516717875334994\n",
      "iterations:  2438\n",
      "Training loss: 0.003994603653453454, Validation loss: 0.008194082346091548\n",
      "new best w\n",
      "iterations:  2439\n",
      "Training loss: 0.003981188844165582, Validation loss: 0.010491286058966152\n",
      "iterations:  2440\n",
      "Training loss: 0.0040001642537957054, Validation loss: 0.008171731566526109\n",
      "new best w\n",
      "iterations:  2441\n",
      "Training loss: 0.003985442543763689, Validation loss: 0.010531620472136233\n",
      "iterations:  2442\n",
      "Training loss: 0.003991797069419139, Validation loss: 0.008162154323146285\n",
      "new best w\n",
      "iterations:  2443\n",
      "Training loss: 0.003980089690343021, Validation loss: 0.010516578148019085\n",
      "iterations:  2444\n",
      "Training loss: 0.0039927602525941675, Validation loss: 0.008139000679122549\n",
      "new best w\n",
      "iterations:  2445\n",
      "Training loss: 0.0039801125228373264, Validation loss: 0.010538739594350506\n",
      "iterations:  2446\n",
      "Training loss: 0.004001085319142361, Validation loss: 0.008116238397189175\n",
      "new best w\n",
      "iterations:  2447\n",
      "Training loss: 0.003986590991609229, Validation loss: 0.010592369698243159\n",
      "iterations:  2448\n",
      "Training loss: 0.004011011190470993, Validation loss: 0.008093754280772778\n",
      "new best w\n",
      "iterations:  2449\n",
      "Training loss: 0.003994384865375048, Validation loss: 0.010649289363187828\n",
      "iterations:  2450\n",
      "Training loss: 0.004001807517920353, Validation loss: 0.008085675309856854\n",
      "new best w\n",
      "iterations:  2451\n",
      "Training loss: 0.003987469369445631, Validation loss: 0.010634936520184077\n",
      "iterations:  2452\n",
      "Training loss: 0.004010729032975905, Validation loss: 0.008065065702076556\n",
      "new best w\n",
      "iterations:  2453\n",
      "Training loss: 0.003994835114606744, Validation loss: 0.010685734454415816\n",
      "iterations:  2454\n",
      "Training loss: 0.004003957042867415, Validation loss: 0.008055352771358352\n",
      "new best w\n",
      "iterations:  2455\n",
      "Training loss: 0.003990965969401191, Validation loss: 0.010678328461424512\n",
      "iterations:  2456\n",
      "Training loss: 0.0040067785996932784, Validation loss: 0.008033354641123251\n",
      "new best w\n",
      "iterations:  2457\n",
      "Training loss: 0.003992594719810054, Validation loss: 0.010707020124074476\n",
      "iterations:  2458\n",
      "Training loss: 0.0040193136174038815, Validation loss: 0.008013639798976813\n",
      "new best w\n",
      "iterations:  2459\n",
      "Training loss: 0.004003058816409778, Validation loss: 0.010773908230243252\n",
      "iterations:  2460\n",
      "Training loss: 0.004013871951580294, Validation loss: 0.008001935283384734\n",
      "new best w\n",
      "iterations:  2461\n",
      "Training loss: 0.0039990553339579035, Validation loss: 0.010775736091208726\n",
      "iterations:  2462\n",
      "Training loss: 0.00402593590576696, Validation loss: 0.00798703909995391\n",
      "new best w\n",
      "iterations:  2463\n",
      "Training loss: 0.004009638883477041, Validation loss: 0.010838473371266996\n",
      "iterations:  2464\n",
      "Training loss: 0.004024751660503601, Validation loss: 0.007973346499342499\n",
      "new best w\n",
      "iterations:  2465\n",
      "Training loss: 0.00400990639108752, Validation loss: 0.010854354745061669\n",
      "iterations:  2466\n",
      "Training loss: 0.004025145450108833, Validation loss: 0.007963353179691702\n",
      "new best w\n",
      "iterations:  2467\n",
      "Training loss: 0.004011155930693349, Validation loss: 0.010872184038245285\n",
      "iterations:  2468\n",
      "Training loss: 0.0040429988629768245, Validation loss: 0.007941524248701481\n",
      "new best w\n",
      "iterations:  2469\n",
      "Training loss: 0.004027346899419612, Validation loss: 0.010958088714044672\n",
      "iterations:  2470\n",
      "Training loss: 0.00404206221208961, Validation loss: 0.007927116736673097\n",
      "new best w\n",
      "iterations:  2471\n",
      "Training loss: 0.004026562966026789, Validation loss: 0.010975467209024255\n",
      "iterations:  2472\n",
      "Training loss: 0.004058073525283792, Validation loss: 0.007918401800115398\n",
      "new best w\n",
      "iterations:  2473\n",
      "Training loss: 0.004042068511817904, Validation loss: 0.01105420961858823\n",
      "iterations:  2474\n",
      "Training loss: 0.004060760850623795, Validation loss: 0.007904687285033019\n",
      "new best w\n",
      "iterations:  2475\n",
      "Training loss: 0.0040452309670263725, Validation loss: 0.011082366172223398\n",
      "iterations:  2476\n",
      "Training loss: 0.004064567314257419, Validation loss: 0.007898115684441322\n",
      "new best w\n",
      "iterations:  2477\n",
      "Training loss: 0.004050719501667593, Validation loss: 0.011115798787869014\n",
      "iterations:  2478\n",
      "Training loss: 0.004087834310314779, Validation loss: 0.007880999628806262\n",
      "new best w\n",
      "iterations:  2479\n",
      "Training loss: 0.004071452798968884, Validation loss: 0.011216295794610467\n",
      "iterations:  2480\n",
      "Training loss: 0.004089862431691114, Validation loss: 0.007868616454521487\n",
      "new best w\n",
      "iterations:  2481\n",
      "Training loss: 0.004074397595325541, Validation loss: 0.0112477387051091\n",
      "iterations:  2482\n",
      "Training loss: 0.004095541268084038, Validation loss: 0.00786323692631812\n",
      "new best w\n",
      "iterations:  2483\n",
      "Training loss: 0.0040808980353870625, Validation loss: 0.011288373633461071\n",
      "iterations:  2484\n",
      "Training loss: 0.0041095990635154084, Validation loss: 0.00785645937487835\n",
      "new best w\n",
      "iterations:  2485\n",
      "Training loss: 0.004095434959208296, Validation loss: 0.011360647987791888\n",
      "iterations:  2486\n",
      "Training loss: 0.004131829260156044, Validation loss: 0.00785663878797144\n",
      "iterations:  2487\n",
      "Training loss: 0.004116988513904456, Validation loss: 0.011454899630463254\n",
      "iterations:  2488\n",
      "Training loss: 0.004137740552453373, Validation loss: 0.00783929385299138\n",
      "new best w\n",
      "iterations:  2489\n",
      "Training loss: 0.004124072980256388, Validation loss: 0.011498806855291392\n",
      "iterations:  2490\n",
      "Training loss: 0.0041704483841210095, Validation loss: 0.007840288216934438\n",
      "iterations:  2491\n",
      "Training loss: 0.004153903445736283, Validation loss: 0.011631512058268971\n",
      "iterations:  2492\n",
      "Training loss: 0.004182415846784125, Validation loss: 0.007839996953719692\n",
      "iterations:  2493\n",
      "Training loss: 0.004167623874840032, Validation loss: 0.01169717799705449\n",
      "iterations:  2494\n",
      "Training loss: 0.00420194026251612, Validation loss: 0.007838501659371335\n",
      "new best w\n",
      "iterations:  2495\n",
      "Training loss: 0.004186582864538295, Validation loss: 0.011784020787520846\n",
      "iterations:  2496\n",
      "Training loss: 0.004234145013818055, Validation loss: 0.007853227047575961\n",
      "iterations:  2497\n",
      "Training loss: 0.004219248793477268, Validation loss: 0.01191338464893114\n",
      "iterations:  2498\n",
      "Training loss: 0.00424789935096227, Validation loss: 0.007840698232494881\n",
      "iterations:  2499\n",
      "Training loss: 0.00423280885700903, Validation loss: 0.011981770646318854\n",
      "iterations:  2500\n",
      "Training loss: 0.004266714582261437, Validation loss: 0.007850761152825778\n",
      "iterations:  2501\n",
      "Training loss: 0.00425354642371658, Validation loss: 0.012070886716350319\n",
      "iterations:  2502\n",
      "Training loss: 0.0042957471890478025, Validation loss: 0.007856703852962344\n",
      "iterations:  2503\n",
      "Training loss: 0.004279472922434514, Validation loss: 0.012180265547347458\n",
      "iterations:  2504\n",
      "Training loss: 0.004332777928003894, Validation loss: 0.007883072505978292\n",
      "iterations:  2505\n",
      "Training loss: 0.004317096050842952, Validation loss: 0.012323121210654103\n",
      "iterations:  2506\n",
      "Training loss: 0.004352786028475472, Validation loss: 0.007879735065039478\n",
      "iterations:  2507\n",
      "Training loss: 0.004336501846634304, Validation loss: 0.01241000965699889\n",
      "iterations:  2508\n",
      "Training loss: 0.0044041390648863095, Validation loss: 0.007917297036513738\n",
      "iterations:  2509\n",
      "Training loss: 0.004386432422615135, Validation loss: 0.012601205510013699\n",
      "iterations:  2510\n",
      "Training loss: 0.004427529173614497, Validation loss: 0.007935779764203517\n",
      "iterations:  2511\n",
      "Training loss: 0.004411274837360996, Validation loss: 0.012700005167877495\n",
      "iterations:  2512\n",
      "Training loss: 0.004459072554571628, Validation loss: 0.007955652692066616\n",
      "iterations:  2513\n",
      "Training loss: 0.004444752513813714, Validation loss: 0.012825537329020881\n"
     ]
    }
   ],
   "source": [
    "mlp_sinc = NNregressor_onelayer(activation_function = 'relu')\n",
    "mlp_sinc.estimate_weights(trainx, trainy, valx, valy, n_hidden=100, \n",
    "                              iterations=10000, patience=10, rate=0.05, \n",
    "                              verbose=True, weight_initialization_factors=[0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some magic to list object variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weights', 'activation_function', 'training_loss', 'validation_loss', 'gradient_norm', 'iterations'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_sinc.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the gradient norm as a function of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_magnitude = mlp_sinc.gradient_norm\n",
    "iteration_vector = np.arange(mlp_sinc.iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(iteration_vector, gradient_magnitude, s=1)\n",
    "plt.title('Gradient magnitude over training iterations.')\n",
    "plt.ylabel('Gradient norm')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "training_loss = mlp_sinc.training_loss\n",
    "validation_loss = mlp_sinc.validation_loss\n",
    "plt.scatter(iteration_vector, training_loss, s=3, c='lime', label='Train')\n",
    "plt.scatter(iteration_vector, validation_loss, s=3, c='fuchsia', label='Validation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see learnt function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(-15,15, 0.05)\n",
    "sinc_out = np.sin(x_range)/x_range\n",
    "neuralnetwork_out = mlp_sinc.predict(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_range, sinc_out, s=3, c='lime', label='Sinc(x)')\n",
    "plt.scatter(x_range, neuralnetwork_out, s=3, c='fuchsia', label='NeuralNetwork(x)')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
