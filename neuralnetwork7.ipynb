{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train=np.loadtxt('sincTrain25.dt')\n",
    "trainx=train[:,0]\n",
    "trainy=train[:,1]\n",
    "valid=np.loadtxt('sincValidate10.dt')\n",
    "valx=valid[:,0]\n",
    "valy=valid[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training data and validation data (they are generated from the function sinc(x)=sin(x)/x)so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.subplots_adjust(hspace=0.7)\n",
    "ax[0].scatter(trainx, trainy, s=2)\n",
    "ax[0].set_title('Training data')\n",
    "ax[0].set_ylabel('Sinc(x)')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[1].scatter(valx, valy, s=2)\n",
    "ax[1].set_title('Validation data')\n",
    "ax[1].set_ylabel('Sinc(x)')\n",
    "ax[1].set_xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network function and gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x, w, activation_function, has_ones = False):\n",
    "    w0 = w[0]\n",
    "    w1 = w[1]\n",
    "    if len(x.shape)<1:\n",
    "        x = x.reshape(1,1)\n",
    "    N = x.shape[0]\n",
    "    if not has_ones:\n",
    "        x = np.c_[np.ones(N), x]\n",
    "    a = x@w0 # dimenson is d*M where M is number of columns in w0, i.e. number of neurons\n",
    "    if activation_function == 'relu':\n",
    "        z = relu(a)\n",
    "    elif activation_function == 'softsign':\n",
    "        z = a/(1+np.abs(a))\n",
    "    z = np.c_[np.ones(z.shape[0]), z] # ones for bias\n",
    "    out = z@w1\n",
    "    return(dict(first_mult = a, first_mult_nonlin = z, second_mult = out))\n",
    "\n",
    "def relu(x):\n",
    "    zeroes = np.zeros(x.shape)\n",
    "    zeroes[x>0] = x[x>0]\n",
    "    return(zeroes)\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = (x > 0)*1\n",
    "    return(grad)\n",
    "\n",
    "def nn_grad(x, y, a, z, out, w, activation_function):\n",
    "    '''y must be an array of dimension at least 2'''\n",
    "    w_out = w[1]\n",
    "    N = x.shape[0]\n",
    "    delta_outs = out-y # N*1\n",
    "    qq = np.zeros(21)\n",
    "    delta_outs_repeated = np.repeat((out-y), w_out.shape[0], axis=1) # N*(n_hidden+1)\n",
    "    output_grad = np.sum(np.multiply(delta_outs, z), axis=0)/N # gradient of output unit\n",
    "    if len(output_grad.shape)<2:\n",
    "        output_grad = output_grad.reshape(output_grad.shape[0], 1)\n",
    "    if activation_function == 'relu':\n",
    "        hidden_activation_deriv = relu_grad(a)\n",
    "    elif activation_function == 'softsign':\n",
    "        hidden_activation_deriv = 1/((1+np.abs(a))**2)\n",
    "    delta_hidden_sum_parts = delta_outs@w_out.T # this must be a sum with more than 1 output neuron\n",
    "    delta_hidden_sum_parts = delta_hidden_sum_parts[:,1:] # removing bias column\n",
    "    delta_hidden = hidden_activation_deriv*delta_hidden_sum_parts\n",
    "    hidden_grad = 0\n",
    "    for i in range(N):\n",
    "        vector_of_deltas = delta_hidden[i,:]\n",
    "        vector_of_deltas = vector_of_deltas.reshape(a.shape[1],1)\n",
    "        grad_element = x[i,:]*vector_of_deltas\n",
    "        hidden_grad += grad_element/N\n",
    "    return([hidden_grad.T, output_grad])\n",
    "\n",
    "def nn_gradient_descent(x_train, y_train, x_val, y_val, n_hidden, rate, iterations, patience,\n",
    "                       verbose, weights, initialization_factors, activation_function):\n",
    "    if len(y_train.shape)==1:\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "    if len(y_val.shape)==1:\n",
    "        y_val = y_val.reshape(y_val.shape[0], 1)\n",
    "    if len(x_train.shape)==1:\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "    K = y_train.shape[1]\n",
    "    D = x_train.shape[1]\n",
    "    N = x_train.shape[0]\n",
    "    x_train = np.c_[np.ones(N), x_train, ]\n",
    "    # initialize weights\n",
    "    if initialization_factors == None:\n",
    "        initialization_factors = [np.sqrt(2/(D+1 + n_hidden)), np.sqrt(2/(n_hidden+1 + K))]\n",
    "    if weights == None:\n",
    "        w_hidden0 = np.random.normal(0, 1, (D+1, n_hidden)) * initialization_factors[0]\n",
    "        w_out0 = np.random.normal(0, 1, (n_hidden+1, K)) * initialization_factors[1]\n",
    "    w_hidden1 = None\n",
    "    w_out1 = None\n",
    "    w_best = None\n",
    "    \n",
    "    patience_counter = patience\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    gradient_norm = []\n",
    "    max_iter = iterations\n",
    "    while iterations > 0 and patience_counter >0:\n",
    "        w=[w_hidden0, w_out0]\n",
    "        nn_outs_train = neural_network(x_train, w, activation_function, has_ones=True)\n",
    "        a = nn_outs_train['first_mult']\n",
    "        z = nn_outs_train['first_mult_nonlin']\n",
    "        out = nn_outs_train['second_mult']\n",
    "        train_error = 0.5*np.sum((y_train - out)**2)/N\n",
    "        if len(out.shape)<2:\n",
    "            out=out.reshape(y_train.shape[0], 1) # reshaping to get broadcasting to work later\n",
    "        train_loss.append(train_error)\n",
    "        delta_outs = (out-y_train) # N*1\n",
    "        grads = nn_grad(x=x_train, y=y_train, a=a, z=z, out=out, w=w, activation_function=activation_function)\n",
    "        hidden_grad = grads[0]\n",
    "        output_grad = grads[1]\n",
    "        w_hidden1 = w_hidden0 - rate*hidden_grad #/N\n",
    "        w_out1 = w_out0 - rate*output_grad #/N\n",
    "        val_out = neural_network(x_val, w, activation_function=activation_function)['second_mult']\n",
    "        val_error = 0.5*np.sum((y_val - val_out)**2)/x_val.shape[0]\n",
    "        iterations -= 1\n",
    "        if verbose: \n",
    "            print('iterations: ', max_iter-iterations)\n",
    "            print('Training loss: {}, Validation loss: {}'.format(train_error, val_error))\n",
    "        if len(val_loss)>1:\n",
    "            if val_error < min(val_loss):\n",
    "                if verbose: \n",
    "                    print('new best w')\n",
    "                w_best = w\n",
    "                patience_counter = patience\n",
    "            elif val_error >= val_loss[-1]:\n",
    "                patience_counter -= 1\n",
    "        val_loss.append(val_error)\n",
    "        w_out0 = w_out1\n",
    "        w_hidden0 = w_hidden1\n",
    "        gradient_norm_i = np.sqrt(np.sum(np.concatenate([w_hidden0.flatten(), w_out0.flatten()])**2))\n",
    "        gradient_norm.append(gradient_norm_i)\n",
    "    if w_best == None:\n",
    "        w_best = [w_hidden0, w_out0]\n",
    "    return(dict(weights=w_best, train_loss=train_loss, val_loss=val_loss, gradient_norm=gradient_norm,\n",
    "               iterations=max_iter-iterations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the things into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNregressor_onelayer:\n",
    "    def __init__(self, activation_function, weights=None):\n",
    "        self.weights = weights\n",
    "        self.activation_function = activation_function\n",
    "    def estimate_weights(self, trainx, trainy, valx, valy, n_hidden, rate, iterations, patience, verbose,\n",
    "                        weight_initialization_factors):\n",
    "        training_results = nn_gradient_descent(trainx, trainy, valx, valy, n_hidden, \n",
    "                                           rate, iterations, patience, verbose,\n",
    "                                           self.weights, weight_initialization_factors,\n",
    "                                               activation_function = self.activation_function)\n",
    "        self.weights = training_results['weights']\n",
    "        self.training_loss = training_results['train_loss']\n",
    "        self.validation_loss = training_results['val_loss']\n",
    "        self.gradient_norm = training_results['gradient_norm']\n",
    "        self.iterations = training_results['iterations']\n",
    "    def predict(self, x):\n",
    "        predictions = neural_network(x, self.weights, activation_function = self.activation_function)\n",
    "        return(predictions['second_mult'].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to validate the gradient. We make up some random weights and calculate some outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173.37861170911933"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(84)\n",
    "wtest1=np.random.normal(0,1,(2,20))\n",
    "wtest2=np.random.normal(0,1,(21,1))\n",
    "test_out=neural_network(trainx,[wtest1,wtest2], 'relu')\n",
    "test_a = test_out['first_mult']\n",
    "test_z = test_out['first_mult_nonlin']\n",
    "test_pred = test_out['second_mult']\n",
    "test_error = 0.5*np.sum((test_pred - trainy.reshape(trainy.shape[0], 1))**2)/25\n",
    "test_grad = nn_grad(np.c_[np.ones(25), trainx.reshape(25,1)], trainy.reshape(25,1), \n",
    "                    test_a, test_z, test_pred, [wtest1, wtest2], 'relu')\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a bunch of for loops to check the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_grad_out_all = np.zeros((21,1))\n",
    "error_grad_hidden_all = np.zeros((2, 20))\n",
    "for n in range(25):\n",
    "    eps_val = np.sqrt(np.finfo(float).eps)\n",
    "    error_grad_out = []\n",
    "    for i in range(wtest2.shape[0]):\n",
    "        for j in range(wtest2.shape[1]):\n",
    "            wtest2_2 = np.copy(wtest2)\n",
    "            wtest2_2[i,j] = wtest2_2[i, j] + eps_val\n",
    "            pred_noneps = neural_network(trainx[n], [wtest1, wtest2], 'relu')['second_mult'].reshape(1,1)\n",
    "            pred_eps = neural_network(trainx[n], [wtest1, wtest2_2], 'relu')['second_mult'].reshape(1,1)\n",
    "            error_noneps = 0.5*(pred_noneps-trainy[n])**2\n",
    "            error_eps = 0.5*(pred_eps-trainy[n])**2\n",
    "            error_grad_ij = (error_eps-error_noneps)/eps_val\n",
    "            error_grad_out.append(error_grad_ij)\n",
    "    error_grad_hidden = []\n",
    "    for i in range(wtest1.shape[0]):\n",
    "        for j in range(wtest1.shape[1]):\n",
    "            wtest1_2 = np.copy(wtest1)\n",
    "            wtest1_2[i,j] = wtest1_2[i, j] + eps_val\n",
    "            pred_noneps = neural_network(trainx[n], [wtest1, wtest2], 'relu')['second_mult'].reshape(1,1)\n",
    "            pred_eps = neural_network(trainx[n], [wtest1_2, wtest2], 'relu')['second_mult'].reshape(1,1)\n",
    "            error_noneps = 0.5*(pred_noneps-trainy[n])**2\n",
    "            error_eps = 0.5*(pred_eps-trainy[n])**2\n",
    "            error_grad_ij = (error_eps-error_noneps)/eps_val\n",
    "            error_grad_hidden.append(error_grad_ij)\n",
    "    error_grad_hidden_all+=(np.array(error_grad_hidden).reshape(2,20))/25\n",
    "    error_grad_out_all+=np.array(error_grad_out).reshape(21,1)/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000021, 1.00000008, 1.00000001, 1.00000011, 1.        ,\n",
       "        0.9999999 , 0.99999983, 0.99999999, 1.00000001, 1.00000515,\n",
       "        0.99999986, 1.        , 0.99999937, 0.9999999 , 1.00000002,\n",
       "        0.99999977, 0.99999993, 0.99999998, 1.        , 1.00000002],\n",
       "       [0.99999993, 0.99999997, 1.        , 0.99999998, 1.        ,\n",
       "        1.00000001, 1.00000001, 1.00000001, 0.99999999, 0.99999943,\n",
       "        1.00000002, 0.99999998, 1.00000014, 1.00000002, 1.        ,\n",
       "        1.00000006, 0.99999999, 1.00000001, 1.        , 1.        ]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_grad[0]+np.finfo(float).eps)/(error_grad_hidden_all+np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999999, 1.00000001, 1.00000003, 0.99999999, 1.00000008,\n",
       "        1.        , 1.        , 1.00000001, 1.00000003, 0.99999999,\n",
       "        1.00000002, 1.00000002, 1.00000001, 1.00000001, 1.00000001,\n",
       "        0.99999999, 0.99999997, 1.        , 0.99999999, 1.00000001,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((test_grad[1]+np.finfo(float).eps)/(error_grad_out_all+np.finfo(float).eps)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checks out. Let's make a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1\n",
      "Training loss: 0.11935339825862956, Validation loss: 0.23002532007794446\n",
      "iterations:  2\n",
      "Training loss: 0.0516400775430421, Validation loss: 0.09760446732779782\n",
      "iterations:  3\n",
      "Training loss: 0.04700884118202189, Validation loss: 0.08086725557605871\n",
      "new best w\n",
      "iterations:  4\n",
      "Training loss: 0.04591573940921677, Validation loss: 0.07682663757784478\n",
      "new best w\n",
      "iterations:  5\n",
      "Training loss: 0.04515548653065247, Validation loss: 0.07507783955707727\n",
      "new best w\n",
      "iterations:  6\n",
      "Training loss: 0.04445453186031382, Validation loss: 0.07390379796609378\n",
      "new best w\n",
      "iterations:  7\n",
      "Training loss: 0.04378512906402175, Validation loss: 0.07291305282882843\n",
      "new best w\n",
      "iterations:  8\n",
      "Training loss: 0.04314486620949521, Validation loss: 0.07199820814471623\n",
      "new best w\n",
      "iterations:  9\n",
      "Training loss: 0.04253906223760406, Validation loss: 0.07110178903494604\n",
      "new best w\n",
      "iterations:  10\n",
      "Training loss: 0.0419636036374888, Validation loss: 0.07024789059547731\n",
      "new best w\n",
      "iterations:  11\n",
      "Training loss: 0.041412213396085536, Validation loss: 0.06940476644601346\n",
      "new best w\n",
      "iterations:  12\n",
      "Training loss: 0.04088511782182022, Validation loss: 0.06861787390700048\n",
      "new best w\n",
      "iterations:  13\n",
      "Training loss: 0.04037713989462031, Validation loss: 0.06788266295614945\n",
      "new best w\n",
      "iterations:  14\n",
      "Training loss: 0.039872016213431886, Validation loss: 0.06709208805003383\n",
      "new best w\n",
      "iterations:  15\n",
      "Training loss: 0.039387319478788554, Validation loss: 0.06636613112667947\n",
      "new best w\n",
      "iterations:  16\n",
      "Training loss: 0.038922804493446414, Validation loss: 0.06565700815863693\n",
      "new best w\n",
      "iterations:  17\n",
      "Training loss: 0.038477086489626874, Validation loss: 0.06498530487336199\n",
      "new best w\n",
      "iterations:  18\n",
      "Training loss: 0.038049532292945644, Validation loss: 0.06432108541279552\n",
      "new best w\n",
      "iterations:  19\n",
      "Training loss: 0.037638733628399416, Validation loss: 0.06365992760186931\n",
      "new best w\n",
      "iterations:  20\n",
      "Training loss: 0.03724362291821974, Validation loss: 0.06305476469977545\n",
      "new best w\n",
      "iterations:  21\n",
      "Training loss: 0.036864342856651564, Validation loss: 0.06244670929707612\n",
      "new best w\n",
      "iterations:  22\n",
      "Training loss: 0.03649973310728886, Validation loss: 0.061880201838426116\n",
      "new best w\n",
      "iterations:  23\n",
      "Training loss: 0.03614926885643019, Validation loss: 0.0613176803558695\n",
      "new best w\n",
      "iterations:  24\n",
      "Training loss: 0.03581263201688054, Validation loss: 0.060769478221437835\n",
      "new best w\n",
      "iterations:  25\n",
      "Training loss: 0.03548740366533418, Validation loss: 0.06024160640889485\n",
      "new best w\n",
      "iterations:  26\n",
      "Training loss: 0.0351705832552549, Validation loss: 0.05972042999644045\n",
      "new best w\n",
      "iterations:  27\n",
      "Training loss: 0.03486801741063797, Validation loss: 0.05922615526767131\n",
      "new best w\n",
      "iterations:  28\n",
      "Training loss: 0.034577136713831884, Validation loss: 0.05873381748054098\n",
      "new best w\n",
      "iterations:  29\n",
      "Training loss: 0.03429678097553486, Validation loss: 0.05827725825537346\n",
      "new best w\n",
      "iterations:  30\n",
      "Training loss: 0.03401915693693613, Validation loss: 0.05782515647575789\n",
      "new best w\n",
      "iterations:  31\n",
      "Training loss: 0.03375275502561808, Validation loss: 0.05737963696310398\n",
      "new best w\n",
      "iterations:  32\n",
      "Training loss: 0.033495050494965616, Validation loss: 0.05696596782452314\n",
      "new best w\n",
      "iterations:  33\n",
      "Training loss: 0.03324684763400886, Validation loss: 0.05654342432606545\n",
      "new best w\n",
      "iterations:  34\n",
      "Training loss: 0.03300745240645076, Validation loss: 0.05614806480665548\n",
      "new best w\n",
      "iterations:  35\n",
      "Training loss: 0.032775733595769574, Validation loss: 0.05574669581609311\n",
      "new best w\n",
      "iterations:  36\n",
      "Training loss: 0.03255237871046974, Validation loss: 0.05537426609620637\n",
      "new best w\n",
      "iterations:  37\n",
      "Training loss: 0.03233646642224772, Validation loss: 0.05499827562091014\n",
      "new best w\n",
      "iterations:  38\n",
      "Training loss: 0.03212841611685874, Validation loss: 0.05464901950686751\n",
      "new best w\n",
      "iterations:  39\n",
      "Training loss: 0.031927076606724836, Validation loss: 0.054292676354786\n",
      "new best w\n",
      "iterations:  40\n",
      "Training loss: 0.03173294126303212, Validation loss: 0.053957744669956356\n",
      "new best w\n",
      "iterations:  41\n",
      "Training loss: 0.03154485145578861, Validation loss: 0.053638825359599365\n",
      "new best w\n",
      "iterations:  42\n",
      "Training loss: 0.03136333989833533, Validation loss: 0.05331926162811956\n",
      "new best w\n",
      "iterations:  43\n",
      "Training loss: 0.031187346958374995, Validation loss: 0.05301938684934902\n",
      "new best w\n",
      "iterations:  44\n",
      "Training loss: 0.03101717707675962, Validation loss: 0.05271506562402088\n",
      "new best w\n",
      "iterations:  45\n",
      "Training loss: 0.030851126764539535, Validation loss: 0.05243080695675428\n",
      "new best w\n",
      "iterations:  46\n",
      "Training loss: 0.030690855508155913, Validation loss: 0.05213827931541615\n",
      "new best w\n",
      "iterations:  47\n",
      "Training loss: 0.030535574018646674, Validation loss: 0.05187054403286888\n",
      "new best w\n",
      "iterations:  48\n",
      "Training loss: 0.03038571332061007, Validation loss: 0.05158853921346185\n",
      "new best w\n",
      "iterations:  49\n",
      "Training loss: 0.03024114553945144, Validation loss: 0.05133414160738307\n",
      "new best w\n",
      "iterations:  50\n",
      "Training loss: 0.030101935384021377, Validation loss: 0.05107472407524982\n",
      "new best w\n",
      "iterations:  51\n",
      "Training loss: 0.029968161816779196, Validation loss: 0.05083368236195322\n",
      "new best w\n",
      "iterations:  52\n",
      "Training loss: 0.029841560222545152, Validation loss: 0.0505817973996861\n",
      "new best w\n",
      "iterations:  53\n",
      "Training loss: 0.029720201453321504, Validation loss: 0.050354494606039135\n",
      "new best w\n",
      "iterations:  54\n",
      "Training loss: 0.029602292615294835, Validation loss: 0.05011619516308448\n",
      "new best w\n",
      "iterations:  55\n",
      "Training loss: 0.029487886666942652, Validation loss: 0.049901202889972844\n",
      "new best w\n",
      "iterations:  56\n",
      "Training loss: 0.029377849645862795, Validation loss: 0.0496781826392535\n",
      "new best w\n",
      "iterations:  57\n",
      "Training loss: 0.0292719546686713, Validation loss: 0.049475117528539794\n",
      "new best w\n",
      "iterations:  58\n",
      "Training loss: 0.029168935314697348, Validation loss: 0.04925907225505697\n",
      "new best w\n",
      "iterations:  59\n",
      "Training loss: 0.029068753910619874, Validation loss: 0.04905963931121721\n",
      "new best w\n",
      "iterations:  60\n",
      "Training loss: 0.028971837634175265, Validation loss: 0.04885998303477296\n",
      "new best w\n",
      "iterations:  61\n",
      "Training loss: 0.028876298548456535, Validation loss: 0.04866680794390967\n",
      "new best w\n",
      "iterations:  62\n",
      "Training loss: 0.028784363143017383, Validation loss: 0.048472685512839696\n",
      "new best w\n",
      "iterations:  63\n",
      "Training loss: 0.028694426378854346, Validation loss: 0.04829571485189889\n",
      "new best w\n",
      "iterations:  64\n",
      "Training loss: 0.02860652099722789, Validation loss: 0.04810537729233951\n",
      "new best w\n",
      "iterations:  65\n",
      "Training loss: 0.028521095476840026, Validation loss: 0.04793807578422605\n",
      "new best w\n",
      "iterations:  66\n",
      "Training loss: 0.02843734571599916, Validation loss: 0.0477575105040672\n",
      "new best w\n",
      "iterations:  67\n",
      "Training loss: 0.02835238320210183, Validation loss: 0.04758987937265985\n",
      "new best w\n",
      "iterations:  68\n",
      "Training loss: 0.028269186131347754, Validation loss: 0.04743166742270803\n",
      "new best w\n",
      "iterations:  69\n",
      "Training loss: 0.02818862840840865, Validation loss: 0.04725015636303166\n",
      "new best w\n",
      "iterations:  70\n",
      "Training loss: 0.02810941766486835, Validation loss: 0.04710228428091944\n",
      "new best w\n",
      "iterations:  71\n",
      "Training loss: 0.028032990720258645, Validation loss: 0.046931055882217236\n",
      "new best w\n",
      "iterations:  72\n",
      "Training loss: 0.027957415526543558, Validation loss: 0.04679388882932464\n",
      "new best w\n",
      "iterations:  73\n",
      "Training loss: 0.027884611357265828, Validation loss: 0.046636666492535554\n",
      "new best w\n",
      "iterations:  74\n",
      "Training loss: 0.027812367498663382, Validation loss: 0.04648899202723776\n",
      "new best w\n",
      "iterations:  75\n",
      "Training loss: 0.02774090110232542, Validation loss: 0.04633954772792856\n",
      "new best w\n",
      "iterations:  76\n",
      "Training loss: 0.02767079659326223, Validation loss: 0.04619648635861219\n",
      "new best w\n",
      "iterations:  77\n",
      "Training loss: 0.02760214408290826, Validation loss: 0.046057848362646085\n",
      "new best w\n",
      "iterations:  78\n",
      "Training loss: 0.02753609380757008, Validation loss: 0.04592640411503148\n",
      "new best w\n",
      "iterations:  79\n",
      "Training loss: 0.027470905279312174, Validation loss: 0.04579014966338952\n",
      "new best w\n",
      "iterations:  80\n",
      "Training loss: 0.027401344356167732, Validation loss: 0.04569437672160353\n",
      "new best w\n",
      "iterations:  81\n",
      "Training loss: 0.027333255856737786, Validation loss: 0.04557975064831478\n",
      "new best w\n",
      "iterations:  82\n",
      "Training loss: 0.027234712542365456, Validation loss: 0.04550449240508781\n",
      "new best w\n",
      "iterations:  83\n",
      "Training loss: 0.027089544974891536, Validation loss: 0.04548994500661804\n",
      "new best w\n",
      "iterations:  84\n",
      "Training loss: 0.026947764050552228, Validation loss: 0.04539680328075053\n",
      "new best w\n",
      "iterations:  85\n",
      "Training loss: 0.02683019646930101, Validation loss: 0.04526303643188374\n",
      "new best w\n",
      "iterations:  86\n",
      "Training loss: 0.02671355766812256, Validation loss: 0.04516989797602591\n",
      "new best w\n",
      "iterations:  87\n",
      "Training loss: 0.026607724108745442, Validation loss: 0.04506379282400579\n",
      "new best w\n",
      "iterations:  88\n",
      "Training loss: 0.02652786184444735, Validation loss: 0.044919270998465176\n",
      "new best w\n",
      "iterations:  89\n",
      "Training loss: 0.02644317743746784, Validation loss: 0.04478033311411293\n",
      "new best w\n",
      "iterations:  90\n",
      "Training loss: 0.026360078836228658, Validation loss: 0.044668066224584385\n",
      "new best w\n",
      "iterations:  91\n",
      "Training loss: 0.02628589869554647, Validation loss: 0.04455803162672097\n",
      "new best w\n",
      "iterations:  92\n",
      "Training loss: 0.02621973484138754, Validation loss: 0.044428063071467475\n",
      "new best w\n",
      "iterations:  93\n",
      "Training loss: 0.026155033098006823, Validation loss: 0.044317698119000284\n",
      "new best w\n",
      "iterations:  94\n",
      "Training loss: 0.026091048991465922, Validation loss: 0.04421279896959266\n",
      "new best w\n",
      "iterations:  95\n",
      "Training loss: 0.026027551558238528, Validation loss: 0.0441167757633582\n",
      "new best w\n",
      "iterations:  96\n",
      "Training loss: 0.025970494426905556, Validation loss: 0.0440003741690512\n",
      "new best w\n",
      "iterations:  97\n",
      "Training loss: 0.02591477960144835, Validation loss: 0.04389220605509369\n",
      "new best w\n",
      "iterations:  98\n",
      "Training loss: 0.025860235701082087, Validation loss: 0.043794336544208404\n",
      "new best w\n",
      "iterations:  99\n",
      "Training loss: 0.02580766776536309, Validation loss: 0.04370590342452251\n",
      "new best w\n",
      "iterations:  100\n",
      "Training loss: 0.025756602105001813, Validation loss: 0.04360111451755442\n",
      "new best w\n",
      "iterations:  101\n",
      "Training loss: 0.02570621244542588, Validation loss: 0.043512788546464023\n",
      "new best w\n",
      "iterations:  102\n",
      "Training loss: 0.025656816950996083, Validation loss: 0.043428942092234066\n",
      "new best w\n",
      "iterations:  103\n",
      "Training loss: 0.02560821421814726, Validation loss: 0.04333547931014936\n",
      "new best w\n",
      "iterations:  104\n",
      "Training loss: 0.0255603990794154, Validation loss: 0.04325728277397427\n",
      "new best w\n",
      "iterations:  105\n",
      "Training loss: 0.025513391558166672, Validation loss: 0.043171616473507\n",
      "new best w\n",
      "iterations:  106\n",
      "Training loss: 0.025466643246782737, Validation loss: 0.0430910410722894\n",
      "new best w\n",
      "iterations:  107\n",
      "Training loss: 0.025420562578805518, Validation loss: 0.04301862154629036\n",
      "new best w\n",
      "iterations:  108\n",
      "Training loss: 0.025375225199224184, Validation loss: 0.04293678699772005\n",
      "new best w\n",
      "iterations:  109\n",
      "Training loss: 0.025330306957758075, Validation loss: 0.042865034345504524\n",
      "new best w\n",
      "iterations:  110\n",
      "Training loss: 0.02528606832562164, Validation loss: 0.0427953321575006\n",
      "new best w\n",
      "iterations:  111\n",
      "Training loss: 0.025242756425056113, Validation loss: 0.04271809600011067\n",
      "new best w\n",
      "iterations:  112\n",
      "Training loss: 0.02520164791146958, Validation loss: 0.042656503430487346\n",
      "new best w\n",
      "iterations:  113\n",
      "Training loss: 0.02516120098388678, Validation loss: 0.04259385875708808\n",
      "new best w\n",
      "iterations:  114\n",
      "Training loss: 0.025121394159021312, Validation loss: 0.04252238548234846\n",
      "new best w\n",
      "iterations:  115\n",
      "Training loss: 0.025081910777520314, Validation loss: 0.04245425207924415\n",
      "new best w\n",
      "iterations:  116\n",
      "Training loss: 0.02504298550039309, Validation loss: 0.04239940450008813\n",
      "new best w\n",
      "iterations:  117\n",
      "Training loss: 0.025004388213934355, Validation loss: 0.04232796405820115\n",
      "new best w\n",
      "iterations:  118\n",
      "Training loss: 0.024966108742982843, Validation loss: 0.042265467323676705\n",
      "new best w\n",
      "iterations:  119\n",
      "Training loss: 0.024929807228771653, Validation loss: 0.04221090003110781\n",
      "new best w\n",
      "iterations:  120\n",
      "Training loss: 0.02489846071607931, Validation loss: 0.042176099378944176\n",
      "new best w\n",
      "iterations:  121\n",
      "Training loss: 0.02486722914178026, Validation loss: 0.04212791138853748\n",
      "new best w\n",
      "iterations:  122\n",
      "Training loss: 0.02483633577581557, Validation loss: 0.0420800288254008\n",
      "new best w\n",
      "iterations:  123\n",
      "Training loss: 0.024806146115425455, Validation loss: 0.0420299949451408\n",
      "new best w\n",
      "iterations:  124\n",
      "Training loss: 0.024775931446897412, Validation loss: 0.04197384654075904\n",
      "new best w\n",
      "iterations:  125\n",
      "Training loss: 0.024745900143130984, Validation loss: 0.04192288262215356\n",
      "new best w\n",
      "iterations:  126\n",
      "Training loss: 0.024716393011413963, Validation loss: 0.041876675328933924\n",
      "new best w\n",
      "iterations:  127\n",
      "Training loss: 0.024687533091836644, Validation loss: 0.04181358901389643\n",
      "new best w\n",
      "iterations:  128\n",
      "Training loss: 0.02465948251540995, Validation loss: 0.04176705047001311\n",
      "new best w\n",
      "iterations:  129\n",
      "Training loss: 0.024632175744266377, Validation loss: 0.041752687671954676\n",
      "new best w\n",
      "iterations:  130\n",
      "Training loss: 0.024606162004595623, Validation loss: 0.04170384955020431\n",
      "new best w\n",
      "iterations:  131\n",
      "Training loss: 0.0245798052304363, Validation loss: 0.041645859633899815\n",
      "new best w\n",
      "iterations:  132\n",
      "Training loss: 0.024553576565142832, Validation loss: 0.04162668126960058\n",
      "new best w\n",
      "iterations:  133\n",
      "Training loss: 0.024528076978133438, Validation loss: 0.04158890185406159\n",
      "new best w\n",
      "iterations:  134\n",
      "Training loss: 0.02450280089758161, Validation loss: 0.041499500307045724\n",
      "new best w\n",
      "iterations:  135\n",
      "Training loss: 0.02447681497443355, Validation loss: 0.0414942425721585\n",
      "new best w\n",
      "iterations:  136\n",
      "Training loss: 0.02445177961352467, Validation loss: 0.041468143124814213\n",
      "new best w\n",
      "iterations:  137\n",
      "Training loss: 0.024427243209516725, Validation loss: 0.0414076883409088\n",
      "new best w\n",
      "iterations:  138\n",
      "Training loss: 0.024402221970881, Validation loss: 0.04134811905792236\n",
      "new best w\n",
      "iterations:  139\n",
      "Training loss: 0.024377531311932224, Validation loss: 0.041340457957493526\n",
      "new best w\n",
      "iterations:  140\n",
      "Training loss: 0.024353398939504847, Validation loss: 0.04128593631424104\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  141\n",
      "Training loss: 0.024328881949925996, Validation loss: 0.04123275617134471\n",
      "new best w\n",
      "iterations:  142\n",
      "Training loss: 0.02430452423256869, Validation loss: 0.041224203524118035\n",
      "new best w\n",
      "iterations:  143\n",
      "Training loss: 0.024280773691130166, Validation loss: 0.041172793632176394\n",
      "new best w\n",
      "iterations:  144\n",
      "Training loss: 0.02425687971474871, Validation loss: 0.041112727351446755\n",
      "new best w\n",
      "iterations:  145\n",
      "Training loss: 0.024232831339818236, Validation loss: 0.041109418325095466\n",
      "new best w\n",
      "iterations:  146\n",
      "Training loss: 0.02420933273000197, Validation loss: 0.04105793339290995\n",
      "new best w\n",
      "iterations:  147\n",
      "Training loss: 0.024185377601846227, Validation loss: 0.04100579010574167\n",
      "new best w\n",
      "iterations:  148\n",
      "Training loss: 0.024161204323200694, Validation loss: 0.04099907072869619\n",
      "new best w\n",
      "iterations:  149\n",
      "Training loss: 0.02413766276938457, Validation loss: 0.04094953436361068\n",
      "new best w\n",
      "iterations:  150\n",
      "Training loss: 0.02411401294754242, Validation loss: 0.04089783651230678\n",
      "new best w\n",
      "iterations:  151\n",
      "Training loss: 0.02408965079784429, Validation loss: 0.040883726129561544\n",
      "new best w\n",
      "iterations:  152\n",
      "Training loss: 0.02406600528816132, Validation loss: 0.040836354942066846\n",
      "new best w\n",
      "iterations:  153\n",
      "Training loss: 0.0240422801728265, Validation loss: 0.0407896134913686\n",
      "new best w\n",
      "iterations:  154\n",
      "Training loss: 0.024018227079294352, Validation loss: 0.04077972006399637\n",
      "new best w\n",
      "iterations:  155\n",
      "Training loss: 0.023994966739224632, Validation loss: 0.04073287249111588\n",
      "new best w\n",
      "iterations:  156\n",
      "Training loss: 0.02397144799512243, Validation loss: 0.04069057072378814\n",
      "new best w\n",
      "iterations:  157\n",
      "Training loss: 0.02394792358453798, Validation loss: 0.04066973111021376\n",
      "new best w\n",
      "iterations:  158\n",
      "Training loss: 0.023925198724995676, Validation loss: 0.04060013622617232\n",
      "new best w\n",
      "iterations:  159\n",
      "Training loss: 0.02390157257256093, Validation loss: 0.0406024104944053\n",
      "iterations:  160\n",
      "Training loss: 0.023878456543391685, Validation loss: 0.040577345141684276\n",
      "new best w\n",
      "iterations:  161\n",
      "Training loss: 0.023855758975600966, Validation loss: 0.04050159371473905\n",
      "new best w\n",
      "iterations:  162\n",
      "Training loss: 0.023832468353401928, Validation loss: 0.040507347066131495\n",
      "iterations:  163\n",
      "Training loss: 0.023809794293173637, Validation loss: 0.04046357387581452\n",
      "new best w\n",
      "iterations:  164\n",
      "Training loss: 0.02378721233861891, Validation loss: 0.04041445744427524\n",
      "new best w\n",
      "iterations:  165\n",
      "Training loss: 0.02376401794277334, Validation loss: 0.040405407284510156\n",
      "new best w\n",
      "iterations:  166\n",
      "Training loss: 0.023741674304331203, Validation loss: 0.04036229529595032\n",
      "new best w\n",
      "iterations:  167\n",
      "Training loss: 0.023719150792731437, Validation loss: 0.04031996415435499\n",
      "new best w\n",
      "iterations:  168\n",
      "Training loss: 0.023696231230623796, Validation loss: 0.04031258553052863\n",
      "new best w\n",
      "iterations:  169\n",
      "Training loss: 0.023674200986411514, Validation loss: 0.04023885985496324\n",
      "new best w\n",
      "iterations:  170\n",
      "Training loss: 0.023651301769323608, Validation loss: 0.040247153224755874\n",
      "iterations:  171\n",
      "Training loss: 0.02362916197481877, Validation loss: 0.04022257026646624\n",
      "new best w\n",
      "iterations:  172\n",
      "Training loss: 0.023607198959893792, Validation loss: 0.04014064931291766\n",
      "new best w\n",
      "iterations:  173\n",
      "Training loss: 0.023584481914021738, Validation loss: 0.04015313233558853\n",
      "iterations:  174\n",
      "Training loss: 0.023562391117615045, Validation loss: 0.04011207029286971\n",
      "new best w\n",
      "iterations:  175\n",
      "Training loss: 0.023540348591149125, Validation loss: 0.040064954259760886\n",
      "new best w\n",
      "iterations:  176\n",
      "Training loss: 0.023517774466697144, Validation loss: 0.040063461471270864\n",
      "new best w\n",
      "iterations:  177\n",
      "Training loss: 0.023495835409139063, Validation loss: 0.040018584416806294\n",
      "new best w\n",
      "iterations:  178\n",
      "Training loss: 0.023475037062640383, Validation loss: 0.03997970162212215\n",
      "new best w\n",
      "iterations:  179\n",
      "Training loss: 0.02345405973016608, Validation loss: 0.03998926255390347\n",
      "iterations:  180\n",
      "Training loss: 0.02343343682398826, Validation loss: 0.03992112662087839\n",
      "new best w\n",
      "iterations:  181\n",
      "Training loss: 0.02341300165619891, Validation loss: 0.039936583685820724\n",
      "iterations:  182\n",
      "Training loss: 0.023392602450744645, Validation loss: 0.03989636671249359\n",
      "new best w\n",
      "iterations:  183\n",
      "Training loss: 0.023372396153337038, Validation loss: 0.03984049405254439\n",
      "new best w\n",
      "iterations:  184\n",
      "Training loss: 0.023351512645208024, Validation loss: 0.039854720578019934\n",
      "iterations:  185\n",
      "Training loss: 0.023331562864379304, Validation loss: 0.03981874080206857\n",
      "new best w\n",
      "iterations:  186\n",
      "Training loss: 0.023311643746565833, Validation loss: 0.03976056502855026\n",
      "new best w\n",
      "iterations:  187\n",
      "Training loss: 0.02329075588829256, Validation loss: 0.03977435789371499\n",
      "iterations:  188\n",
      "Training loss: 0.023270386759534654, Validation loss: 0.03975593190884445\n",
      "new best w\n",
      "iterations:  189\n",
      "Training loss: 0.023251223633013175, Validation loss: 0.03967298689457075\n",
      "new best w\n",
      "iterations:  190\n",
      "Training loss: 0.023230321762415934, Validation loss: 0.039694874626375734\n",
      "iterations:  191\n",
      "Training loss: 0.023209683205633453, Validation loss: 0.03967584135754298\n",
      "iterations:  192\n",
      "Training loss: 0.02319057170811309, Validation loss: 0.03960864180438306\n",
      "new best w\n",
      "iterations:  193\n",
      "Training loss: 0.023170117901293023, Validation loss: 0.03960728349453553\n",
      "new best w\n",
      "iterations:  194\n",
      "Training loss: 0.023149410531090747, Validation loss: 0.039594685315879445\n",
      "new best w\n",
      "iterations:  195\n",
      "Training loss: 0.023130509306958674, Validation loss: 0.039530731344523055\n",
      "new best w\n",
      "iterations:  196\n",
      "Training loss: 0.02311093189294159, Validation loss: 0.03953106843839032\n",
      "iterations:  197\n",
      "Training loss: 0.023090568041915237, Validation loss: 0.03951922102853968\n",
      "new best w\n",
      "iterations:  198\n",
      "Training loss: 0.023071469233174057, Validation loss: 0.03947474644265795\n",
      "new best w\n",
      "iterations:  199\n",
      "Training loss: 0.02305227240261971, Validation loss: 0.03945062170793529\n",
      "new best w\n",
      "iterations:  200\n",
      "Training loss: 0.02303195033264344, Validation loss: 0.039430286296383274\n",
      "new best w\n",
      "iterations:  201\n",
      "Training loss: 0.023012761047419655, Validation loss: 0.03940577266359406\n",
      "new best w\n",
      "iterations:  202\n",
      "Training loss: 0.022993632140230313, Validation loss: 0.03937497323177869\n",
      "new best w\n",
      "iterations:  203\n",
      "Training loss: 0.022973543419904634, Validation loss: 0.03935597588132841\n",
      "new best w\n",
      "iterations:  204\n",
      "Training loss: 0.022954216893001464, Validation loss: 0.03934923016034039\n",
      "new best w\n",
      "iterations:  205\n",
      "Training loss: 0.022935254665818645, Validation loss: 0.0392910492888348\n",
      "new best w\n",
      "iterations:  206\n",
      "Training loss: 0.02291549520881015, Validation loss: 0.039280869301558406\n",
      "new best w\n",
      "iterations:  207\n",
      "Training loss: 0.0228957462806572, Validation loss: 0.03927413432552575\n",
      "new best w\n",
      "iterations:  208\n",
      "Training loss: 0.02287738094787623, Validation loss: 0.03921815372333658\n",
      "new best w\n",
      "iterations:  209\n",
      "Training loss: 0.02285795476696692, Validation loss: 0.039224038849330616\n",
      "iterations:  210\n",
      "Training loss: 0.0228382131439135, Validation loss: 0.03919480267651919\n",
      "new best w\n",
      "iterations:  211\n",
      "Training loss: 0.02282001108101244, Validation loss: 0.039171915272400416\n",
      "new best w\n",
      "iterations:  212\n",
      "Training loss: 0.022801134405938717, Validation loss: 0.03913526853516212\n",
      "new best w\n",
      "iterations:  213\n",
      "Training loss: 0.022781391986407128, Validation loss: 0.039117310423256606\n",
      "new best w\n",
      "iterations:  214\n",
      "Training loss: 0.02276290745529681, Validation loss: 0.03911379985643013\n",
      "new best w\n",
      "iterations:  215\n",
      "Training loss: 0.022744465821971187, Validation loss: 0.039052800495183025\n",
      "new best w\n",
      "iterations:  216\n",
      "Training loss: 0.0227249391459899, Validation loss: 0.03904422953948694\n",
      "new best w\n",
      "iterations:  217\n",
      "Training loss: 0.02270594509159921, Validation loss: 0.0390400842591647\n",
      "new best w\n",
      "iterations:  218\n",
      "Training loss: 0.022687542214025067, Validation loss: 0.039009941825263274\n",
      "new best w\n",
      "iterations:  219\n",
      "Training loss: 0.02266881410732585, Validation loss: 0.03895232744868271\n",
      "new best w\n",
      "iterations:  220\n",
      "Training loss: 0.0226493147415922, Validation loss: 0.03896308904961902\n",
      "iterations:  221\n",
      "Training loss: 0.02263059851002645, Validation loss: 0.038950090067979365\n",
      "new best w\n",
      "iterations:  222\n",
      "Training loss: 0.022612626802009022, Validation loss: 0.03889839472641728\n",
      "new best w\n",
      "iterations:  223\n",
      "Training loss: 0.022593133469741664, Validation loss: 0.038870248253916946\n",
      "new best w\n",
      "iterations:  224\n",
      "Training loss: 0.02257406533968408, Validation loss: 0.038871020171463704\n",
      "iterations:  225\n",
      "Training loss: 0.022556061218339387, Validation loss: 0.03884680723019121\n",
      "new best w\n",
      "iterations:  226\n",
      "Training loss: 0.022537164324572603, Validation loss: 0.038785057532064356\n",
      "new best w\n",
      "iterations:  227\n",
      "Training loss: 0.0225180392925992, Validation loss: 0.038795622130858506\n",
      "iterations:  228\n",
      "Training loss: 0.022499927144265918, Validation loss: 0.03877201688965922\n",
      "new best w\n",
      "iterations:  229\n",
      "Training loss: 0.022481199521459275, Validation loss: 0.03874356329897952\n",
      "new best w\n",
      "iterations:  230\n",
      "Training loss: 0.022463006371562004, Validation loss: 0.03870636671775858\n",
      "new best w\n",
      "iterations:  231\n",
      "Training loss: 0.022444311664984787, Validation loss: 0.03869395474168623\n",
      "new best w\n",
      "iterations:  232\n",
      "Training loss: 0.02242539829366497, Validation loss: 0.03869090123743034\n",
      "new best w\n",
      "iterations:  233\n",
      "Training loss: 0.022407721754780577, Validation loss: 0.03865424237818791\n",
      "new best w\n",
      "iterations:  234\n",
      "Training loss: 0.022389094170478508, Validation loss: 0.03860020775484327\n",
      "new best w\n",
      "iterations:  235\n",
      "Training loss: 0.022370109964715924, Validation loss: 0.0386126347293104\n",
      "iterations:  236\n",
      "Training loss: 0.022352386367505077, Validation loss: 0.03860662474843676\n",
      "iterations:  237\n",
      "Training loss: 0.022333558460402542, Validation loss: 0.03855200241211101\n",
      "new best w\n",
      "iterations:  238\n",
      "Training loss: 0.022315257762278905, Validation loss: 0.03852564067547999\n",
      "new best w\n",
      "iterations:  239\n",
      "Training loss: 0.02229713385570363, Validation loss: 0.038520118183728864\n",
      "new best w\n",
      "iterations:  240\n",
      "Training loss: 0.02227824023188534, Validation loss: 0.03851842993457435\n",
      "new best w\n",
      "iterations:  241\n",
      "Training loss: 0.022260230402726188, Validation loss: 0.03848468825027997\n",
      "new best w\n",
      "iterations:  242\n",
      "Training loss: 0.022242130770749834, Validation loss: 0.03843004112880447\n",
      "new best w\n",
      "iterations:  243\n",
      "Training loss: 0.02222347640677954, Validation loss: 0.03844712810764357\n",
      "iterations:  244\n",
      "Training loss: 0.022205280622591483, Validation loss: 0.038441934235826224\n",
      "iterations:  245\n",
      "Training loss: 0.02218680777237677, Validation loss: 0.038404289997383816\n",
      "new best w\n",
      "iterations:  246\n",
      "Training loss: 0.022168939279427445, Validation loss: 0.03837017611572716\n",
      "new best w\n",
      "iterations:  247\n",
      "Training loss: 0.022150705386195155, Validation loss: 0.03834543936090819\n",
      "new best w\n",
      "iterations:  248\n",
      "Training loss: 0.022132501266885235, Validation loss: 0.038339476714532736\n",
      "new best w\n",
      "iterations:  249\n",
      "Training loss: 0.022114058701728084, Validation loss: 0.03832669779204375\n",
      "new best w\n",
      "iterations:  250\n",
      "Training loss: 0.022096269770697438, Validation loss: 0.03828036533507463\n",
      "new best w\n",
      "iterations:  251\n",
      "Training loss: 0.022078783692489633, Validation loss: 0.03825768699118422\n",
      "new best w\n",
      "iterations:  252\n",
      "Training loss: 0.02206040724762597, Validation loss: 0.03825919699210004\n",
      "iterations:  253\n",
      "Training loss: 0.02204257479897106, Validation loss: 0.03823343215047548\n",
      "new best w\n",
      "iterations:  254\n",
      "Training loss: 0.022024155635498022, Validation loss: 0.03822243898114697\n",
      "new best w\n",
      "iterations:  255\n",
      "Training loss: 0.022006347104524534, Validation loss: 0.0381812816191351\n",
      "new best w\n",
      "iterations:  256\n",
      "Training loss: 0.021988688769708463, Validation loss: 0.038141013006947985\n",
      "new best w\n",
      "iterations:  257\n",
      "Training loss: 0.021969449072796948, Validation loss: 0.03814703208386413\n",
      "iterations:  258\n",
      "Training loss: 0.021951696381158313, Validation loss: 0.03812098589538828\n",
      "new best w\n",
      "iterations:  259\n",
      "Training loss: 0.021932647084957076, Validation loss: 0.03811268053039091\n",
      "new best w\n",
      "iterations:  260\n",
      "Training loss: 0.021915020181366797, Validation loss: 0.038068491191712246\n",
      "new best w\n",
      "iterations:  261\n",
      "Training loss: 0.021896840017938325, Validation loss: 0.038033652868438725\n",
      "new best w\n",
      "iterations:  262\n",
      "Training loss: 0.021877751999837883, Validation loss: 0.03803040530196385\n",
      "new best w\n",
      "iterations:  263\n",
      "Training loss: 0.021859888754252377, Validation loss: 0.03801116662155077\n",
      "new best w\n",
      "iterations:  264\n",
      "Training loss: 0.02184075677325073, Validation loss: 0.03799813341759588\n",
      "new best w\n",
      "iterations:  265\n",
      "Training loss: 0.02182365568127411, Validation loss: 0.03795893337822808\n",
      "new best w\n",
      "iterations:  266\n",
      "Training loss: 0.021804864886256662, Validation loss: 0.037920628516549044\n",
      "new best w\n",
      "iterations:  267\n",
      "Training loss: 0.02178656404758529, Validation loss: 0.037921798050977784\n",
      "iterations:  268\n",
      "Training loss: 0.02176825451710737, Validation loss: 0.03789881765860474\n",
      "new best w\n",
      "iterations:  269\n",
      "Training loss: 0.021749966675660933, Validation loss: 0.037888449870122645\n",
      "new best w\n",
      "iterations:  270\n",
      "Training loss: 0.021732652677685267, Validation loss: 0.037851124056693844\n",
      "new best w\n",
      "iterations:  271\n",
      "Training loss: 0.02171418002032587, Validation loss: 0.03780368712976223\n",
      "new best w\n",
      "iterations:  272\n",
      "Training loss: 0.021696311283624994, Validation loss: 0.03781297173195565\n",
      "iterations:  273\n",
      "Training loss: 0.021677544811033816, Validation loss: 0.03778443819076743\n",
      "new best w\n",
      "iterations:  274\n",
      "Training loss: 0.021659881117560894, Validation loss: 0.0377753428249927\n",
      "new best w\n",
      "iterations:  275\n",
      "Training loss: 0.021641854994387963, Validation loss: 0.03774503764970823\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  276\n",
      "Training loss: 0.021624456419371416, Validation loss: 0.03769363865793741\n",
      "new best w\n",
      "iterations:  277\n",
      "Training loss: 0.021605935510422378, Validation loss: 0.03770410109758713\n",
      "iterations:  278\n",
      "Training loss: 0.02158789734868037, Validation loss: 0.03767385996908633\n",
      "new best w\n",
      "iterations:  279\n",
      "Training loss: 0.02156946839285034, Validation loss: 0.03766590113009369\n",
      "new best w\n",
      "iterations:  280\n",
      "Training loss: 0.02155187012715747, Validation loss: 0.037633858532065224\n",
      "new best w\n",
      "iterations:  281\n",
      "Training loss: 0.02153427414250789, Validation loss: 0.03761286386348884\n",
      "new best w\n",
      "iterations:  282\n",
      "Training loss: 0.02151610541592429, Validation loss: 0.03756638269361382\n",
      "new best w\n",
      "iterations:  283\n",
      "Training loss: 0.021498423117355715, Validation loss: 0.03756329134602701\n",
      "new best w\n",
      "iterations:  284\n",
      "Training loss: 0.02147950082678406, Validation loss: 0.037550257948666725\n",
      "new best w\n",
      "iterations:  285\n",
      "Training loss: 0.02146233002927586, Validation loss: 0.03752441293382687\n",
      "new best w\n",
      "iterations:  286\n",
      "Training loss: 0.021443589392925788, Validation loss: 0.037512843129974285\n",
      "new best w\n",
      "iterations:  287\n",
      "Training loss: 0.021426827507077997, Validation loss: 0.03748199937605261\n",
      "new best w\n",
      "iterations:  288\n",
      "Training loss: 0.02140834262556239, Validation loss: 0.03745394717606692\n",
      "new best w\n",
      "iterations:  289\n",
      "Training loss: 0.021390897554589842, Validation loss: 0.03741928932260978\n",
      "new best w\n",
      "iterations:  290\n",
      "Training loss: 0.021372470736741544, Validation loss: 0.03741012080328838\n",
      "new best w\n",
      "iterations:  291\n",
      "Training loss: 0.021354541067711178, Validation loss: 0.03739721368243208\n",
      "new best w\n",
      "iterations:  292\n",
      "Training loss: 0.02133679928161453, Validation loss: 0.03737328146569051\n",
      "new best w\n",
      "iterations:  293\n",
      "Training loss: 0.021318861789459763, Validation loss: 0.03735898612431436\n",
      "new best w\n",
      "iterations:  294\n",
      "Training loss: 0.02130221659784036, Validation loss: 0.03733243839055171\n",
      "new best w\n",
      "iterations:  295\n",
      "Training loss: 0.0212844725087722, Validation loss: 0.03730251223009243\n",
      "new best w\n",
      "iterations:  296\n",
      "Training loss: 0.0212671570426076, Validation loss: 0.0372753810169414\n",
      "new best w\n",
      "iterations:  297\n",
      "Training loss: 0.02124893762629455, Validation loss: 0.03725585056652365\n",
      "new best w\n",
      "iterations:  298\n",
      "Training loss: 0.02123151405708392, Validation loss: 0.03725757264176398\n",
      "iterations:  299\n",
      "Training loss: 0.021213822659870804, Validation loss: 0.03722083140957273\n",
      "new best w\n",
      "iterations:  300\n",
      "Training loss: 0.021196422073182303, Validation loss: 0.03722131297825225\n",
      "iterations:  301\n",
      "Training loss: 0.02117861920967957, Validation loss: 0.0371866039298963\n",
      "new best w\n",
      "iterations:  302\n",
      "Training loss: 0.021161410333344413, Validation loss: 0.03715719941923311\n",
      "new best w\n",
      "iterations:  303\n",
      "Training loss: 0.021143413084836492, Validation loss: 0.037151190032680234\n",
      "new best w\n",
      "iterations:  304\n",
      "Training loss: 0.021126580225889213, Validation loss: 0.037091580956600124\n",
      "new best w\n",
      "iterations:  305\n",
      "Training loss: 0.021108067344543523, Validation loss: 0.03710306313205742\n",
      "iterations:  306\n",
      "Training loss: 0.021091333379145295, Validation loss: 0.03707494486224866\n",
      "new best w\n",
      "iterations:  307\n",
      "Training loss: 0.021072839930911292, Validation loss: 0.03707173180669458\n",
      "new best w\n",
      "iterations:  308\n",
      "Training loss: 0.02105638015908571, Validation loss: 0.037042912834947116\n",
      "new best w\n",
      "iterations:  309\n",
      "Training loss: 0.02103761864132668, Validation loss: 0.03702135956122293\n",
      "new best w\n",
      "iterations:  310\n",
      "Training loss: 0.021021245466928712, Validation loss: 0.036999305727595924\n",
      "new best w\n",
      "iterations:  311\n",
      "Training loss: 0.021002577192403726, Validation loss: 0.03698463025979028\n",
      "new best w\n",
      "iterations:  312\n",
      "Training loss: 0.02098602657383235, Validation loss: 0.03696528801535601\n",
      "new best w\n",
      "iterations:  313\n",
      "Training loss: 0.02096778218352934, Validation loss: 0.036915852295071064\n",
      "new best w\n",
      "iterations:  314\n",
      "Training loss: 0.020950789679752443, Validation loss: 0.03691672520232052\n",
      "iterations:  315\n",
      "Training loss: 0.020933076336124783, Validation loss: 0.036864777173392274\n",
      "new best w\n",
      "iterations:  316\n",
      "Training loss: 0.020915703708628153, Validation loss: 0.03687204734888708\n",
      "iterations:  317\n",
      "Training loss: 0.020898266108934332, Validation loss: 0.03684838139972948\n",
      "new best w\n",
      "iterations:  318\n",
      "Training loss: 0.020880546940823398, Validation loss: 0.03683231896348331\n",
      "new best w\n",
      "iterations:  319\n",
      "Training loss: 0.02086317554689692, Validation loss: 0.03681180177011745\n",
      "new best w\n",
      "iterations:  320\n",
      "Training loss: 0.0208454304996853, Validation loss: 0.03678538859675334\n",
      "new best w\n",
      "iterations:  321\n",
      "Training loss: 0.020827445019241414, Validation loss: 0.03678394041063543\n",
      "new best w\n",
      "iterations:  322\n",
      "Training loss: 0.020809335859151165, Validation loss: 0.036739330094806524\n",
      "new best w\n",
      "iterations:  323\n",
      "Training loss: 0.020791022508808255, Validation loss: 0.03673580088568958\n",
      "new best w\n",
      "iterations:  324\n",
      "Training loss: 0.02077322619342487, Validation loss: 0.03669552644309127\n",
      "new best w\n",
      "iterations:  325\n",
      "Training loss: 0.02075466155497079, Validation loss: 0.03669284274248512\n",
      "new best w\n",
      "iterations:  326\n",
      "Training loss: 0.02073719473751579, Validation loss: 0.036653103603466554\n",
      "new best w\n",
      "iterations:  327\n",
      "Training loss: 0.020718502779547385, Validation loss: 0.03663206638095957\n",
      "new best w\n",
      "iterations:  328\n",
      "Training loss: 0.020701275919096273, Validation loss: 0.03660415159626722\n",
      "new best w\n",
      "iterations:  329\n",
      "Training loss: 0.020682230312418396, Validation loss: 0.036598838796143916\n",
      "new best w\n",
      "iterations:  330\n",
      "Training loss: 0.020665369974296643, Validation loss: 0.03654502316814458\n",
      "new best w\n",
      "iterations:  331\n",
      "Training loss: 0.020646159636959163, Validation loss: 0.036548534130261615\n",
      "iterations:  332\n",
      "Training loss: 0.020629378351343434, Validation loss: 0.03652253264632912\n",
      "new best w\n",
      "iterations:  333\n",
      "Training loss: 0.020610347436690324, Validation loss: 0.03651351682090873\n",
      "new best w\n",
      "iterations:  334\n",
      "Training loss: 0.020593831812919943, Validation loss: 0.036500030153313816\n",
      "new best w\n",
      "iterations:  335\n",
      "Training loss: 0.02057480157037756, Validation loss: 0.03646845392368332\n",
      "new best w\n",
      "iterations:  336\n",
      "Training loss: 0.020558045166820117, Validation loss: 0.03645276827749429\n",
      "new best w\n",
      "iterations:  337\n",
      "Training loss: 0.020539208320114674, Validation loss: 0.03642251247902396\n",
      "new best w\n",
      "iterations:  338\n",
      "Training loss: 0.020522339433213653, Validation loss: 0.036410723371671165\n",
      "new best w\n",
      "iterations:  339\n",
      "Training loss: 0.020503652407308903, Validation loss: 0.036380518849188415\n",
      "new best w\n",
      "iterations:  340\n",
      "Training loss: 0.020486663836382743, Validation loss: 0.03637483177442695\n",
      "new best w\n",
      "iterations:  341\n",
      "Training loss: 0.020468201278353873, Validation loss: 0.03633451783003021\n",
      "new best w\n",
      "iterations:  342\n",
      "Training loss: 0.020451091406823562, Validation loss: 0.0363327729069719\n",
      "new best w\n",
      "iterations:  343\n",
      "Training loss: 0.020433029195035025, Validation loss: 0.03629071215336206\n",
      "new best w\n",
      "iterations:  344\n",
      "Training loss: 0.02041565073897189, Validation loss: 0.03628948872316813\n",
      "new best w\n",
      "iterations:  345\n",
      "Training loss: 0.02039770301452079, Validation loss: 0.0362525471257278\n",
      "new best w\n",
      "iterations:  346\n",
      "Training loss: 0.020380103900548917, Validation loss: 0.03623902633208602\n",
      "new best w\n",
      "iterations:  347\n",
      "Training loss: 0.020362398871172006, Validation loss: 0.0362093517830505\n",
      "new best w\n",
      "iterations:  348\n",
      "Training loss: 0.020344663090848782, Validation loss: 0.03619754644570083\n",
      "new best w\n",
      "iterations:  349\n",
      "Training loss: 0.020327196002767542, Validation loss: 0.036181239075631984\n",
      "new best w\n",
      "iterations:  350\n",
      "Training loss: 0.020309853913991253, Validation loss: 0.03616654457909142\n",
      "new best w\n",
      "iterations:  351\n",
      "Training loss: 0.020292763158375235, Validation loss: 0.03614446498015442\n",
      "new best w\n",
      "iterations:  352\n",
      "Training loss: 0.020275299346360548, Validation loss: 0.03610852065149431\n",
      "new best w\n",
      "iterations:  353\n",
      "Training loss: 0.020258409418083986, Validation loss: 0.036102719167028716\n",
      "new best w\n",
      "iterations:  354\n",
      "Training loss: 0.020241124051855897, Validation loss: 0.03606800302012367\n",
      "new best w\n",
      "iterations:  355\n",
      "Training loss: 0.020224159085035175, Validation loss: 0.036061767448743724\n",
      "new best w\n",
      "iterations:  356\n",
      "Training loss: 0.020206976655168706, Validation loss: 0.03602734597196107\n",
      "new best w\n",
      "iterations:  357\n",
      "Training loss: 0.020189872912431, Validation loss: 0.036026918187622624\n",
      "new best w\n",
      "iterations:  358\n",
      "Training loss: 0.0201728415713231, Validation loss: 0.03598298631226453\n",
      "new best w\n",
      "iterations:  359\n",
      "Training loss: 0.020155586852790855, Validation loss: 0.03598531306600356\n",
      "iterations:  360\n",
      "Training loss: 0.02013863626574973, Validation loss: 0.035950724100741856\n",
      "new best w\n",
      "iterations:  361\n",
      "Training loss: 0.02012135966910943, Validation loss: 0.035938952864965984\n",
      "new best w\n",
      "iterations:  362\n",
      "Training loss: 0.020104291892832235, Validation loss: 0.03591638907847161\n",
      "new best w\n",
      "iterations:  363\n",
      "Training loss: 0.02008698548214444, Validation loss: 0.03589100992378977\n",
      "new best w\n",
      "iterations:  364\n",
      "Training loss: 0.020069760381171364, Validation loss: 0.03587352992703653\n",
      "new best w\n",
      "iterations:  365\n",
      "Training loss: 0.020052646850824883, Validation loss: 0.03585549191479186\n",
      "new best w\n",
      "iterations:  366\n",
      "Training loss: 0.020035823369537745, Validation loss: 0.03584149715127491\n",
      "new best w\n",
      "iterations:  367\n",
      "Training loss: 0.02001922656874321, Validation loss: 0.035802722764695726\n",
      "new best w\n",
      "iterations:  368\n",
      "Training loss: 0.02000233906176092, Validation loss: 0.0357982415432292\n",
      "new best w\n",
      "iterations:  369\n",
      "Training loss: 0.019986283791477953, Validation loss: 0.03576310645567146\n",
      "new best w\n",
      "iterations:  370\n",
      "Training loss: 0.019969809653872895, Validation loss: 0.03575776389235782\n",
      "new best w\n",
      "iterations:  371\n",
      "Training loss: 0.0199537321983933, Validation loss: 0.035722915973831953\n",
      "new best w\n",
      "iterations:  372\n",
      "Training loss: 0.01993730044797697, Validation loss: 0.03571717571419562\n",
      "new best w\n",
      "iterations:  373\n",
      "Training loss: 0.019921406254495302, Validation loss: 0.03568214017506453\n",
      "new best w\n",
      "iterations:  374\n",
      "Training loss: 0.019905043897667552, Validation loss: 0.035682471643591576\n",
      "iterations:  375\n",
      "Training loss: 0.019889221954784646, Validation loss: 0.035637882564153664\n",
      "new best w\n",
      "iterations:  376\n",
      "Training loss: 0.01987278895040444, Validation loss: 0.03564069730270343\n",
      "iterations:  377\n",
      "Training loss: 0.01985711264959813, Validation loss: 0.03561229959108283\n",
      "new best w\n",
      "iterations:  378\n",
      "Training loss: 0.01984063840875932, Validation loss: 0.0356016239728581\n",
      "new best w\n",
      "iterations:  379\n",
      "Training loss: 0.01982468952063437, Validation loss: 0.035576665599452856\n",
      "new best w\n",
      "iterations:  380\n",
      "Training loss: 0.019808416847122125, Validation loss: 0.03554662058584322\n",
      "new best w\n",
      "iterations:  381\n",
      "Training loss: 0.01979234534099328, Validation loss: 0.03552971829877612\n",
      "new best w\n",
      "iterations:  382\n",
      "Training loss: 0.01977620076170901, Validation loss: 0.03550532402293668\n",
      "new best w\n",
      "iterations:  383\n",
      "Training loss: 0.01976015608999433, Validation loss: 0.035506464231693666\n",
      "iterations:  384\n",
      "Training loss: 0.01974423897796423, Validation loss: 0.035461982555286396\n",
      "new best w\n",
      "iterations:  385\n",
      "Training loss: 0.01972813849610614, Validation loss: 0.03545787105118334\n",
      "new best w\n",
      "iterations:  386\n",
      "Training loss: 0.019712310299289448, Validation loss: 0.03542077582862263\n",
      "new best w\n",
      "iterations:  387\n",
      "Training loss: 0.019696385120066728, Validation loss: 0.03541532289536935\n",
      "new best w\n",
      "iterations:  388\n",
      "Training loss: 0.019681232004265057, Validation loss: 0.03537956678951135\n",
      "new best w\n",
      "iterations:  389\n",
      "Training loss: 0.01966593784726385, Validation loss: 0.03537473890671379\n",
      "new best w\n",
      "iterations:  390\n",
      "Training loss: 0.019650753972717375, Validation loss: 0.0353400461754039\n",
      "new best w\n",
      "iterations:  391\n",
      "Training loss: 0.01963547576466524, Validation loss: 0.03533502187083977\n",
      "new best w\n",
      "iterations:  392\n",
      "Training loss: 0.019620286377330785, Validation loss: 0.03530022070232791\n",
      "new best w\n",
      "iterations:  393\n",
      "Training loss: 0.01960504115411955, Validation loss: 0.03530144752689264\n",
      "iterations:  394\n",
      "Training loss: 0.019589870372784254, Validation loss: 0.03525689948145881\n",
      "new best w\n",
      "iterations:  395\n",
      "Training loss: 0.019574564132760205, Validation loss: 0.035260753976489104\n",
      "iterations:  396\n",
      "Training loss: 0.019559422440699713, Validation loss: 0.03521714694819066\n",
      "new best w\n",
      "iterations:  397\n",
      "Training loss: 0.019544096433804437, Validation loss: 0.03522109919205922\n",
      "iterations:  398\n",
      "Training loss: 0.019528992130223827, Validation loss: 0.0351774376980846\n",
      "new best w\n",
      "iterations:  399\n",
      "Training loss: 0.019513621161804905, Validation loss: 0.035181541850656146\n",
      "iterations:  400\n",
      "Training loss: 0.019498482666052877, Validation loss: 0.035155070689293924\n",
      "new best w\n",
      "iterations:  401\n",
      "Training loss: 0.019483327174701026, Validation loss: 0.03512896527859242\n",
      "new best w\n",
      "iterations:  402\n",
      "Training loss: 0.01946834929621189, Validation loss: 0.03511244943940311\n",
      "new best w\n",
      "iterations:  403\n",
      "Training loss: 0.019453258720556355, Validation loss: 0.03508874696881188\n",
      "new best w\n",
      "iterations:  404\n",
      "Training loss: 0.019438359345952082, Validation loss: 0.03506937024736375\n",
      "new best w\n",
      "iterations:  405\n",
      "Training loss: 0.019423178099624402, Validation loss: 0.03504953550118701\n",
      "new best w\n",
      "iterations:  406\n",
      "Training loss: 0.019408454243295615, Validation loss: 0.03504803007041092\n",
      "new best w\n",
      "iterations:  407\n",
      "Training loss: 0.019393285999580564, Validation loss: 0.03502061020103292\n",
      "new best w\n",
      "iterations:  408\n",
      "Training loss: 0.019378448679346683, Validation loss: 0.035001523646941604\n",
      "new best w\n",
      "iterations:  409\n",
      "Training loss: 0.019363137824465856, Validation loss: 0.03496504432285682\n",
      "new best w\n",
      "iterations:  410\n",
      "Training loss: 0.01934842405011858, Validation loss: 0.034959994296009544\n",
      "new best w\n",
      "iterations:  411\n",
      "Training loss: 0.01933306041948869, Validation loss: 0.03492489350133938\n",
      "new best w\n",
      "iterations:  412\n",
      "Training loss: 0.019318431481987614, Validation loss: 0.03492015475671643\n",
      "new best w\n",
      "iterations:  413\n",
      "Training loss: 0.01930297859604145, Validation loss: 0.034885073675275304\n",
      "new best w\n",
      "iterations:  414\n",
      "Training loss: 0.01928843628503463, Validation loss: 0.03487989122642421\n",
      "new best w\n",
      "iterations:  415\n",
      "Training loss: 0.01927239275134151, Validation loss: 0.03483892276266196\n",
      "new best w\n",
      "iterations:  416\n",
      "Training loss: 0.01925752998147078, Validation loss: 0.03483187369892117\n",
      "new best w\n",
      "iterations:  417\n",
      "Training loss: 0.01924140426066528, Validation loss: 0.034795856166111985\n",
      "new best w\n",
      "iterations:  418\n",
      "Training loss: 0.01922663180165407, Validation loss: 0.03478940749585059\n",
      "new best w\n",
      "iterations:  419\n",
      "Training loss: 0.019210541473614978, Validation loss: 0.0347489380104177\n",
      "new best w\n",
      "iterations:  420\n",
      "Training loss: 0.019195778586641395, Validation loss: 0.034752962770430315\n",
      "iterations:  421\n",
      "Training loss: 0.01917975685572553, Validation loss: 0.0347021258954211\n",
      "new best w\n",
      "iterations:  422\n",
      "Training loss: 0.019164907897866592, Validation loss: 0.03470913677437108\n",
      "iterations:  423\n",
      "Training loss: 0.019149003243945154, Validation loss: 0.034658812631627\n",
      "new best w\n",
      "iterations:  424\n",
      "Training loss: 0.019134026732630455, Validation loss: 0.03466617668013587\n",
      "iterations:  425\n",
      "Training loss: 0.01911833063621824, Validation loss: 0.03461181517406758\n",
      "new best w\n",
      "iterations:  426\n",
      "Training loss: 0.01910313668109726, Validation loss: 0.03462262506631788\n",
      "iterations:  427\n",
      "Training loss: 0.01908766306484434, Validation loss: 0.03456717660256051\n",
      "new best w\n",
      "iterations:  428\n",
      "Training loss: 0.01907227284941575, Validation loss: 0.03457809343874748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  429\n",
      "Training loss: 0.01905706637982051, Validation loss: 0.034520578741219936\n",
      "new best w\n",
      "iterations:  430\n",
      "Training loss: 0.019041385411304335, Validation loss: 0.03453356110381556\n",
      "iterations:  431\n",
      "Training loss: 0.019026454886215346, Validation loss: 0.034475847274024696\n",
      "new best w\n",
      "iterations:  432\n",
      "Training loss: 0.019010516039860436, Validation loss: 0.034488902155161545\n",
      "iterations:  433\n",
      "Training loss: 0.01899586136144889, Validation loss: 0.03443108969187695\n",
      "new best w\n",
      "iterations:  434\n",
      "Training loss: 0.018979665791888492, Validation loss: 0.0344442277734088\n",
      "iterations:  435\n",
      "Training loss: 0.01896529127208663, Validation loss: 0.034385332707570646\n",
      "new best w\n",
      "iterations:  436\n",
      "Training loss: 0.018948847523283213, Validation loss: 0.034396722942143534\n",
      "iterations:  437\n",
      "Training loss: 0.018934700595562598, Validation loss: 0.03434555591369377\n",
      "new best w\n",
      "iterations:  438\n",
      "Training loss: 0.01891760367558802, Validation loss: 0.03434189567859101\n",
      "new best w\n",
      "iterations:  439\n",
      "Training loss: 0.01889906630585189, Validation loss: 0.03427392694876362\n",
      "new best w\n",
      "iterations:  440\n",
      "Training loss: 0.018873247448333116, Validation loss: 0.03424190511632817\n",
      "new best w\n",
      "iterations:  441\n",
      "Training loss: 0.018850692697541506, Validation loss: 0.034180565673249556\n",
      "new best w\n",
      "iterations:  442\n",
      "Training loss: 0.018825033338581373, Validation loss: 0.034129546609242035\n",
      "new best w\n",
      "iterations:  443\n",
      "Training loss: 0.018802478877680134, Validation loss: 0.0340740647266437\n",
      "new best w\n",
      "iterations:  444\n",
      "Training loss: 0.01877704212061019, Validation loss: 0.03402428052702884\n",
      "new best w\n",
      "iterations:  445\n",
      "Training loss: 0.01875440709592841, Validation loss: 0.03397366142284996\n",
      "new best w\n",
      "iterations:  446\n",
      "Training loss: 0.018729202187681492, Validation loss: 0.03392952418132447\n",
      "new best w\n",
      "iterations:  447\n",
      "Training loss: 0.018706449124939538, Validation loss: 0.033879796858334435\n",
      "new best w\n",
      "iterations:  448\n",
      "Training loss: 0.018681501653968688, Validation loss: 0.033835647565241156\n",
      "new best w\n",
      "iterations:  449\n",
      "Training loss: 0.018658601421933943, Validation loss: 0.03378584845260209\n",
      "new best w\n",
      "iterations:  450\n",
      "Training loss: 0.018634005800893864, Validation loss: 0.033743539162608464\n",
      "new best w\n",
      "iterations:  451\n",
      "Training loss: 0.018610811348240785, Validation loss: 0.03370961947114978\n",
      "new best w\n",
      "iterations:  452\n",
      "Training loss: 0.018586675212485623, Validation loss: 0.033643218221808775\n",
      "new best w\n",
      "iterations:  453\n",
      "Training loss: 0.018563040780227944, Validation loss: 0.033609630711835405\n",
      "new best w\n",
      "iterations:  454\n",
      "Training loss: 0.018539422060216124, Validation loss: 0.03354795352465617\n",
      "new best w\n",
      "iterations:  455\n",
      "Training loss: 0.018515388815350274, Validation loss: 0.033516127166117106\n",
      "new best w\n",
      "iterations:  456\n",
      "Training loss: 0.0184922253336929, Validation loss: 0.033452967903657205\n",
      "new best w\n",
      "iterations:  457\n",
      "Training loss: 0.01846781240804389, Validation loss: 0.03342356846687712\n",
      "new best w\n",
      "iterations:  458\n",
      "Training loss: 0.018445131862320177, Validation loss: 0.03335853223206722\n",
      "new best w\n",
      "iterations:  459\n",
      "Training loss: 0.018420447035108655, Validation loss: 0.03334317695942011\n",
      "new best w\n",
      "iterations:  460\n",
      "Training loss: 0.018398045064198065, Validation loss: 0.03328615228795493\n",
      "new best w\n",
      "iterations:  461\n",
      "Training loss: 0.018373177148511572, Validation loss: 0.03328910868583902\n",
      "iterations:  462\n",
      "Training loss: 0.018350971801475234, Validation loss: 0.03322820936054983\n",
      "new best w\n",
      "iterations:  463\n",
      "Training loss: 0.018332276462925776, Validation loss: 0.03323533809726586\n",
      "iterations:  464\n",
      "Training loss: 0.018317689247807526, Validation loss: 0.033203027914878426\n",
      "new best w\n",
      "iterations:  465\n",
      "Training loss: 0.01830137483269876, Validation loss: 0.03317841886931804\n",
      "new best w\n",
      "iterations:  466\n",
      "Training loss: 0.018286543957792778, Validation loss: 0.03316960266434311\n",
      "new best w\n",
      "iterations:  467\n",
      "Training loss: 0.01827062872141473, Validation loss: 0.0331241523522154\n",
      "new best w\n",
      "iterations:  468\n",
      "Training loss: 0.018255215982504847, Validation loss: 0.033112902250596585\n",
      "new best w\n",
      "iterations:  469\n",
      "Training loss: 0.018239805451317186, Validation loss: 0.03307339866320296\n",
      "new best w\n",
      "iterations:  470\n",
      "Training loss: 0.01822393907428876, Validation loss: 0.03306422572410117\n",
      "new best w\n",
      "iterations:  471\n",
      "Training loss: 0.01820900879383299, Validation loss: 0.03302345412340213\n",
      "new best w\n",
      "iterations:  472\n",
      "Training loss: 0.01819267487578278, Validation loss: 0.03302132792050929\n",
      "new best w\n",
      "iterations:  473\n",
      "Training loss: 0.018178284359806226, Validation loss: 0.032970297742958385\n",
      "new best w\n",
      "iterations:  474\n",
      "Training loss: 0.018161385393163327, Validation loss: 0.03297154325346237\n",
      "iterations:  475\n",
      "Training loss: 0.018147430305617232, Validation loss: 0.03293073660895576\n",
      "new best w\n",
      "iterations:  476\n",
      "Training loss: 0.018130150025089678, Validation loss: 0.03291835929056768\n",
      "new best w\n",
      "iterations:  477\n",
      "Training loss: 0.018116118742231326, Validation loss: 0.03289302998646261\n",
      "new best w\n",
      "iterations:  478\n",
      "Training loss: 0.01809894041361756, Validation loss: 0.03286112240433559\n",
      "new best w\n",
      "iterations:  479\n",
      "Training loss: 0.018084807951060655, Validation loss: 0.03286132103662083\n",
      "iterations:  480\n",
      "Training loss: 0.01806784475031039, Validation loss: 0.032807689019867484\n",
      "new best w\n",
      "iterations:  481\n",
      "Training loss: 0.018053352575055234, Validation loss: 0.03280627608885447\n",
      "new best w\n",
      "iterations:  482\n",
      "Training loss: 0.018036699934571864, Validation loss: 0.032756834732946515\n",
      "new best w\n",
      "iterations:  483\n",
      "Training loss: 0.018021937942878617, Validation loss: 0.032757048175865014\n",
      "iterations:  484\n",
      "Training loss: 0.018005578446392983, Validation loss: 0.03270803017754774\n",
      "new best w\n",
      "iterations:  485\n",
      "Training loss: 0.01799053961283942, Validation loss: 0.032714922591984864\n",
      "iterations:  486\n",
      "Training loss: 0.017974558845698107, Validation loss: 0.03265532599476871\n",
      "new best w\n",
      "iterations:  487\n",
      "Training loss: 0.017959031951038797, Validation loss: 0.03266531328369263\n",
      "iterations:  488\n",
      "Training loss: 0.01794346166991387, Validation loss: 0.03262044015867553\n",
      "new best w\n",
      "iterations:  489\n",
      "Training loss: 0.017927729713684643, Validation loss: 0.032610270905443085\n",
      "new best w\n",
      "iterations:  490\n",
      "Training loss: 0.01791220951196229, Validation loss: 0.032583445264834754\n",
      "new best w\n",
      "iterations:  491\n",
      "Training loss: 0.017896521171242546, Validation loss: 0.03255447786341994\n",
      "new best w\n",
      "iterations:  492\n",
      "Training loss: 0.017880969557071805, Validation loss: 0.032556920121257264\n",
      "iterations:  493\n",
      "Training loss: 0.01786551557569015, Validation loss: 0.03250859570820848\n",
      "new best w\n",
      "iterations:  494\n",
      "Training loss: 0.01784956615247784, Validation loss: 0.03250818981444218\n",
      "new best w\n",
      "iterations:  495\n",
      "Training loss: 0.017834420810445742, Validation loss: 0.03246594895421071\n",
      "new best w\n",
      "iterations:  496\n",
      "Training loss: 0.017818214851129267, Validation loss: 0.0324677516117123\n",
      "iterations:  497\n",
      "Training loss: 0.017803364876276935, Validation loss: 0.03242398786950686\n",
      "new best w\n",
      "iterations:  498\n",
      "Training loss: 0.017787881248678555, Validation loss: 0.032426781994994894\n",
      "iterations:  499\n",
      "Training loss: 0.017773346836083496, Validation loss: 0.03237887209992192\n",
      "new best w\n",
      "iterations:  500\n",
      "Training loss: 0.01775749454765101, Validation loss: 0.032388674330521096\n",
      "iterations:  501\n",
      "Training loss: 0.017743312858832624, Validation loss: 0.032330041583597556\n",
      "new best w\n",
      "iterations:  502\n",
      "Training loss: 0.01772698394784784, Validation loss: 0.03234329355044792\n",
      "iterations:  503\n",
      "Training loss: 0.017713158048951034, Validation loss: 0.032295116875223684\n",
      "new best w\n",
      "iterations:  504\n",
      "Training loss: 0.01769661722807536, Validation loss: 0.032292321038534515\n",
      "new best w\n",
      "iterations:  505\n",
      "Training loss: 0.0176828907758772, Validation loss: 0.03226236288609531\n",
      "new best w\n",
      "iterations:  506\n",
      "Training loss: 0.017666314491184235, Validation loss: 0.032239462703754364\n",
      "new best w\n",
      "iterations:  507\n",
      "Training loss: 0.01765264237211415, Validation loss: 0.03221655640273717\n",
      "new best w\n",
      "iterations:  508\n",
      "Training loss: 0.017636164742194288, Validation loss: 0.03219494969297087\n",
      "new best w\n",
      "iterations:  509\n",
      "Training loss: 0.017622306563322282, Validation loss: 0.03219188027752272\n",
      "new best w\n",
      "iterations:  510\n",
      "Training loss: 0.01760609623779107, Validation loss: 0.032142584275441255\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  511\n",
      "Training loss: 0.017591923347405215, Validation loss: 0.03214074701026691\n",
      "new best w\n",
      "iterations:  512\n",
      "Training loss: 0.017575982483062316, Validation loss: 0.03209736247605133\n",
      "new best w\n",
      "iterations:  513\n",
      "Training loss: 0.01756160498624918, Validation loss: 0.03209786393700645\n",
      "iterations:  514\n",
      "Training loss: 0.01754590263244478, Validation loss: 0.03205277787866777\n",
      "new best w\n",
      "iterations:  515\n",
      "Training loss: 0.017531291759128858, Validation loss: 0.03205415573089741\n",
      "iterations:  516\n",
      "Training loss: 0.017515862290400565, Validation loss: 0.03200721497030394\n",
      "new best w\n",
      "iterations:  517\n",
      "Training loss: 0.017500979924404792, Validation loss: 0.032010740470545376\n",
      "iterations:  518\n",
      "Training loss: 0.01748587645831214, Validation loss: 0.0319639643035424\n",
      "new best w\n",
      "iterations:  519\n",
      "Training loss: 0.017470791657098382, Validation loss: 0.03197436559465652\n",
      "iterations:  520\n",
      "Training loss: 0.01745600484471482, Validation loss: 0.03191639075624839\n",
      "new best w\n",
      "iterations:  521\n",
      "Training loss: 0.017440561644276445, Validation loss: 0.0319304537286222\n",
      "iterations:  522\n",
      "Training loss: 0.01742611854788727, Validation loss: 0.03187307033896267\n",
      "new best w\n",
      "iterations:  523\n",
      "Training loss: 0.017410356478913785, Validation loss: 0.03188733617385219\n",
      "iterations:  524\n",
      "Training loss: 0.01739625516150048, Validation loss: 0.031829400235827844\n",
      "new best w\n",
      "iterations:  525\n",
      "Training loss: 0.01738016775092359, Validation loss: 0.031844372908428994\n",
      "iterations:  526\n",
      "Training loss: 0.01736633426451393, Validation loss: 0.031796419027248224\n",
      "new best w\n",
      "iterations:  527\n",
      "Training loss: 0.017350058570595826, Validation loss: 0.03179480871208698\n",
      "new best w\n",
      "iterations:  528\n",
      "Training loss: 0.017336394759837597, Validation loss: 0.03176591375621944\n",
      "new best w\n",
      "iterations:  529\n",
      "Training loss: 0.01731999113201907, Validation loss: 0.031739564992786705\n",
      "new best w\n",
      "iterations:  530\n",
      "Training loss: 0.01730651505817745, Validation loss: 0.03171787138149797\n",
      "new best w\n",
      "iterations:  531\n",
      "Training loss: 0.01729001225147159, Validation loss: 0.03169688369909893\n",
      "new best w\n",
      "iterations:  532\n",
      "Training loss: 0.017276481121914503, Validation loss: 0.031675553649049275\n",
      "new best w\n",
      "iterations:  533\n",
      "Training loss: 0.017260080037575714, Validation loss: 0.03165006634440406\n",
      "new best w\n",
      "iterations:  534\n",
      "Training loss: 0.01724648363794512, Validation loss: 0.03163505616310408\n",
      "new best w\n",
      "iterations:  535\n",
      "Training loss: 0.01723028643037925, Validation loss: 0.031605719323573415\n",
      "new best w\n",
      "iterations:  536\n",
      "Training loss: 0.017216514664196755, Validation loss: 0.03160311057000916\n",
      "new best w\n",
      "iterations:  537\n",
      "Training loss: 0.017200433029205774, Validation loss: 0.0315640288322329\n",
      "new best w\n",
      "iterations:  538\n",
      "Training loss: 0.017186439419609966, Validation loss: 0.03156850693959832\n",
      "iterations:  539\n",
      "Training loss: 0.017170747261294642, Validation loss: 0.03151589077664087\n",
      "new best w\n",
      "iterations:  540\n",
      "Training loss: 0.017156349434091788, Validation loss: 0.03152563003784947\n",
      "iterations:  541\n",
      "Training loss: 0.0171410314607815, Validation loss: 0.031465151507533734\n",
      "new best w\n",
      "iterations:  542\n",
      "Training loss: 0.017126360316216404, Validation loss: 0.031482043007294076\n",
      "iterations:  543\n",
      "Training loss: 0.01711124178585555, Validation loss: 0.03142533406536744\n",
      "new best w\n",
      "iterations:  544\n",
      "Training loss: 0.01709637134063232, Validation loss: 0.03143431973965908\n",
      "iterations:  545\n",
      "Training loss: 0.017081494277523587, Validation loss: 0.03138541412029607\n",
      "new best w\n",
      "iterations:  546\n",
      "Training loss: 0.01706672870820362, Validation loss: 0.03139219273961685\n",
      "iterations:  547\n",
      "Training loss: 0.017051881045115277, Validation loss: 0.031345047328952734\n",
      "new best w\n",
      "iterations:  548\n",
      "Training loss: 0.017037212725205544, Validation loss: 0.031340939738418575\n",
      "new best w\n",
      "iterations:  549\n",
      "Training loss: 0.0170222429156184, Validation loss: 0.03131524603544241\n",
      "new best w\n",
      "iterations:  550\n",
      "Training loss: 0.01700771555187487, Validation loss: 0.03130166383447591\n",
      "new best w\n",
      "iterations:  551\n",
      "Training loss: 0.016992587890667963, Validation loss: 0.03126151665499908\n",
      "new best w\n",
      "iterations:  552\n",
      "Training loss: 0.01697829403755371, Validation loss: 0.03126589245307339\n",
      "iterations:  553\n",
      "Training loss: 0.016962929308639164, Validation loss: 0.031220893948241946\n",
      "new best w\n",
      "iterations:  554\n",
      "Training loss: 0.01694886227747815, Validation loss: 0.03121602130754763\n",
      "new best w\n",
      "iterations:  555\n",
      "Training loss: 0.016933474988993294, Validation loss: 0.031191232935939645\n",
      "new best w\n",
      "iterations:  556\n",
      "Training loss: 0.016919497604652244, Validation loss: 0.03117279418734618\n",
      "new best w\n",
      "iterations:  557\n",
      "Training loss: 0.01690385686663901, Validation loss: 0.031133797527055602\n",
      "new best w\n",
      "iterations:  558\n",
      "Training loss: 0.01688997857191507, Validation loss: 0.03113788082946996\n",
      "iterations:  559\n",
      "Training loss: 0.01687442366044023, Validation loss: 0.031088790357162804\n",
      "new best w\n",
      "iterations:  560\n",
      "Training loss: 0.01686038029296678, Validation loss: 0.031089459361220574\n",
      "iterations:  561\n",
      "Training loss: 0.016845062000088156, Validation loss: 0.03105649749276279\n",
      "new best w\n",
      "iterations:  562\n",
      "Training loss: 0.016830758682775445, Validation loss: 0.0310502953000781\n",
      "new best w\n",
      "iterations:  563\n",
      "Training loss: 0.016815735008774108, Validation loss: 0.031003085651876355\n",
      "new best w\n",
      "iterations:  564\n",
      "Training loss: 0.01680119631913055, Validation loss: 0.031014498322299085\n",
      "iterations:  565\n",
      "Training loss: 0.016786456907265393, Validation loss: 0.03096179183415689\n",
      "new best w\n",
      "iterations:  566\n",
      "Training loss: 0.016771867547863598, Validation loss: 0.03098003830289975\n",
      "iterations:  567\n",
      "Training loss: 0.016757256738706414, Validation loss: 0.030917829670814502\n",
      "new best w\n",
      "iterations:  568\n",
      "Training loss: 0.016742376503400703, Validation loss: 0.030934229921728108\n",
      "iterations:  569\n",
      "Training loss: 0.016727653791112754, Validation loss: 0.03088331830977615\n",
      "new best w\n",
      "iterations:  570\n",
      "Training loss: 0.016713019943663858, Validation loss: 0.030876360984660157\n",
      "new best w\n",
      "iterations:  571\n",
      "Training loss: 0.01669810817378402, Validation loss: 0.0308480119976135\n",
      "new best w\n",
      "iterations:  572\n",
      "Training loss: 0.016683757387317165, Validation loss: 0.03083411074410532\n",
      "new best w\n",
      "iterations:  573\n",
      "Training loss: 0.01666855565081805, Validation loss: 0.030793745231609555\n",
      "new best w\n",
      "iterations:  574\n",
      "Training loss: 0.016654578015930354, Validation loss: 0.030797279209031508\n",
      "iterations:  575\n",
      "Training loss: 0.016639208039411362, Validation loss: 0.03074734592215611\n",
      "new best w\n",
      "iterations:  576\n",
      "Training loss: 0.016625564409321687, Validation loss: 0.03076246410837471\n",
      "iterations:  577\n",
      "Training loss: 0.016609687075124144, Validation loss: 0.030701122302194496\n",
      "new best w\n",
      "iterations:  578\n",
      "Training loss: 0.016596159917430093, Validation loss: 0.03071860226379815\n",
      "iterations:  579\n",
      "Training loss: 0.016580403685610402, Validation loss: 0.03065472899812032\n",
      "new best w\n",
      "iterations:  580\n",
      "Training loss: 0.016566801518645206, Validation loss: 0.03068400190795259\n",
      "iterations:  581\n",
      "Training loss: 0.01655116156330794, Validation loss: 0.030612477698166607\n",
      "new best w\n",
      "iterations:  582\n",
      "Training loss: 0.016537299375570937, Validation loss: 0.03063747965718\n",
      "iterations:  583\n",
      "Training loss: 0.016521946839291243, Validation loss: 0.030569614564519686\n",
      "new best w\n",
      "iterations:  584\n",
      "Training loss: 0.016507992270208594, Validation loss: 0.030581021131740264\n",
      "iterations:  585\n",
      "Training loss: 0.016492832747130693, Validation loss: 0.030540071271368003\n",
      "new best w\n",
      "iterations:  586\n",
      "Training loss: 0.016478529677550917, Validation loss: 0.030526257596959185\n",
      "new best w\n",
      "iterations:  587\n",
      "Training loss: 0.016463405963995065, Validation loss: 0.03050420363675358\n",
      "new best w\n",
      "iterations:  588\n",
      "Training loss: 0.0164493815068899, Validation loss: 0.030480973791724864\n",
      "new best w\n",
      "iterations:  589\n",
      "Training loss: 0.016433964988402664, Validation loss: 0.03046838369848167\n",
      "new best w\n",
      "iterations:  590\n",
      "Training loss: 0.016420294308230707, Validation loss: 0.03043984149449777\n",
      "new best w\n",
      "iterations:  591\n",
      "Training loss: 0.016404481612209813, Validation loss: 0.03041617725298349\n",
      "new best w\n",
      "iterations:  592\n",
      "Training loss: 0.01639120868864134, Validation loss: 0.03038855909437353\n",
      "new best w\n",
      "iterations:  593\n",
      "Training loss: 0.016375362806742188, Validation loss: 0.030381023862462508\n",
      "new best w\n",
      "iterations:  594\n",
      "Training loss: 0.016361986559386584, Validation loss: 0.03034054774784603\n",
      "new best w\n",
      "iterations:  595\n",
      "Training loss: 0.016346332761569837, Validation loss: 0.03034449818206366\n",
      "iterations:  596\n",
      "Training loss: 0.016332515108606723, Validation loss: 0.030295897473824877\n",
      "new best w\n",
      "iterations:  597\n",
      "Training loss: 0.01631736508188686, Validation loss: 0.030295108937371383\n",
      "new best w\n",
      "iterations:  598\n",
      "Training loss: 0.01630310590762186, Validation loss: 0.030275373712703633\n",
      "new best w\n",
      "iterations:  599\n",
      "Training loss: 0.016288532431248972, Validation loss: 0.030255151870443437\n",
      "new best w\n",
      "iterations:  600\n",
      "Training loss: 0.016273885092598087, Validation loss: 0.030220351592621104\n",
      "new best w\n",
      "iterations:  601\n",
      "Training loss: 0.01625976364953653, Validation loss: 0.030208503834442246\n",
      "new best w\n",
      "iterations:  602\n",
      "Training loss: 0.016244642892249875, Validation loss: 0.030180450670146304\n",
      "new best w\n",
      "iterations:  603\n",
      "Training loss: 0.016230575461700935, Validation loss: 0.03016988891746178\n",
      "new best w\n",
      "iterations:  604\n",
      "Training loss: 0.016215690871191374, Validation loss: 0.03012646667508882\n",
      "new best w\n",
      "iterations:  605\n",
      "Training loss: 0.016201472421446924, Validation loss: 0.030139674392872323\n",
      "iterations:  606\n",
      "Training loss: 0.01618671615934439, Validation loss: 0.03008382377047718\n",
      "new best w\n",
      "iterations:  607\n",
      "Training loss: 0.016172287309833836, Validation loss: 0.0300937840175938\n",
      "iterations:  608\n",
      "Training loss: 0.016157879225891344, Validation loss: 0.0300333801678124\n",
      "new best w\n",
      "iterations:  609\n",
      "Training loss: 0.016143525436038524, Validation loss: 0.03005906462535351\n",
      "iterations:  610\n",
      "Training loss: 0.01612870794792882, Validation loss: 0.02998780870768266\n",
      "new best w\n",
      "iterations:  611\n",
      "Training loss: 0.01611470142685953, Validation loss: 0.030011186530335306\n",
      "iterations:  612\n",
      "Training loss: 0.016099433824168872, Validation loss: 0.029948254820405786\n",
      "new best w\n",
      "iterations:  613\n",
      "Training loss: 0.016085872747627992, Validation loss: 0.029951499780114278\n",
      "iterations:  614\n",
      "Training loss: 0.016070194010104597, Validation loss: 0.029923096703419687\n",
      "new best w\n",
      "iterations:  615\n",
      "Training loss: 0.016057136936051854, Validation loss: 0.029906469530170805\n",
      "new best w\n",
      "iterations:  616\n",
      "Training loss: 0.016041259334678135, Validation loss: 0.0298783811889609\n",
      "new best w\n",
      "iterations:  617\n",
      "Training loss: 0.01602816921339895, Validation loss: 0.029854466777550014\n",
      "new best w\n",
      "iterations:  618\n",
      "Training loss: 0.01601259688082067, Validation loss: 0.02984400668180589\n",
      "new best w\n",
      "iterations:  619\n",
      "Training loss: 0.015998934491185328, Validation loss: 0.029799911592909383\n",
      "new best w\n",
      "iterations:  620\n",
      "Training loss: 0.015984065497426182, Validation loss: 0.029812324316125943\n",
      "iterations:  621\n",
      "Training loss: 0.015969673054581076, Validation loss: 0.029762041827814962\n",
      "new best w\n",
      "iterations:  622\n",
      "Training loss: 0.01595554943214635, Validation loss: 0.029764374851841323\n",
      "iterations:  623\n",
      "Training loss: 0.01594071891807243, Validation loss: 0.029721786208997097\n",
      "new best w\n",
      "iterations:  624\n",
      "Training loss: 0.01592678242149081, Validation loss: 0.029725192312494007\n",
      "iterations:  625\n",
      "Training loss: 0.01591164718221575, Validation loss: 0.029679970381553133\n",
      "new best w\n",
      "iterations:  626\n",
      "Training loss: 0.015897885232231426, Validation loss: 0.029685221146716085\n",
      "iterations:  627\n",
      "Training loss: 0.01588291455504137, Validation loss: 0.02962546848260536\n",
      "new best w\n",
      "iterations:  628\n",
      "Training loss: 0.015868984042551305, Validation loss: 0.029647405522706803\n",
      "iterations:  629\n",
      "Training loss: 0.015854284869456607, Validation loss: 0.0295775181634765\n",
      "new best w\n",
      "iterations:  630\n",
      "Training loss: 0.0158402603338264, Validation loss: 0.029594354599828964\n",
      "iterations:  631\n",
      "Training loss: 0.015825311523559756, Validation loss: 0.029549923373754235\n",
      "new best w\n",
      "iterations:  632\n",
      "Training loss: 0.015811442963910287, Validation loss: 0.029542083632829546\n",
      "new best w\n",
      "iterations:  633\n",
      "Training loss: 0.015796358618260144, Validation loss: 0.02952064437287577\n",
      "new best w\n",
      "iterations:  634\n",
      "Training loss: 0.015782755129283958, Validation loss: 0.029484316581106278\n",
      "new best w\n",
      "iterations:  635\n",
      "Training loss: 0.015767573830293816, Validation loss: 0.029489966152258008\n",
      "iterations:  636\n",
      "Training loss: 0.0157541404203027, Validation loss: 0.02943675222161411\n",
      "new best w\n",
      "iterations:  637\n",
      "Training loss: 0.015738920690179216, Validation loss: 0.029445106036637924\n",
      "iterations:  638\n",
      "Training loss: 0.015725215889084977, Validation loss: 0.029399580860156906\n",
      "new best w\n",
      "iterations:  639\n",
      "Training loss: 0.015710385014042136, Validation loss: 0.029394596898316257\n",
      "new best w\n",
      "iterations:  640\n",
      "Training loss: 0.015696118476742113, Validation loss: 0.029354798689456996\n",
      "new best w\n",
      "iterations:  641\n",
      "Training loss: 0.015681980358557394, Validation loss: 0.029360753137989047\n",
      "iterations:  642\n",
      "Training loss: 0.015667016831626867, Validation loss: 0.029310813253769212\n",
      "new best w\n",
      "iterations:  643\n",
      "Training loss: 0.015653810089462674, Validation loss: 0.029323872887691588\n",
      "iterations:  644\n",
      "Training loss: 0.01563815034213726, Validation loss: 0.029259399445847146\n",
      "new best w\n",
      "iterations:  645\n",
      "Training loss: 0.01562510742660139, Validation loss: 0.029255902664851213\n",
      "new best w\n",
      "iterations:  646\n",
      "Training loss: 0.015609519895704551, Validation loss: 0.02923778502752184\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  647\n",
      "Training loss: 0.015596163541268965, Validation loss: 0.029213522798727333\n",
      "new best w\n",
      "iterations:  648\n",
      "Training loss: 0.01558111146199494, Validation loss: 0.02918892126383605\n",
      "new best w\n",
      "iterations:  649\n",
      "Training loss: 0.015567279918566386, Validation loss: 0.029158280585187303\n",
      "new best w\n",
      "iterations:  650\n",
      "Training loss: 0.015552965426006947, Validation loss: 0.029159866136478918\n",
      "iterations:  651\n",
      "Training loss: 0.015538512792678627, Validation loss: 0.029110555165985652\n",
      "new best w\n",
      "iterations:  652\n",
      "Training loss: 0.01552438900839705, Validation loss: 0.029121651715976932\n",
      "iterations:  653\n",
      "Training loss: 0.015509846860446326, Validation loss: 0.02906889804548223\n",
      "new best w\n",
      "iterations:  654\n",
      "Training loss: 0.015495656047014042, Validation loss: 0.02908220451013293\n",
      "iterations:  655\n",
      "Training loss: 0.015481294360226135, Validation loss: 0.02902882924530683\n",
      "new best w\n",
      "iterations:  656\n",
      "Training loss: 0.015467267830884947, Validation loss: 0.029042695263848395\n",
      "iterations:  657\n",
      "Training loss: 0.015452729457527372, Validation loss: 0.02896827932293891\n",
      "new best w\n",
      "iterations:  658\n",
      "Training loss: 0.015438732637143326, Validation loss: 0.028984002103660155\n",
      "iterations:  659\n",
      "Training loss: 0.015423592040762933, Validation loss: 0.02894357175951609\n",
      "new best w\n",
      "iterations:  660\n",
      "Training loss: 0.01541014850120689, Validation loss: 0.028929973190549595\n",
      "new best w\n",
      "iterations:  661\n",
      "Training loss: 0.015394809330141207, Validation loss: 0.028914200480522956\n",
      "new best w\n",
      "iterations:  662\n",
      "Training loss: 0.015381598733472336, Validation loss: 0.028862629918456496\n",
      "new best w\n",
      "iterations:  663\n",
      "Training loss: 0.01536640062069078, Validation loss: 0.028880549420206163\n",
      "iterations:  664\n",
      "Training loss: 0.01535267982699039, Validation loss: 0.028819866406146886\n",
      "new best w\n",
      "iterations:  665\n",
      "Training loss: 0.015338023692174406, Validation loss: 0.02883950590792017\n",
      "iterations:  666\n",
      "Training loss: 0.015323482374056658, Validation loss: 0.02878211796932646\n",
      "new best w\n",
      "iterations:  667\n",
      "Training loss: 0.015309729777908258, Validation loss: 0.028786043553308306\n",
      "iterations:  668\n",
      "Training loss: 0.015294450626028473, Validation loss: 0.028739722921168283\n",
      "new best w\n",
      "iterations:  669\n",
      "Training loss: 0.015281521268513036, Validation loss: 0.028731255655367667\n",
      "new best w\n",
      "iterations:  670\n",
      "Training loss: 0.015265766896336573, Validation loss: 0.028711726646237102\n",
      "new best w\n",
      "iterations:  671\n",
      "Training loss: 0.015252750074788338, Validation loss: 0.028687801891183466\n",
      "new best w\n",
      "iterations:  672\n",
      "Training loss: 0.015237519414483243, Validation loss: 0.028664632558494114\n",
      "new best w\n",
      "iterations:  673\n",
      "Training loss: 0.015223721593039745, Validation loss: 0.028621690292610087\n",
      "new best w\n",
      "iterations:  674\n",
      "Training loss: 0.015209517586054712, Validation loss: 0.028633811545163146\n",
      "iterations:  675\n",
      "Training loss: 0.015194876657685184, Validation loss: 0.028582663324918146\n",
      "new best w\n",
      "iterations:  676\n",
      "Training loss: 0.015181386065581727, Validation loss: 0.028593265278591785\n",
      "iterations:  677\n",
      "Training loss: 0.01516657727430963, Validation loss: 0.02853288619674745\n",
      "new best w\n",
      "iterations:  678\n",
      "Training loss: 0.015153180877429015, Validation loss: 0.02856135682649511\n",
      "iterations:  679\n",
      "Training loss: 0.015138504755265979, Validation loss: 0.02848616181112703\n",
      "new best w\n",
      "iterations:  680\n",
      "Training loss: 0.015124843374093675, Validation loss: 0.02851183878888026\n",
      "iterations:  681\n",
      "Training loss: 0.015110451283666872, Validation loss: 0.02846576383240095\n",
      "new best w\n",
      "iterations:  682\n",
      "Training loss: 0.01509659545641308, Validation loss: 0.02845464071080859\n",
      "new best w\n",
      "iterations:  683\n",
      "Training loss: 0.015082227851896642, Validation loss: 0.028416307831605602\n",
      "new best w\n",
      "iterations:  684\n",
      "Training loss: 0.015068439195565582, Validation loss: 0.028382597515984563\n",
      "new best w\n",
      "iterations:  685\n",
      "Training loss: 0.01505377553872222, Validation loss: 0.02838936706484609\n",
      "iterations:  686\n",
      "Training loss: 0.015038727080726956, Validation loss: 0.028340491197998984\n",
      "new best w\n",
      "iterations:  687\n",
      "Training loss: 0.015021857774482157, Validation loss: 0.028336704627834948\n",
      "new best w\n",
      "iterations:  688\n",
      "Training loss: 0.015005492861867873, Validation loss: 0.028266948462531572\n",
      "new best w\n",
      "iterations:  689\n",
      "Training loss: 0.014989162544542366, Validation loss: 0.028279337903124556\n",
      "iterations:  690\n",
      "Training loss: 0.014972003543288892, Validation loss: 0.028212361013423976\n",
      "new best w\n",
      "iterations:  691\n",
      "Training loss: 0.014956244825804335, Validation loss: 0.02819705574066031\n",
      "new best w\n",
      "iterations:  692\n",
      "Training loss: 0.014938698195294715, Validation loss: 0.02817123756941874\n",
      "new best w\n",
      "iterations:  693\n",
      "Training loss: 0.014923457669844452, Validation loss: 0.028129194176396883\n",
      "new best w\n",
      "iterations:  694\n",
      "Training loss: 0.014905931622207864, Validation loss: 0.028111003233422877\n",
      "new best w\n",
      "iterations:  695\n",
      "Training loss: 0.014890249607833108, Validation loss: 0.028070571802737827\n",
      "new best w\n",
      "iterations:  696\n",
      "Training loss: 0.014873334328923451, Validation loss: 0.028070661461816877\n",
      "iterations:  697\n",
      "Training loss: 0.014856939924608188, Validation loss: 0.02801455592720592\n",
      "new best w\n",
      "iterations:  698\n",
      "Training loss: 0.014840664527926333, Validation loss: 0.02800731276466293\n",
      "new best w\n",
      "iterations:  699\n",
      "Training loss: 0.014823721864647137, Validation loss: 0.027954632893363834\n",
      "new best w\n",
      "iterations:  700\n",
      "Training loss: 0.014808312751253001, Validation loss: 0.027962471750661678\n",
      "iterations:  701\n",
      "Training loss: 0.014790652994344981, Validation loss: 0.02789682877496803\n",
      "new best w\n",
      "iterations:  702\n",
      "Training loss: 0.014775525486224884, Validation loss: 0.02787820028958641\n",
      "new best w\n",
      "iterations:  703\n",
      "Training loss: 0.014757875654418195, Validation loss: 0.027853133725521943\n",
      "new best w\n",
      "iterations:  704\n",
      "Training loss: 0.014742557828824982, Validation loss: 0.027835179592092613\n",
      "new best w\n",
      "iterations:  705\n",
      "Training loss: 0.014725515353178316, Validation loss: 0.027785415364313965\n",
      "new best w\n",
      "iterations:  706\n",
      "Training loss: 0.014709459331673587, Validation loss: 0.027769771491674596\n",
      "new best w\n",
      "iterations:  707\n",
      "Training loss: 0.0146932269456522, Validation loss: 0.02776918054670525\n",
      "new best w\n",
      "iterations:  708\n",
      "Training loss: 0.014677932042874748, Validation loss: 0.027724148733293826\n",
      "new best w\n",
      "iterations:  709\n",
      "Training loss: 0.014664427928609019, Validation loss: 0.027718405835790244\n",
      "new best w\n",
      "iterations:  710\n",
      "Training loss: 0.014649666869358107, Validation loss: 0.02769213259566859\n",
      "new best w\n",
      "iterations:  711\n",
      "Training loss: 0.014636323532529267, Validation loss: 0.02767244331178354\n",
      "new best w\n",
      "iterations:  712\n",
      "Training loss: 0.014621659783393061, Validation loss: 0.0276301822065405\n",
      "new best w\n",
      "iterations:  713\n",
      "Training loss: 0.01460829301472649, Validation loss: 0.027647892332558828\n",
      "iterations:  714\n",
      "Training loss: 0.01459282093419307, Validation loss: 0.027589407557656116\n",
      "new best w\n",
      "iterations:  715\n",
      "Training loss: 0.014578349453177274, Validation loss: 0.02759034042423386\n",
      "iterations:  716\n",
      "Training loss: 0.014563252791815934, Validation loss: 0.027550520184761777\n",
      "new best w\n",
      "iterations:  717\n",
      "Training loss: 0.014548624606479724, Validation loss: 0.027543211467067712\n",
      "new best w\n",
      "iterations:  718\n",
      "Training loss: 0.014533409926573242, Validation loss: 0.027504601118854315\n",
      "new best w\n",
      "iterations:  719\n",
      "Training loss: 0.01451897842994972, Validation loss: 0.0274702566386186\n",
      "new best w\n",
      "iterations:  720\n",
      "Training loss: 0.014503607991743512, Validation loss: 0.02747246401970067\n",
      "iterations:  721\n",
      "Training loss: 0.014489396282214007, Validation loss: 0.027429307777803107\n",
      "new best w\n",
      "iterations:  722\n",
      "Training loss: 0.014474064385510545, Validation loss: 0.027431698994970034\n",
      "iterations:  723\n",
      "Training loss: 0.014459574758574477, Validation loss: 0.027382521971882328\n",
      "new best w\n",
      "iterations:  724\n",
      "Training loss: 0.014444679974507595, Validation loss: 0.0273847653634198\n",
      "iterations:  725\n",
      "Training loss: 0.014429578572905182, Validation loss: 0.02733337868063121\n",
      "new best w\n",
      "iterations:  726\n",
      "Training loss: 0.014415826118739346, Validation loss: 0.027347376086193885\n",
      "iterations:  727\n",
      "Training loss: 0.014400065429949302, Validation loss: 0.027283370900746805\n",
      "new best w\n",
      "iterations:  728\n",
      "Training loss: 0.01438649449801052, Validation loss: 0.027276737273837743\n",
      "new best w\n",
      "iterations:  729\n",
      "Training loss: 0.014370577773642552, Validation loss: 0.027257377704495933\n",
      "new best w\n",
      "iterations:  730\n",
      "Training loss: 0.014356888726709522, Validation loss: 0.027223136682642656\n",
      "new best w\n",
      "iterations:  731\n",
      "Training loss: 0.014341685558936963, Validation loss: 0.02720607441231821\n",
      "new best w\n",
      "iterations:  732\n",
      "Training loss: 0.014327136336591767, Validation loss: 0.027188707556295065\n",
      "new best w\n",
      "iterations:  733\n",
      "Training loss: 0.014312852740161735, Validation loss: 0.027176858005736514\n",
      "new best w\n",
      "iterations:  734\n",
      "Training loss: 0.01429770716961383, Validation loss: 0.027137871644816743\n",
      "new best w\n",
      "iterations:  735\n",
      "Training loss: 0.014283424044313849, Validation loss: 0.027131271445592957\n",
      "new best w\n",
      "iterations:  736\n",
      "Training loss: 0.014268475582931506, Validation loss: 0.027090746574896102\n",
      "new best w\n",
      "iterations:  737\n",
      "Training loss: 0.014253971798608338, Validation loss: 0.027093476299865832\n",
      "iterations:  738\n",
      "Training loss: 0.014239704105432884, Validation loss: 0.027037251143995644\n",
      "new best w\n",
      "iterations:  739\n",
      "Training loss: 0.014225386440383117, Validation loss: 0.027057554275996144\n",
      "iterations:  740\n",
      "Training loss: 0.014210705197031175, Validation loss: 0.026992045802262\n",
      "new best w\n",
      "iterations:  741\n",
      "Training loss: 0.014196578758339174, Validation loss: 0.026988682947476583\n",
      "new best w\n",
      "iterations:  742\n",
      "Training loss: 0.014181416503209343, Validation loss: 0.026969396121447786\n",
      "new best w\n",
      "iterations:  743\n",
      "Training loss: 0.014168228979001735, Validation loss: 0.026928727496682126\n",
      "new best w\n",
      "iterations:  744\n",
      "Training loss: 0.01415285941893692, Validation loss: 0.02692958678561557\n",
      "iterations:  745\n",
      "Training loss: 0.014139247694593928, Validation loss: 0.026882011165953994\n",
      "new best w\n",
      "iterations:  746\n",
      "Training loss: 0.014124262083372273, Validation loss: 0.02688510495972813\n",
      "iterations:  747\n",
      "Training loss: 0.014109882674092722, Validation loss: 0.026845572197408317\n",
      "new best w\n",
      "iterations:  748\n",
      "Training loss: 0.01409601611191716, Validation loss: 0.02683901736257323\n",
      "new best w\n",
      "iterations:  749\n",
      "Training loss: 0.01408092092973336, Validation loss: 0.026795561924554855\n",
      "new best w\n",
      "iterations:  750\n",
      "Training loss: 0.014067912161260198, Validation loss: 0.026801541913846056\n",
      "iterations:  751\n",
      "Training loss: 0.014052156875421917, Validation loss: 0.02675031766728918\n",
      "new best w\n",
      "iterations:  752\n",
      "Training loss: 0.014038627986895242, Validation loss: 0.026740443137391663\n",
      "new best w\n",
      "iterations:  753\n",
      "Training loss: 0.014023880198448767, Validation loss: 0.02671657230497717\n",
      "new best w\n",
      "iterations:  754\n",
      "Training loss: 0.014009902855141181, Validation loss: 0.026685029485300115\n",
      "new best w\n",
      "iterations:  755\n",
      "Training loss: 0.01399579542479469, Validation loss: 0.02668059247295822\n",
      "new best w\n",
      "iterations:  756\n",
      "Training loss: 0.013981114152159415, Validation loss: 0.026640544986389087\n",
      "new best w\n",
      "iterations:  757\n",
      "Training loss: 0.013966913546440257, Validation loss: 0.026636817391887824\n",
      "new best w\n",
      "iterations:  758\n",
      "Training loss: 0.0139527049947953, Validation loss: 0.026592422575802514\n",
      "new best w\n",
      "iterations:  759\n",
      "Training loss: 0.013938277716567955, Validation loss: 0.026598364494075977\n",
      "iterations:  760\n",
      "Training loss: 0.013924268616262565, Validation loss: 0.02654019417877911\n",
      "new best w\n",
      "iterations:  761\n",
      "Training loss: 0.013909994914275036, Validation loss: 0.026537964915836598\n",
      "new best w\n",
      "iterations:  762\n",
      "Training loss: 0.013895342279835098, Validation loss: 0.026514425375431715\n",
      "new best w\n",
      "iterations:  763\n",
      "Training loss: 0.013881874599415499, Validation loss: 0.02649044099282149\n",
      "new best w\n",
      "iterations:  764\n",
      "Training loss: 0.013866715846202032, Validation loss: 0.026462126982749568\n",
      "new best w\n",
      "iterations:  765\n",
      "Training loss: 0.01385358310865018, Validation loss: 0.02643211011160196\n",
      "new best w\n",
      "iterations:  766\n",
      "Training loss: 0.013838520340071365, Validation loss: 0.026436736462924975\n",
      "iterations:  767\n",
      "Training loss: 0.013824653844667842, Validation loss: 0.026390825722628658\n",
      "new best w\n",
      "iterations:  768\n",
      "Training loss: 0.013810597535740196, Validation loss: 0.02638634464665616\n",
      "new best w\n",
      "iterations:  769\n",
      "Training loss: 0.013795676917397621, Validation loss: 0.026342249731408883\n",
      "new best w\n",
      "iterations:  770\n",
      "Training loss: 0.01378275537458402, Validation loss: 0.026332282870869434\n",
      "new best w\n",
      "iterations:  771\n",
      "Training loss: 0.013767245453757, Validation loss: 0.026312760298038362\n",
      "new best w\n",
      "iterations:  772\n",
      "Training loss: 0.013754166088868338, Validation loss: 0.026290823667691216\n",
      "new best w\n",
      "iterations:  773\n",
      "Training loss: 0.013739405950156632, Validation loss: 0.026251466965738508\n",
      "new best w\n",
      "iterations:  774\n",
      "Training loss: 0.013725316075650055, Validation loss: 0.02623484121621008\n",
      "new best w\n",
      "iterations:  775\n",
      "Training loss: 0.013711634983662376, Validation loss: 0.026229574028746327\n",
      "new best w\n",
      "iterations:  776\n",
      "Training loss: 0.01369681751514489, Validation loss: 0.026191323396372285\n",
      "new best w\n",
      "iterations:  777\n",
      "Training loss: 0.013683279588421979, Validation loss: 0.026187565044415544\n",
      "new best w\n",
      "iterations:  778\n",
      "Training loss: 0.013668726500915213, Validation loss: 0.02613244799079406\n",
      "new best w\n",
      "iterations:  779\n",
      "Training loss: 0.01365490920352616, Validation loss: 0.026145492788766017\n",
      "iterations:  780\n",
      "Training loss: 0.013640738410829798, Validation loss: 0.0261079198347073\n",
      "new best w\n",
      "iterations:  781\n",
      "Training loss: 0.013626763260978988, Validation loss: 0.026090765867086328\n",
      "new best w\n",
      "iterations:  782\n",
      "Training loss: 0.013612373789020492, Validation loss: 0.026052022677349452\n",
      "new best w\n",
      "iterations:  783\n",
      "Training loss: 0.013598713097117007, Validation loss: 0.026024813605774168\n",
      "new best w\n",
      "iterations:  784\n",
      "Training loss: 0.013584086832878026, Validation loss: 0.026033853802829725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  785\n",
      "Training loss: 0.013570579977336815, Validation loss: 0.02598735216988964\n",
      "new best w\n",
      "iterations:  786\n",
      "Training loss: 0.013556290915913665, Validation loss: 0.0259881412347152\n",
      "iterations:  787\n",
      "Training loss: 0.013541872702439405, Validation loss: 0.025933787302189964\n",
      "new best w\n",
      "iterations:  788\n",
      "Training loss: 0.013528384047298097, Validation loss: 0.02592657937596634\n",
      "new best w\n",
      "iterations:  789\n",
      "Training loss: 0.013513123826080808, Validation loss: 0.025907033611180713\n",
      "new best w\n",
      "iterations:  790\n",
      "Training loss: 0.013500226974819785, Validation loss: 0.025864077662505914\n",
      "new best w\n",
      "iterations:  791\n",
      "Training loss: 0.013485047013833726, Validation loss: 0.025874856176542088\n",
      "iterations:  792\n",
      "Training loss: 0.01347092202975132, Validation loss: 0.025829262748998643\n",
      "new best w\n",
      "iterations:  793\n",
      "Training loss: 0.01345722619933929, Validation loss: 0.025825344650420346\n",
      "new best w\n",
      "iterations:  794\n",
      "Training loss: 0.013442238779411739, Validation loss: 0.025782360230809702\n",
      "new best w\n",
      "iterations:  795\n",
      "Training loss: 0.013429064920222479, Validation loss: 0.025768072640351757\n",
      "new best w\n",
      "iterations:  796\n",
      "Training loss: 0.013413831537938584, Validation loss: 0.025748489163214296\n",
      "new best w\n",
      "iterations:  797\n",
      "Training loss: 0.013400268400735597, Validation loss: 0.02571636336627669\n",
      "new best w\n",
      "iterations:  798\n",
      "Training loss: 0.013386236958955737, Validation loss: 0.025703221099488575\n",
      "new best w\n",
      "iterations:  799\n",
      "Training loss: 0.013371689288991809, Validation loss: 0.02567812333317539\n",
      "new best w\n",
      "iterations:  800\n",
      "Training loss: 0.013357876128507045, Validation loss: 0.025667559888422276\n",
      "new best w\n",
      "iterations:  801\n",
      "Training loss: 0.013343285935624716, Validation loss: 0.02562163417009501\n",
      "new best w\n",
      "iterations:  802\n",
      "Training loss: 0.013329470185345053, Validation loss: 0.025637705191730358\n",
      "iterations:  803\n",
      "Training loss: 0.013315320242936095, Validation loss: 0.02557427864272588\n",
      "new best w\n",
      "iterations:  804\n",
      "Training loss: 0.013301240040786004, Validation loss: 0.02557494285844289\n",
      "iterations:  805\n",
      "Training loss: 0.013286929463184487, Validation loss: 0.02553609462605149\n",
      "new best w\n",
      "iterations:  806\n",
      "Training loss: 0.013273209082676018, Validation loss: 0.025517952890412116\n",
      "new best w\n",
      "iterations:  807\n",
      "Training loss: 0.013258571618108122, Validation loss: 0.025519107524896268\n",
      "iterations:  808\n",
      "Training loss: 0.013245118945955016, Validation loss: 0.02547301743357478\n",
      "new best w\n",
      "iterations:  809\n",
      "Training loss: 0.013230845871706127, Validation loss: 0.02547440140030342\n",
      "iterations:  810\n",
      "Training loss: 0.013216264426216515, Validation loss: 0.02542002753155439\n",
      "new best w\n",
      "iterations:  811\n",
      "Training loss: 0.013203263739129244, Validation loss: 0.02541352589859095\n",
      "new best w\n",
      "iterations:  812\n",
      "Training loss: 0.013188218644977911, Validation loss: 0.025387353112630677\n",
      "new best w\n",
      "iterations:  813\n",
      "Training loss: 0.013175038656298156, Validation loss: 0.025353809991861643\n",
      "new best w\n",
      "iterations:  814\n",
      "Training loss: 0.01316045962291186, Validation loss: 0.025357076067669605\n",
      "iterations:  815\n",
      "Training loss: 0.013146134094301393, Validation loss: 0.025321193150988887\n",
      "new best w\n",
      "iterations:  816\n",
      "Training loss: 0.013133115576871903, Validation loss: 0.025314948503583883\n",
      "new best w\n",
      "iterations:  817\n",
      "Training loss: 0.013117990176216796, Validation loss: 0.025264827172048865\n",
      "new best w\n",
      "iterations:  818\n",
      "Training loss: 0.013104873869471814, Validation loss: 0.025260756587161082\n",
      "new best w\n",
      "iterations:  819\n",
      "Training loss: 0.013090390380250307, Validation loss: 0.025222012405955146\n",
      "new best w\n",
      "iterations:  820\n",
      "Training loss: 0.013076426877116134, Validation loss: 0.025207688215347922\n",
      "new best w\n",
      "iterations:  821\n",
      "Training loss: 0.01306294030229274, Validation loss: 0.025202725029129035\n",
      "new best w\n",
      "iterations:  822\n",
      "Training loss: 0.013048292667283069, Validation loss: 0.0251654056147863\n",
      "new best w\n",
      "iterations:  823\n",
      "Training loss: 0.013034678763330119, Validation loss: 0.025162936442378814\n",
      "new best w\n",
      "iterations:  824\n",
      "Training loss: 0.013020727279927653, Validation loss: 0.02510617352667662\n",
      "new best w\n",
      "iterations:  825\n",
      "Training loss: 0.01300684827060246, Validation loss: 0.025114395072802546\n",
      "iterations:  826\n",
      "Training loss: 0.012992788665739757, Validation loss: 0.02506666400424689\n",
      "new best w\n",
      "iterations:  827\n",
      "Training loss: 0.012978986392374617, Validation loss: 0.025047323179713143\n",
      "new best w\n",
      "iterations:  828\n",
      "Training loss: 0.012964601801955664, Validation loss: 0.025052936957402676\n",
      "iterations:  829\n",
      "Training loss: 0.012951242663131307, Validation loss: 0.025007540520446653\n",
      "new best w\n",
      "iterations:  830\n",
      "Training loss: 0.012937105121948893, Validation loss: 0.025010832019413055\n",
      "iterations:  831\n",
      "Training loss: 0.01292293868113817, Validation loss: 0.02495517247508842\n",
      "new best w\n",
      "iterations:  832\n",
      "Training loss: 0.012909730912990134, Validation loss: 0.024957456253313224\n",
      "iterations:  833\n",
      "Training loss: 0.012894912382764816, Validation loss: 0.024922037594511493\n",
      "new best w\n",
      "iterations:  834\n",
      "Training loss: 0.012882046572265634, Validation loss: 0.024895657303559644\n",
      "new best w\n",
      "iterations:  835\n",
      "Training loss: 0.012867537424460795, Validation loss: 0.024892689683002355\n",
      "new best w\n",
      "iterations:  836\n",
      "Training loss: 0.012853351402748467, Validation loss: 0.02485884760130669\n",
      "new best w\n",
      "iterations:  837\n",
      "Training loss: 0.012840553500058612, Validation loss: 0.024852556694551272\n",
      "new best w\n",
      "iterations:  838\n",
      "Training loss: 0.012825528400734323, Validation loss: 0.024802654484513024\n",
      "new best w\n",
      "iterations:  839\n",
      "Training loss: 0.01281263006690184, Validation loss: 0.02479940636808018\n",
      "new best w\n",
      "iterations:  840\n",
      "Training loss: 0.012798278015714292, Validation loss: 0.02476042100379667\n",
      "new best w\n",
      "iterations:  841\n",
      "Training loss: 0.012784345180309225, Validation loss: 0.024747102467374226\n",
      "new best w\n",
      "iterations:  842\n",
      "Training loss: 0.012771124903177127, Validation loss: 0.024742240276353026\n",
      "new best w\n",
      "iterations:  843\n",
      "Training loss: 0.01275653378606035, Validation loss: 0.02469766233254276\n",
      "new best w\n",
      "iterations:  844\n",
      "Training loss: 0.012743158955989655, Validation loss: 0.024686337891493403\n",
      "new best w\n",
      "iterations:  845\n",
      "Training loss: 0.012729302613823221, Validation loss: 0.024664515812487173\n",
      "new best w\n",
      "iterations:  846\n",
      "Training loss: 0.012715685792409124, Validation loss: 0.02465646462529609\n",
      "new best w\n",
      "iterations:  847\n",
      "Training loss: 0.012701732938517885, Validation loss: 0.024607647278474486\n",
      "new best w\n",
      "iterations:  848\n",
      "Training loss: 0.012687959485649882, Validation loss: 0.0245890984361313\n",
      "new best w\n",
      "iterations:  849\n",
      "Training loss: 0.0126739567364547, Validation loss: 0.024595200084431352\n",
      "iterations:  850\n",
      "Training loss: 0.01266066467314966, Validation loss: 0.02453735973159548\n",
      "new best w\n",
      "iterations:  851\n",
      "Training loss: 0.01264677046641764, Validation loss: 0.0245392974374422\n",
      "iterations:  852\n",
      "Training loss: 0.012632590475729148, Validation loss: 0.024509454262062712\n",
      "new best w\n",
      "iterations:  853\n",
      "Training loss: 0.01261989104758491, Validation loss: 0.02446987756952349\n",
      "new best w\n",
      "iterations:  854\n",
      "Training loss: 0.012605358985884425, Validation loss: 0.024490808055321077\n",
      "iterations:  855\n",
      "Training loss: 0.012592030549551245, Validation loss: 0.024434928622739965\n",
      "new best w\n",
      "iterations:  856\n",
      "Training loss: 0.012578274710518074, Validation loss: 0.02444052077189269\n",
      "iterations:  857\n",
      "Training loss: 0.01256394970021058, Validation loss: 0.024391075443184026\n",
      "new best w\n",
      "iterations:  858\n",
      "Training loss: 0.012551473126573898, Validation loss: 0.02439265469914794\n",
      "iterations:  859\n",
      "Training loss: 0.01253669944864352, Validation loss: 0.024350806511955134\n",
      "new best w\n",
      "iterations:  860\n",
      "Training loss: 0.012523557559690724, Validation loss: 0.024329251608516757\n",
      "new best w\n",
      "iterations:  861\n",
      "Training loss: 0.012510017505663298, Validation loss: 0.02432653737372868\n",
      "new best w\n",
      "iterations:  862\n",
      "Training loss: 0.012495804831932604, Validation loss: 0.02429779774346679\n",
      "new best w\n",
      "iterations:  863\n",
      "Training loss: 0.012482952495017215, Validation loss: 0.02428571019582213\n",
      "new best w\n",
      "iterations:  864\n",
      "Training loss: 0.012468538196399008, Validation loss: 0.024235548420455534\n",
      "new best w\n",
      "iterations:  865\n",
      "Training loss: 0.01245520974990954, Validation loss: 0.024239781075724477\n",
      "iterations:  866\n",
      "Training loss: 0.012440555625022371, Validation loss: 0.02419850942712976\n",
      "new best w\n",
      "iterations:  867\n",
      "Training loss: 0.012425396682738907, Validation loss: 0.024180326678100557\n",
      "new best w\n",
      "iterations:  868\n",
      "Training loss: 0.012410578104808865, Validation loss: 0.024180317672873465\n",
      "new best w\n",
      "iterations:  869\n",
      "Training loss: 0.012396032132189101, Validation loss: 0.02412331579473296\n",
      "new best w\n",
      "iterations:  870\n",
      "Training loss: 0.012381144735076477, Validation loss: 0.024133771238480976\n",
      "iterations:  871\n",
      "Training loss: 0.012366155100701015, Validation loss: 0.02409996870620082\n",
      "new best w\n",
      "iterations:  872\n",
      "Training loss: 0.012351861857368822, Validation loss: 0.024066929052200425\n",
      "new best w\n",
      "iterations:  873\n",
      "Training loss: 0.012336313209849133, Validation loss: 0.02406532811788386\n",
      "new best w\n",
      "iterations:  874\n",
      "Training loss: 0.01232235090877659, Validation loss: 0.02402520875011053\n",
      "new best w\n",
      "iterations:  875\n",
      "Training loss: 0.012307254704622857, Validation loss: 0.02403037352344959\n",
      "iterations:  876\n",
      "Training loss: 0.012292006593505054, Validation loss: 0.02397569322584905\n",
      "new best w\n",
      "iterations:  877\n",
      "Training loss: 0.012278438872540353, Validation loss: 0.023973996899304496\n",
      "new best w\n",
      "iterations:  878\n",
      "Training loss: 0.012262673605947674, Validation loss: 0.02393703557635607\n",
      "new best w\n",
      "iterations:  879\n",
      "Training loss: 0.012248565112182802, Validation loss: 0.023923257552193617\n",
      "new best w\n",
      "iterations:  880\n",
      "Training loss: 0.01223395285644407, Validation loss: 0.023912304063237754\n",
      "new best w\n",
      "iterations:  881\n",
      "Training loss: 0.012218719810507128, Validation loss: 0.023884581150211106\n",
      "new best w\n",
      "iterations:  882\n",
      "Training loss: 0.01220491724221372, Validation loss: 0.023871905039540824\n",
      "new best w\n",
      "iterations:  883\n",
      "Training loss: 0.012189568396531266, Validation loss: 0.023821757505203066\n",
      "new best w\n",
      "iterations:  884\n",
      "Training loss: 0.012175279581665158, Validation loss: 0.023826575843719604\n",
      "iterations:  885\n",
      "Training loss: 0.012160923020596029, Validation loss: 0.023783790487166473\n",
      "new best w\n",
      "iterations:  886\n",
      "Training loss: 0.01214596479531871, Validation loss: 0.023767014143457266\n",
      "new best w\n",
      "iterations:  887\n",
      "Training loss: 0.012131337212685136, Validation loss: 0.023767274738663875\n",
      "iterations:  888\n",
      "Training loss: 0.01211725071885819, Validation loss: 0.02371056008963047\n",
      "new best w\n",
      "iterations:  889\n",
      "Training loss: 0.012102506914970652, Validation loss: 0.023720493493907722\n",
      "iterations:  890\n",
      "Training loss: 0.012087683520497603, Validation loss: 0.02367321753049411\n",
      "new best w\n",
      "iterations:  891\n",
      "Training loss: 0.012073718297991818, Validation loss: 0.023652894989191944\n",
      "new best w\n",
      "iterations:  892\n",
      "Training loss: 0.012058550765747873, Validation loss: 0.023662883931181336\n",
      "iterations:  893\n",
      "Training loss: 0.0120444901785927, Validation loss: 0.023617767948144748\n",
      "new best w\n",
      "iterations:  894\n",
      "Training loss: 0.012030165283794705, Validation loss: 0.02361400456203573\n",
      "new best w\n",
      "iterations:  895\n",
      "Training loss: 0.012015140318563622, Validation loss: 0.02356764246785966\n",
      "new best w\n",
      "iterations:  896\n",
      "Training loss: 0.01200216922406503, Validation loss: 0.02356320500194907\n",
      "new best w\n",
      "iterations:  897\n",
      "Training loss: 0.011986897988657073, Validation loss: 0.02352855057066995\n",
      "new best w\n",
      "iterations:  898\n",
      "Training loss: 0.011972875151796203, Validation loss: 0.02351491274991737\n",
      "new best w\n",
      "iterations:  899\n",
      "Training loss: 0.011959284367449117, Validation loss: 0.02350475698493712\n",
      "new best w\n",
      "iterations:  900\n",
      "Training loss: 0.011944361789697171, Validation loss: 0.02346473785567106\n",
      "new best w\n",
      "iterations:  901\n",
      "Training loss: 0.01193078237833823, Validation loss: 0.02347056529360071\n",
      "iterations:  902\n",
      "Training loss: 0.01191631341050592, Validation loss: 0.023416907709382102\n",
      "new best w\n",
      "iterations:  903\n",
      "Training loss: 0.011903533559347073, Validation loss: 0.023417131242573486\n",
      "iterations:  904\n",
      "Training loss: 0.011890806528136297, Validation loss: 0.023392206635121128\n",
      "new best w\n",
      "iterations:  905\n",
      "Training loss: 0.01187758339126192, Validation loss: 0.023359194614336685\n",
      "new best w\n",
      "iterations:  906\n",
      "Training loss: 0.011864772742989076, Validation loss: 0.023367594669270936\n",
      "iterations:  907\n",
      "Training loss: 0.011852316966734564, Validation loss: 0.023308813673890268\n",
      "new best w\n",
      "iterations:  908\n",
      "Training loss: 0.011839485844756343, Validation loss: 0.023342879245609426\n",
      "iterations:  909\n",
      "Training loss: 0.01182627331064901, Validation loss: 0.023276519426946266\n",
      "new best w\n",
      "iterations:  910\n",
      "Training loss: 0.011814170729373012, Validation loss: 0.023255436157676012\n",
      "new best w\n",
      "iterations:  911\n",
      "Training loss: 0.011801027186576744, Validation loss: 0.023265949430801472\n",
      "iterations:  912\n",
      "Training loss: 0.011789165226074191, Validation loss: 0.02322523691155971\n",
      "new best w\n",
      "iterations:  913\n",
      "Training loss: 0.011776675366844314, Validation loss: 0.023219765581837824\n",
      "new best w\n",
      "iterations:  914\n",
      "Training loss: 0.011763715548069256, Validation loss: 0.023174414641539295\n",
      "new best w\n",
      "iterations:  915\n",
      "Training loss: 0.011752403081169576, Validation loss: 0.023169663670548207\n",
      "new best w\n",
      "iterations:  916\n",
      "Training loss: 0.011739026875679, Validation loss: 0.023145822850062853\n",
      "new best w\n",
      "iterations:  917\n",
      "Training loss: 0.011727265481764974, Validation loss: 0.02313144982825321\n",
      "new best w\n",
      "iterations:  918\n",
      "Training loss: 0.011714542529217472, Validation loss: 0.023121338558982817\n",
      "new best w\n",
      "iterations:  919\n",
      "Training loss: 0.011701782745064664, Validation loss: 0.023079845808910596\n",
      "new best w\n",
      "iterations:  920\n",
      "Training loss: 0.0116903895944376, Validation loss: 0.023066204437975503\n",
      "new best w\n",
      "iterations:  921\n",
      "Training loss: 0.011676954259700067, Validation loss: 0.023053792662606206\n",
      "new best w\n",
      "iterations:  922\n",
      "Training loss: 0.011665479867516102, Validation loss: 0.023017541711471887\n",
      "new best w\n",
      "iterations:  923\n",
      "Training loss: 0.0116527817720815, Validation loss: 0.02300623118830333\n",
      "new best w\n",
      "iterations:  924\n",
      "Training loss: 0.011640102339145943, Validation loss: 0.022988869132449602\n",
      "new best w\n",
      "iterations:  925\n",
      "Training loss: 0.01162888805407998, Validation loss: 0.023010912510180666\n",
      "iterations:  926\n",
      "Training loss: 0.01161555179597285, Validation loss: 0.022934844794541116\n",
      "new best w\n",
      "iterations:  927\n",
      "Training loss: 0.01160356269403363, Validation loss: 0.022939981583180724\n",
      "iterations:  928\n",
      "Training loss: 0.011591290756588964, Validation loss: 0.02289447874741938\n",
      "new best w\n",
      "iterations:  929\n",
      "Training loss: 0.011579057187557942, Validation loss: 0.02289559390206166\n",
      "iterations:  930\n",
      "Training loss: 0.011566726715886811, Validation loss: 0.022877630653869968\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  931\n",
      "Training loss: 0.011554179837650553, Validation loss: 0.02284329729458338\n",
      "new best w\n",
      "iterations:  932\n",
      "Training loss: 0.011542033521296624, Validation loss: 0.022866809063505487\n",
      "iterations:  933\n",
      "Training loss: 0.011529998691561261, Validation loss: 0.02279138051502794\n",
      "new best w\n",
      "iterations:  934\n",
      "Training loss: 0.01151744971707509, Validation loss: 0.022797858713937542\n",
      "iterations:  935\n",
      "Training loss: 0.011505088259495233, Validation loss: 0.022758987150838926\n",
      "new best w\n",
      "iterations:  936\n",
      "Training loss: 0.011493222249997158, Validation loss: 0.022738253132945398\n",
      "new best w\n",
      "iterations:  937\n",
      "Training loss: 0.011480343030010437, Validation loss: 0.022750487334485144\n",
      "iterations:  938\n",
      "Training loss: 0.011468803402642488, Validation loss: 0.02270429033584449\n",
      "new best w\n",
      "iterations:  939\n",
      "Training loss: 0.011456354405142598, Validation loss: 0.022713906487261896\n",
      "iterations:  940\n",
      "Training loss: 0.011443771752195494, Validation loss: 0.02267553431914173\n",
      "new best w\n",
      "iterations:  941\n",
      "Training loss: 0.011432298816979157, Validation loss: 0.02265744168855841\n",
      "new best w\n",
      "iterations:  942\n",
      "Training loss: 0.011419284381442803, Validation loss: 0.02263000373653428\n",
      "new best w\n",
      "iterations:  943\n",
      "Training loss: 0.011407863956057593, Validation loss: 0.022598718147902604\n",
      "new best w\n",
      "iterations:  944\n",
      "Training loss: 0.011395050103629518, Validation loss: 0.022605136774824588\n",
      "iterations:  945\n",
      "Training loss: 0.011382705664560249, Validation loss: 0.02257122949332274\n",
      "new best w\n",
      "iterations:  946\n",
      "Training loss: 0.01137143938184968, Validation loss: 0.022561262048216773\n",
      "new best w\n",
      "iterations:  947\n",
      "Training loss: 0.011358303366736029, Validation loss: 0.02254636821480207\n",
      "new best w\n",
      "iterations:  948\n",
      "Training loss: 0.011347007330594395, Validation loss: 0.02251590321377201\n",
      "new best w\n",
      "iterations:  949\n",
      "Training loss: 0.011334145402539342, Validation loss: 0.022482597582039743\n",
      "new best w\n",
      "iterations:  950\n",
      "Training loss: 0.011321974486493976, Validation loss: 0.022470073669261376\n",
      "new best w\n",
      "iterations:  951\n",
      "Training loss: 0.011310412259844048, Validation loss: 0.02246661665649678\n",
      "new best w\n",
      "iterations:  952\n",
      "Training loss: 0.011297723140791662, Validation loss: 0.02241820013630166\n",
      "new best w\n",
      "iterations:  953\n",
      "Training loss: 0.011286230730603516, Validation loss: 0.022446201997915475\n",
      "iterations:  954\n",
      "Training loss: 0.011273679748299745, Validation loss: 0.022378926313473595\n",
      "new best w\n",
      "iterations:  955\n",
      "Training loss: 0.011261704195415397, Validation loss: 0.022405634387390023\n",
      "iterations:  956\n",
      "Training loss: 0.011249752472627262, Validation loss: 0.022340971926285688\n",
      "new best w\n",
      "iterations:  957\n",
      "Training loss: 0.01123719193725097, Validation loss: 0.022333193319185866\n",
      "new best w\n",
      "iterations:  958\n",
      "Training loss: 0.011225225865109219, Validation loss: 0.022331910125776698\n",
      "new best w\n",
      "iterations:  959\n",
      "Training loss: 0.011213415859889114, Validation loss: 0.022274883969844925\n",
      "new best w\n",
      "iterations:  960\n",
      "Training loss: 0.011201083688278029, Validation loss: 0.02230798131102506\n",
      "iterations:  961\n",
      "Training loss: 0.011189180924663436, Validation loss: 0.022245027793038486\n",
      "new best w\n",
      "iterations:  962\n",
      "Training loss: 0.011177201980869491, Validation loss: 0.02223722665465032\n",
      "new best w\n",
      "iterations:  963\n",
      "Training loss: 0.011164629340696043, Validation loss: 0.022223708240825518\n",
      "new best w\n",
      "iterations:  964\n",
      "Training loss: 0.011153195353921565, Validation loss: 0.02218845002396507\n",
      "new best w\n",
      "iterations:  965\n",
      "Training loss: 0.01114111538052288, Validation loss: 0.02223054420778694\n",
      "iterations:  966\n",
      "Training loss: 0.01112903967202364, Validation loss: 0.02213629497021255\n",
      "new best w\n",
      "iterations:  967\n",
      "Training loss: 0.011116925218435285, Validation loss: 0.02214824341211333\n",
      "iterations:  968\n",
      "Training loss: 0.011104294622317393, Validation loss: 0.0221115643854485\n",
      "new best w\n",
      "iterations:  969\n",
      "Training loss: 0.011093448768869243, Validation loss: 0.02210095441448037\n",
      "new best w\n",
      "iterations:  970\n",
      "Training loss: 0.01108040675804326, Validation loss: 0.02208418841554198\n",
      "new best w\n",
      "iterations:  971\n",
      "Training loss: 0.011068813902023623, Validation loss: 0.022054518568914334\n",
      "new best w\n",
      "iterations:  972\n",
      "Training loss: 0.01105709983767129, Validation loss: 0.02206908619293674\n",
      "iterations:  973\n",
      "Training loss: 0.011044447043463874, Validation loss: 0.022011955606127603\n",
      "new best w\n",
      "iterations:  974\n",
      "Training loss: 0.011033415557707777, Validation loss: 0.022005785050435316\n",
      "new best w\n",
      "iterations:  975\n",
      "Training loss: 0.011020461809809892, Validation loss: 0.02197878318768296\n",
      "new best w\n",
      "iterations:  976\n",
      "Training loss: 0.0110091102260734, Validation loss: 0.021952913642896137\n",
      "new best w\n",
      "iterations:  977\n",
      "Training loss: 0.010997070570986317, Validation loss: 0.02195196925449695\n",
      "new best w\n",
      "iterations:  978\n",
      "Training loss: 0.010984700181428496, Validation loss: 0.02192470232436971\n",
      "new best w\n",
      "iterations:  979\n",
      "Training loss: 0.010973892647978899, Validation loss: 0.02194431391246865\n",
      "iterations:  980\n",
      "Training loss: 0.010961000342059631, Validation loss: 0.02186933757387958\n",
      "new best w\n",
      "iterations:  981\n",
      "Training loss: 0.010949313005179763, Validation loss: 0.02187498029444848\n",
      "iterations:  982\n",
      "Training loss: 0.01093747101614815, Validation loss: 0.021829371558463907\n",
      "new best w\n",
      "iterations:  983\n",
      "Training loss: 0.010925539995708205, Validation loss: 0.021825875665238196\n",
      "new best w\n",
      "iterations:  984\n",
      "Training loss: 0.010913730635038444, Validation loss: 0.021814232125107195\n",
      "new best w\n",
      "iterations:  985\n",
      "Training loss: 0.01090154491663984, Validation loss: 0.021779594676428322\n",
      "new best w\n",
      "iterations:  986\n",
      "Training loss: 0.010889865320670539, Validation loss: 0.021804708980193592\n",
      "iterations:  987\n",
      "Training loss: 0.010878347293367547, Validation loss: 0.02172599501140581\n",
      "new best w\n",
      "iterations:  988\n",
      "Training loss: 0.010866307555140686, Validation loss: 0.02174456369035987\n",
      "iterations:  989\n",
      "Training loss: 0.010854330364738159, Validation loss: 0.021688778352111596\n",
      "new best w\n",
      "iterations:  990\n",
      "Training loss: 0.010842503634790966, Validation loss: 0.021681432339919166\n",
      "new best w\n",
      "iterations:  991\n",
      "Training loss: 0.010830325479166191, Validation loss: 0.021686961118216934\n",
      "iterations:  992\n",
      "Training loss: 0.010819203102921456, Validation loss: 0.021666169906267355\n",
      "new best w\n",
      "iterations:  993\n",
      "Training loss: 0.010806936110140553, Validation loss: 0.021625619268059833\n",
      "new best w\n",
      "iterations:  994\n",
      "Training loss: 0.010795243420547546, Validation loss: 0.021617220269446687\n",
      "new best w\n",
      "iterations:  995\n",
      "Training loss: 0.010783684166250886, Validation loss: 0.021594379406891552\n",
      "new best w\n",
      "iterations:  996\n",
      "Training loss: 0.010771157556910993, Validation loss: 0.021564679840876073\n",
      "new best w\n",
      "iterations:  997\n",
      "Training loss: 0.010760435344160657, Validation loss: 0.021559281364719567\n",
      "new best w\n",
      "iterations:  998\n",
      "Training loss: 0.010747902579906649, Validation loss: 0.02154858823105379\n",
      "new best w\n",
      "iterations:  999\n",
      "Training loss: 0.010736471939134637, Validation loss: 0.02151872743838091\n",
      "new best w\n",
      "iterations:  1000\n",
      "Training loss: 0.010724827606709881, Validation loss: 0.021484011148675233\n",
      "new best w\n",
      "iterations:  1001\n",
      "Training loss: 0.010712458142565435, Validation loss: 0.02148330955869375\n",
      "new best w\n",
      "iterations:  1002\n",
      "Training loss: 0.010701934256277293, Validation loss: 0.021487839393035064\n",
      "iterations:  1003\n",
      "Training loss: 0.010689117974597756, Validation loss: 0.021426555464958592\n",
      "new best w\n",
      "iterations:  1004\n",
      "Training loss: 0.010678129320402099, Validation loss: 0.02141099017092793\n",
      "new best w\n",
      "iterations:  1005\n",
      "Training loss: 0.01066604334926418, Validation loss: 0.021411121421912045\n",
      "iterations:  1006\n",
      "Training loss: 0.010653998805235156, Validation loss: 0.021377908305022154\n",
      "new best w\n",
      "iterations:  1007\n",
      "Training loss: 0.010643193204420498, Validation loss: 0.021369996399874837\n",
      "new best w\n",
      "iterations:  1008\n",
      "Training loss: 0.010630714850495628, Validation loss: 0.0213530928774121\n",
      "new best w\n",
      "iterations:  1009\n",
      "Training loss: 0.010619706803563974, Validation loss: 0.02132499513583842\n",
      "new best w\n",
      "iterations:  1010\n",
      "Training loss: 0.010607657570272615, Validation loss: 0.0212895354797562\n",
      "new best w\n",
      "iterations:  1011\n",
      "Training loss: 0.010596032535572635, Validation loss: 0.02130249435120588\n",
      "iterations:  1012\n",
      "Training loss: 0.010584747766459108, Validation loss: 0.0212771796941756\n",
      "new best w\n",
      "iterations:  1013\n",
      "Training loss: 0.010572457605443253, Validation loss: 0.02124160788550199\n",
      "new best w\n",
      "iterations:  1014\n",
      "Training loss: 0.01056149383510441, Validation loss: 0.0212200255006538\n",
      "new best w\n",
      "iterations:  1015\n",
      "Training loss: 0.010549627534819042, Validation loss: 0.02121759045385944\n",
      "new best w\n",
      "iterations:  1016\n",
      "Training loss: 0.010538040824114234, Validation loss: 0.021210222155861286\n",
      "new best w\n",
      "iterations:  1017\n",
      "Training loss: 0.01052704311913246, Validation loss: 0.021147366932084043\n",
      "new best w\n",
      "iterations:  1018\n",
      "Training loss: 0.010515129618211551, Validation loss: 0.02117798205372796\n",
      "iterations:  1019\n",
      "Training loss: 0.01050352965152066, Validation loss: 0.02113504241410875\n",
      "new best w\n",
      "iterations:  1020\n",
      "Training loss: 0.010491479638432387, Validation loss: 0.021109908630421788\n",
      "new best w\n",
      "iterations:  1021\n",
      "Training loss: 0.010480005479002671, Validation loss: 0.02110766309825223\n",
      "new best w\n",
      "iterations:  1022\n",
      "Training loss: 0.010468787207169062, Validation loss: 0.021068674019731243\n",
      "new best w\n",
      "iterations:  1023\n",
      "Training loss: 0.01045719012158585, Validation loss: 0.02107567187667952\n",
      "iterations:  1024\n",
      "Training loss: 0.010445685668055505, Validation loss: 0.021013450622484314\n",
      "new best w\n",
      "iterations:  1025\n",
      "Training loss: 0.01043424781713018, Validation loss: 0.02103140457189868\n",
      "iterations:  1026\n",
      "Training loss: 0.010422166072489785, Validation loss: 0.021007918271796606\n",
      "new best w\n",
      "iterations:  1027\n",
      "Training loss: 0.010411317824188577, Validation loss: 0.02096763340255179\n",
      "new best w\n",
      "iterations:  1028\n",
      "Training loss: 0.010399679281824456, Validation loss: 0.020985089270517128\n",
      "iterations:  1029\n",
      "Training loss: 0.010388348650981791, Validation loss: 0.02092572834347546\n",
      "new best w\n",
      "iterations:  1030\n",
      "Training loss: 0.010376679832411766, Validation loss: 0.020932738017451963\n",
      "iterations:  1031\n",
      "Training loss: 0.010364805148526487, Validation loss: 0.02088498965259785\n",
      "new best w\n",
      "iterations:  1032\n",
      "Training loss: 0.010353892015881154, Validation loss: 0.020891212406258807\n",
      "iterations:  1033\n",
      "Training loss: 0.010341447713197885, Validation loss: 0.020876220730122873\n",
      "new best w\n",
      "iterations:  1034\n",
      "Training loss: 0.010330493132022903, Validation loss: 0.020834357283445033\n",
      "new best w\n",
      "iterations:  1035\n",
      "Training loss: 0.010318710109161766, Validation loss: 0.02082691928622818\n",
      "new best w\n",
      "iterations:  1036\n",
      "Training loss: 0.010306890383862092, Validation loss: 0.020809445755259066\n",
      "new best w\n",
      "iterations:  1037\n",
      "Training loss: 0.010296036353984404, Validation loss: 0.02078418452606365\n",
      "new best w\n",
      "iterations:  1038\n",
      "Training loss: 0.010283700447729536, Validation loss: 0.020778620788773815\n",
      "new best w\n",
      "iterations:  1039\n",
      "Training loss: 0.010273209745008137, Validation loss: 0.020731126934628676\n",
      "new best w\n",
      "iterations:  1040\n",
      "Training loss: 0.010260930602783287, Validation loss: 0.020742259457822078\n",
      "iterations:  1041\n",
      "Training loss: 0.010249582307821019, Validation loss: 0.02070860081961028\n",
      "new best w\n",
      "iterations:  1042\n",
      "Training loss: 0.01023830003249012, Validation loss: 0.02068183763289868\n",
      "new best w\n",
      "iterations:  1043\n",
      "Training loss: 0.01022640825350427, Validation loss: 0.020676296296399714\n",
      "new best w\n",
      "iterations:  1044\n",
      "Training loss: 0.010216084095292937, Validation loss: 0.020668192389818797\n",
      "new best w\n",
      "iterations:  1045\n",
      "Training loss: 0.010203438469088605, Validation loss: 0.020616152264338414\n",
      "new best w\n",
      "iterations:  1046\n",
      "Training loss: 0.010192646190746373, Validation loss: 0.020615510833412235\n",
      "new best w\n",
      "iterations:  1047\n",
      "Training loss: 0.0101808825369671, Validation loss: 0.020606475361406744\n",
      "new best w\n",
      "iterations:  1048\n",
      "Training loss: 0.010169302419242915, Validation loss: 0.020565243619415696\n",
      "new best w\n",
      "iterations:  1049\n",
      "Training loss: 0.01015862205135695, Validation loss: 0.020560948382302307\n",
      "new best w\n",
      "iterations:  1050\n",
      "Training loss: 0.010146317178100153, Validation loss: 0.02053891141272644\n",
      "new best w\n",
      "iterations:  1051\n",
      "Training loss: 0.010135842234720376, Validation loss: 0.020526353983322844\n",
      "new best w\n",
      "iterations:  1052\n",
      "Training loss: 0.010123868600317769, Validation loss: 0.02048172757060831\n",
      "new best w\n",
      "iterations:  1053\n",
      "Training loss: 0.010112626456007762, Validation loss: 0.02049174668416493\n",
      "iterations:  1054\n",
      "Training loss: 0.01010155146461192, Validation loss: 0.020460914805563838\n",
      "new best w\n",
      "iterations:  1055\n",
      "Training loss: 0.010089996573401299, Validation loss: 0.0204438043682467\n",
      "new best w\n",
      "iterations:  1056\n",
      "Training loss: 0.010079670529435423, Validation loss: 0.02041386420639609\n",
      "new best w\n",
      "iterations:  1057\n",
      "Training loss: 0.010067091089203579, Validation loss: 0.02041002739405078\n",
      "new best w\n",
      "iterations:  1058\n",
      "Training loss: 0.010056191789853014, Validation loss: 0.0204002221653406\n",
      "new best w\n",
      "iterations:  1059\n",
      "Training loss: 0.010044768855729629, Validation loss: 0.020341613526758402\n",
      "new best w\n",
      "iterations:  1060\n",
      "Training loss: 0.010033075265141331, Validation loss: 0.020351141574875727\n",
      "iterations:  1061\n",
      "Training loss: 0.010022374480235822, Validation loss: 0.020336063828491742\n",
      "new best w\n",
      "iterations:  1062\n",
      "Training loss: 0.010010635790492006, Validation loss: 0.020312466779470083\n",
      "new best w\n",
      "iterations:  1063\n",
      "Training loss: 0.009999622881113166, Validation loss: 0.02027601823840177\n",
      "new best w\n",
      "iterations:  1064\n",
      "Training loss: 0.009988207068635709, Validation loss: 0.020275583678047902\n",
      "new best w\n",
      "iterations:  1065\n",
      "Training loss: 0.009976981485048931, Validation loss: 0.02024082412856197\n",
      "new best w\n",
      "iterations:  1066\n",
      "Training loss: 0.009966202758919402, Validation loss: 0.020240410021111804\n",
      "new best w\n",
      "iterations:  1067\n",
      "Training loss: 0.009954295620814197, Validation loss: 0.020196870945957908\n",
      "new best w\n",
      "iterations:  1068\n",
      "Training loss: 0.009943577300253015, Validation loss: 0.02021235013414169\n",
      "iterations:  1069\n",
      "Training loss: 0.009932205110045542, Validation loss: 0.020146904361857072\n",
      "new best w\n",
      "iterations:  1070\n",
      "Training loss: 0.009920933094986135, Validation loss: 0.02017517552174749\n",
      "iterations:  1071\n",
      "Training loss: 0.009909915289703742, Validation loss: 0.020108073167620203\n",
      "new best w\n",
      "iterations:  1072\n",
      "Training loss: 0.009899075356275336, Validation loss: 0.020138472825072286\n",
      "iterations:  1073\n",
      "Training loss: 0.009887837155349032, Validation loss: 0.020079668589279367\n",
      "new best w\n",
      "iterations:  1074\n",
      "Training loss: 0.009876091434200956, Validation loss: 0.020089884743335\n",
      "iterations:  1075\n",
      "Training loss: 0.009864922807989866, Validation loss: 0.020059797336906573\n",
      "new best w\n",
      "iterations:  1076\n",
      "Training loss: 0.009854037195303554, Validation loss: 0.020031219570745485\n",
      "new best w\n",
      "iterations:  1077\n",
      "Training loss: 0.009842605430860553, Validation loss: 0.02001925624361124\n",
      "new best w\n",
      "iterations:  1078\n",
      "Training loss: 0.009831901091639717, Validation loss: 0.019994809288345357\n",
      "new best w\n",
      "iterations:  1079\n",
      "Training loss: 0.00982064765942287, Validation loss: 0.019968437697115364\n",
      "new best w\n",
      "iterations:  1080\n",
      "Training loss: 0.009809421986707957, Validation loss: 0.01998159255083141\n",
      "iterations:  1081\n",
      "Training loss: 0.009798499495935954, Validation loss: 0.019925774111614634\n",
      "new best w\n",
      "iterations:  1082\n",
      "Training loss: 0.009786986139462514, Validation loss: 0.019928445532061807\n",
      "iterations:  1083\n",
      "Training loss: 0.00977640050464158, Validation loss: 0.019912450954012537\n",
      "new best w\n",
      "iterations:  1084\n",
      "Training loss: 0.009765134785748454, Validation loss: 0.019884336393231096\n",
      "new best w\n",
      "iterations:  1085\n",
      "Training loss: 0.009754069951775433, Validation loss: 0.019866709080212618\n",
      "new best w\n",
      "iterations:  1086\n",
      "Training loss: 0.009743407686809692, Validation loss: 0.019820271913909924\n",
      "new best w\n",
      "iterations:  1087\n",
      "Training loss: 0.009732103572842519, Validation loss: 0.019864393111665716\n",
      "iterations:  1088\n",
      "Training loss: 0.009721881818971784, Validation loss: 0.019798956117244873\n",
      "new best w\n",
      "iterations:  1089\n",
      "Training loss: 0.009710454228790095, Validation loss: 0.01979070463685676\n",
      "new best w\n",
      "iterations:  1090\n",
      "Training loss: 0.009699586224730269, Validation loss: 0.01976719470527307\n",
      "new best w\n",
      "iterations:  1091\n",
      "Training loss: 0.009688159213703059, Validation loss: 0.01977633110588161\n",
      "iterations:  1092\n",
      "Training loss: 0.009676668564466293, Validation loss: 0.01970269854348326\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1093\n",
      "Training loss: 0.009666197780185988, Validation loss: 0.01971925262227556\n",
      "iterations:  1094\n",
      "Training loss: 0.009654641394006878, Validation loss: 0.01969793268073214\n",
      "new best w\n",
      "iterations:  1095\n",
      "Training loss: 0.009644350679124981, Validation loss: 0.01968523861837127\n",
      "new best w\n",
      "iterations:  1096\n",
      "Training loss: 0.009633039905168674, Validation loss: 0.019650581726110415\n",
      "new best w\n",
      "iterations:  1097\n",
      "Training loss: 0.009622050276953759, Validation loss: 0.019655110836522525\n",
      "iterations:  1098\n",
      "Training loss: 0.009611812497350835, Validation loss: 0.019605899643357936\n",
      "new best w\n",
      "iterations:  1099\n",
      "Training loss: 0.009599983688315805, Validation loss: 0.019603089964139536\n",
      "new best w\n",
      "iterations:  1100\n",
      "Training loss: 0.009589914678857758, Validation loss: 0.01957565156153049\n",
      "new best w\n",
      "iterations:  1101\n",
      "Training loss: 0.009578402874369707, Validation loss: 0.019571707028442204\n",
      "new best w\n",
      "iterations:  1102\n",
      "Training loss: 0.009567519620388402, Validation loss: 0.019532029129249402\n",
      "new best w\n",
      "iterations:  1103\n",
      "Training loss: 0.009556903049586672, Validation loss: 0.01952819319608757\n",
      "new best w\n",
      "iterations:  1104\n",
      "Training loss: 0.009545441970775845, Validation loss: 0.01948284479574034\n",
      "new best w\n",
      "iterations:  1105\n",
      "Training loss: 0.009535376606712522, Validation loss: 0.019488273251666164\n",
      "iterations:  1106\n",
      "Training loss: 0.009523425342887552, Validation loss: 0.01947434487115157\n",
      "new best w\n",
      "iterations:  1107\n",
      "Training loss: 0.009512774029065129, Validation loss: 0.01942435957730152\n",
      "new best w\n",
      "iterations:  1108\n",
      "Training loss: 0.009501810728292726, Validation loss: 0.019444829068350815\n",
      "iterations:  1109\n",
      "Training loss: 0.009490244186114393, Validation loss: 0.01939341999758988\n",
      "new best w\n",
      "iterations:  1110\n",
      "Training loss: 0.009480067301330443, Validation loss: 0.019391410469264465\n",
      "new best w\n",
      "iterations:  1111\n",
      "Training loss: 0.009468259270320786, Validation loss: 0.019351894051196243\n",
      "new best w\n",
      "iterations:  1112\n",
      "Training loss: 0.009458109549212496, Validation loss: 0.019342364383556215\n",
      "new best w\n",
      "iterations:  1113\n",
      "Training loss: 0.009446437754261511, Validation loss: 0.019342979294457382\n",
      "iterations:  1114\n",
      "Training loss: 0.009435538818878713, Validation loss: 0.019296247428683792\n",
      "new best w\n",
      "iterations:  1115\n",
      "Training loss: 0.009424951587252487, Validation loss: 0.019283093856882837\n",
      "new best w\n",
      "iterations:  1116\n",
      "Training loss: 0.009413259993668283, Validation loss: 0.019283014234741887\n",
      "new best w\n",
      "iterations:  1117\n",
      "Training loss: 0.009403401268366652, Validation loss: 0.01923930024893831\n",
      "new best w\n",
      "iterations:  1118\n",
      "Training loss: 0.009391603340476774, Validation loss: 0.01922047672674792\n",
      "new best w\n",
      "iterations:  1119\n",
      "Training loss: 0.009380930048407336, Validation loss: 0.01920787456222059\n",
      "new best w\n",
      "iterations:  1120\n",
      "Training loss: 0.009370148237277479, Validation loss: 0.01920768397263201\n",
      "new best w\n",
      "iterations:  1121\n",
      "Training loss: 0.009358962664818185, Validation loss: 0.019159036048260872\n",
      "new best w\n",
      "iterations:  1122\n",
      "Training loss: 0.009348881722056002, Validation loss: 0.019162643234469028\n",
      "iterations:  1123\n",
      "Training loss: 0.009337122435855761, Validation loss: 0.019109269720558795\n",
      "new best w\n",
      "iterations:  1124\n",
      "Training loss: 0.009326794023691725, Validation loss: 0.01910414962214668\n",
      "new best w\n",
      "iterations:  1125\n",
      "Training loss: 0.009315562893158108, Validation loss: 0.019110642714075896\n",
      "iterations:  1126\n",
      "Training loss: 0.009304563899602231, Validation loss: 0.01905764504774564\n",
      "new best w\n",
      "iterations:  1127\n",
      "Training loss: 0.009294417617507506, Validation loss: 0.01906920676269267\n",
      "iterations:  1128\n",
      "Training loss: 0.00928283881827947, Validation loss: 0.01902507640805597\n",
      "new best w\n",
      "iterations:  1129\n",
      "Training loss: 0.009272767052859588, Validation loss: 0.019022022728820028\n",
      "new best w\n",
      "iterations:  1130\n",
      "Training loss: 0.009261351756591962, Validation loss: 0.018984678270756505\n",
      "new best w\n",
      "iterations:  1131\n",
      "Training loss: 0.009250456978932153, Validation loss: 0.01897863096464231\n",
      "new best w\n",
      "iterations:  1132\n",
      "Training loss: 0.009240225126867566, Validation loss: 0.018973509534642975\n",
      "new best w\n",
      "iterations:  1133\n",
      "Training loss: 0.009228951206023614, Validation loss: 0.018909178843640097\n",
      "new best w\n",
      "iterations:  1134\n",
      "Training loss: 0.009218850459788147, Validation loss: 0.01893076243231559\n",
      "iterations:  1135\n",
      "Training loss: 0.009207340970587898, Validation loss: 0.01888928833753923\n",
      "new best w\n",
      "iterations:  1136\n",
      "Training loss: 0.009197029737430086, Validation loss: 0.018880757916496287\n",
      "new best w\n",
      "iterations:  1137\n",
      "Training loss: 0.009186495371282638, Validation loss: 0.01886843945722884\n",
      "new best w\n",
      "iterations:  1138\n",
      "Training loss: 0.00917530449811941, Validation loss: 0.01884492076115146\n",
      "new best w\n",
      "iterations:  1139\n",
      "Training loss: 0.00916516134446461, Validation loss: 0.018818157826899974\n",
      "new best w\n",
      "iterations:  1140\n",
      "Training loss: 0.009153828896699181, Validation loss: 0.01879774602507328\n",
      "new best w\n",
      "iterations:  1141\n",
      "Training loss: 0.009143539202582603, Validation loss: 0.018790146053440957\n",
      "new best w\n",
      "iterations:  1142\n",
      "Training loss: 0.009132619319864841, Validation loss: 0.01875288495369255\n",
      "new best w\n",
      "iterations:  1143\n",
      "Training loss: 0.00912149154431637, Validation loss: 0.018746192706743846\n",
      "new best w\n",
      "iterations:  1144\n",
      "Training loss: 0.009111417791471031, Validation loss: 0.01874061381748833\n",
      "new best w\n",
      "iterations:  1145\n",
      "Training loss: 0.00910015010600422, Validation loss: 0.01869229509094297\n",
      "new best w\n",
      "iterations:  1146\n",
      "Training loss: 0.009089796842158097, Validation loss: 0.018692850623987976\n",
      "iterations:  1147\n",
      "Training loss: 0.009078812487726339, Validation loss: 0.018649232371291\n",
      "new best w\n",
      "iterations:  1148\n",
      "Training loss: 0.009068012037624468, Validation loss: 0.01865212357308376\n",
      "iterations:  1149\n",
      "Training loss: 0.009057675578015371, Validation loss: 0.01862805564170994\n",
      "new best w\n",
      "iterations:  1150\n",
      "Training loss: 0.009046556914782405, Validation loss: 0.0186039059095322\n",
      "new best w\n",
      "iterations:  1151\n",
      "Training loss: 0.009036340421153883, Validation loss: 0.01859028448917206\n",
      "new best w\n",
      "iterations:  1152\n",
      "Training loss: 0.009025518908878719, Validation loss: 0.018556893919723837\n",
      "new best w\n",
      "iterations:  1153\n",
      "Training loss: 0.009015054124263046, Validation loss: 0.01856304996598399\n",
      "iterations:  1154\n",
      "Training loss: 0.009005048944839729, Validation loss: 0.01852045941116594\n",
      "new best w\n",
      "iterations:  1155\n",
      "Training loss: 0.008994211003036552, Validation loss: 0.01853179024589112\n",
      "iterations:  1156\n",
      "Training loss: 0.008984581116530109, Validation loss: 0.018471038673863786\n",
      "new best w\n",
      "iterations:  1157\n",
      "Training loss: 0.008973352420202656, Validation loss: 0.018499774514581756\n",
      "iterations:  1158\n",
      "Training loss: 0.008962840451752383, Validation loss: 0.01845485150750121\n",
      "new best w\n",
      "iterations:  1159\n",
      "Training loss: 0.008952641942743318, Validation loss: 0.018431692744882093\n",
      "new best w\n",
      "iterations:  1160\n",
      "Training loss: 0.008941829222604645, Validation loss: 0.018411558563865137\n",
      "new best w\n",
      "iterations:  1161\n",
      "Training loss: 0.008932098151199525, Validation loss: 0.018424254211912787\n",
      "iterations:  1162\n",
      "Training loss: 0.008921467838682884, Validation loss: 0.018361232027919012\n",
      "new best w\n",
      "iterations:  1163\n",
      "Training loss: 0.0089112176718543, Validation loss: 0.018376501883935126\n",
      "iterations:  1164\n",
      "Training loss: 0.008901361881437154, Validation loss: 0.018317005167816085\n",
      "new best w\n",
      "iterations:  1165\n",
      "Training loss: 0.008890791183154119, Validation loss: 0.018337232570833646\n",
      "iterations:  1166\n",
      "Training loss: 0.008880798145189175, Validation loss: 0.01830727212224572\n",
      "new best w\n",
      "iterations:  1167\n",
      "Training loss: 0.008870005156368288, Validation loss: 0.018295035594967406\n",
      "new best w\n",
      "iterations:  1168\n",
      "Training loss: 0.008859454752498181, Validation loss: 0.01825631548527872\n",
      "new best w\n",
      "iterations:  1169\n",
      "Training loss: 0.008849475919894285, Validation loss: 0.018258008385982287\n",
      "iterations:  1170\n",
      "Training loss: 0.008839046820921329, Validation loss: 0.01824241183731482\n",
      "new best w\n",
      "iterations:  1171\n",
      "Training loss: 0.008829204949001091, Validation loss: 0.018194777730840044\n",
      "new best w\n",
      "iterations:  1172\n",
      "Training loss: 0.008818469564817006, Validation loss: 0.018193448159298378\n",
      "new best w\n",
      "iterations:  1173\n",
      "Training loss: 0.008808456684085458, Validation loss: 0.018166292496646867\n",
      "new best w\n",
      "iterations:  1174\n",
      "Training loss: 0.00879848146617107, Validation loss: 0.01815263685913912\n",
      "new best w\n",
      "iterations:  1175\n",
      "Training loss: 0.008788310099065397, Validation loss: 0.018157411527555196\n",
      "iterations:  1176\n",
      "Training loss: 0.008778537054252727, Validation loss: 0.01808976037674446\n",
      "new best w\n",
      "iterations:  1177\n",
      "Training loss: 0.00876789561593577, Validation loss: 0.01812757480471416\n",
      "iterations:  1178\n",
      "Training loss: 0.008757622317423325, Validation loss: 0.018067096755043928\n",
      "new best w\n",
      "iterations:  1179\n",
      "Training loss: 0.008747559423939627, Validation loss: 0.018079019192306374\n",
      "iterations:  1180\n",
      "Training loss: 0.008737133366501998, Validation loss: 0.01803902623752305\n",
      "new best w\n",
      "iterations:  1181\n",
      "Training loss: 0.008727360071702812, Validation loss: 0.018009891500841237\n",
      "new best w\n",
      "iterations:  1182\n",
      "Training loss: 0.008717046797552417, Validation loss: 0.018001150818671832\n",
      "new best w\n",
      "iterations:  1183\n",
      "Training loss: 0.008706938951847465, Validation loss: 0.017999478628487667\n",
      "new best w\n",
      "iterations:  1184\n",
      "Training loss: 0.00869697981869916, Validation loss: 0.017959594963702193\n",
      "new best w\n",
      "iterations:  1185\n",
      "Training loss: 0.008686664240148573, Validation loss: 0.017950293529855783\n",
      "new best w\n",
      "iterations:  1186\n",
      "Training loss: 0.008677126729713287, Validation loss: 0.017948006929885903\n",
      "new best w\n",
      "iterations:  1187\n",
      "Training loss: 0.00866687641719094, Validation loss: 0.01790457026695569\n",
      "new best w\n",
      "iterations:  1188\n",
      "Training loss: 0.00865656887951108, Validation loss: 0.01788972071323833\n",
      "new best w\n",
      "iterations:  1189\n",
      "Training loss: 0.00864657821349456, Validation loss: 0.01787517476776524\n",
      "new best w\n",
      "iterations:  1190\n",
      "Training loss: 0.0086362219275883, Validation loss: 0.01784480547989734\n",
      "new best w\n",
      "iterations:  1191\n",
      "Training loss: 0.008626711794501436, Validation loss: 0.017859058768866613\n",
      "iterations:  1192\n",
      "Training loss: 0.00861659784507255, Validation loss: 0.01780850781660712\n",
      "new best w\n",
      "iterations:  1193\n",
      "Training loss: 0.008606329521908661, Validation loss: 0.017806909647605108\n",
      "new best w\n",
      "iterations:  1194\n",
      "Training loss: 0.008596734828950004, Validation loss: 0.017769547552138082\n",
      "new best w\n",
      "iterations:  1195\n",
      "Training loss: 0.0085863744882037, Validation loss: 0.01778658705047244\n",
      "iterations:  1196\n",
      "Training loss: 0.008577023236581483, Validation loss: 0.017716920457426507\n",
      "new best w\n",
      "iterations:  1197\n",
      "Training loss: 0.008566804009790983, Validation loss: 0.01776081880365774\n",
      "iterations:  1198\n",
      "Training loss: 0.008556497891045657, Validation loss: 0.01768100450613616\n",
      "new best w\n",
      "iterations:  1199\n",
      "Training loss: 0.008546757153161824, Validation loss: 0.017688986933098436\n",
      "iterations:  1200\n",
      "Training loss: 0.008536299180037376, Validation loss: 0.017683150610307685\n",
      "iterations:  1201\n",
      "Training loss: 0.00852703624785493, Validation loss: 0.017651996208059597\n",
      "new best w\n",
      "iterations:  1202\n",
      "Training loss: 0.008516728063839568, Validation loss: 0.017638272741596007\n",
      "new best w\n",
      "iterations:  1203\n",
      "Training loss: 0.008506708928773655, Validation loss: 0.017600540622843067\n",
      "new best w\n",
      "iterations:  1204\n",
      "Training loss: 0.008497304242388643, Validation loss: 0.017596285723421486\n",
      "new best w\n",
      "iterations:  1205\n",
      "Training loss: 0.008486978834150961, Validation loss: 0.017585906443428456\n",
      "new best w\n",
      "iterations:  1206\n",
      "Training loss: 0.008477589166816075, Validation loss: 0.017566230091412015\n",
      "new best w\n",
      "iterations:  1207\n",
      "Training loss: 0.008467376313331531, Validation loss: 0.017539830896157044\n",
      "new best w\n",
      "iterations:  1208\n",
      "Training loss: 0.008457429224546674, Validation loss: 0.017519186401726516\n",
      "new best w\n",
      "iterations:  1209\n",
      "Training loss: 0.00844791071672876, Validation loss: 0.017498141114341895\n",
      "new best w\n",
      "iterations:  1210\n",
      "Training loss: 0.008437637909364089, Validation loss: 0.017502814292557393\n",
      "iterations:  1211\n",
      "Training loss: 0.00842856218743708, Validation loss: 0.017459989448987\n",
      "new best w\n",
      "iterations:  1212\n",
      "Training loss: 0.0084184013919879, Validation loss: 0.017465945602494395\n",
      "iterations:  1213\n",
      "Training loss: 0.008408667328724024, Validation loss: 0.01742849911003991\n",
      "new best w\n",
      "iterations:  1214\n",
      "Training loss: 0.008399435364846372, Validation loss: 0.01742287032800311\n",
      "new best w\n",
      "iterations:  1215\n",
      "Training loss: 0.00838916623986137, Validation loss: 0.017403665574371233\n",
      "new best w\n",
      "iterations:  1216\n",
      "Training loss: 0.008380104745223238, Validation loss: 0.017371833998131667\n",
      "new best w\n",
      "iterations:  1217\n",
      "Training loss: 0.008369995888282866, Validation loss: 0.017377759359093817\n",
      "iterations:  1218\n",
      "Training loss: 0.008360380545636538, Validation loss: 0.0173405737426782\n",
      "new best w\n",
      "iterations:  1219\n",
      "Training loss: 0.008350965587606151, Validation loss: 0.017331377452925945\n",
      "new best w\n",
      "iterations:  1220\n",
      "Training loss: 0.008340893832267661, Validation loss: 0.017309765601269147\n",
      "new best w\n",
      "iterations:  1221\n",
      "Training loss: 0.008331820258815374, Validation loss: 0.017283835922452355\n",
      "new best w\n",
      "iterations:  1222\n",
      "Training loss: 0.00832179284709214, Validation loss: 0.017295625468508914\n",
      "iterations:  1223\n",
      "Training loss: 0.008312246475294431, Validation loss: 0.017245360257143377\n",
      "new best w\n",
      "iterations:  1224\n",
      "Training loss: 0.008302864371698632, Validation loss: 0.01723118387866867\n",
      "new best w\n",
      "iterations:  1225\n",
      "Training loss: 0.00829291733982131, Validation loss: 0.017226917005715627\n",
      "new best w\n",
      "iterations:  1226\n",
      "Training loss: 0.008284058493546164, Validation loss: 0.017187034831258052\n",
      "new best w\n",
      "iterations:  1227\n",
      "Training loss: 0.008273841467617305, Validation loss: 0.017196628054503244\n",
      "iterations:  1228\n",
      "Training loss: 0.008264278144448333, Validation loss: 0.017175531711860988\n",
      "new best w\n",
      "iterations:  1229\n",
      "Training loss: 0.008255066913953387, Validation loss: 0.01713166881888056\n",
      "new best w\n",
      "iterations:  1230\n",
      "Training loss: 0.008245240321532558, Validation loss: 0.017157244680654944\n",
      "iterations:  1231\n",
      "Training loss: 0.008236498588340167, Validation loss: 0.017089059948376107\n",
      "new best w\n",
      "iterations:  1232\n",
      "Training loss: 0.008226189953096886, Validation loss: 0.017116941793062328\n",
      "iterations:  1233\n",
      "Training loss: 0.008216620758676177, Validation loss: 0.017066472092972747\n",
      "new best w\n",
      "iterations:  1234\n",
      "Training loss: 0.008207498068271358, Validation loss: 0.01708426888663573\n",
      "iterations:  1235\n",
      "Training loss: 0.008197689614250478, Validation loss: 0.017025663121519895\n",
      "new best w\n",
      "iterations:  1236\n",
      "Training loss: 0.008188677112384586, Validation loss: 0.017043742890289072\n",
      "iterations:  1237\n",
      "Training loss: 0.00817874568998631, Validation loss: 0.016984640562466007\n",
      "new best w\n",
      "iterations:  1238\n",
      "Training loss: 0.00816945410910907, Validation loss: 0.017003610527713563\n",
      "iterations:  1239\n",
      "Training loss: 0.008160291339664722, Validation loss: 0.016965347867288517\n",
      "new best w\n",
      "iterations:  1240\n",
      "Training loss: 0.008150503325311807, Validation loss: 0.01697997847256829\n",
      "iterations:  1241\n",
      "Training loss: 0.008141547134313558, Validation loss: 0.016911407626084122\n",
      "new best w\n",
      "iterations:  1242\n",
      "Training loss: 0.008131649811549934, Validation loss: 0.016944506224362547\n",
      "iterations:  1243\n",
      "Training loss: 0.008122518697522379, Validation loss: 0.016865150420955207\n",
      "new best w\n",
      "iterations:  1244\n",
      "Training loss: 0.00811287722602288, Validation loss: 0.01691065105130574\n",
      "iterations:  1245\n",
      "Training loss: 0.008103129476335241, Validation loss: 0.01685947888640262\n",
      "new best w\n",
      "iterations:  1246\n",
      "Training loss: 0.008094249294209883, Validation loss: 0.016847667464457254\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1247\n",
      "Training loss: 0.00808456676809371, Validation loss: 0.016846905579555146\n",
      "new best w\n",
      "iterations:  1248\n",
      "Training loss: 0.008075390017884147, Validation loss: 0.016811876501369987\n",
      "new best w\n",
      "iterations:  1249\n",
      "Training loss: 0.008065932282368017, Validation loss: 0.01678927605246433\n",
      "new best w\n",
      "iterations:  1250\n",
      "Training loss: 0.008056316294738027, Validation loss: 0.016792289400003782\n",
      "iterations:  1251\n",
      "Training loss: 0.008047473532205331, Validation loss: 0.016761464753578367\n",
      "new best w\n",
      "iterations:  1252\n",
      "Training loss: 0.008037846942946743, Validation loss: 0.01674930486813709\n",
      "new best w\n",
      "iterations:  1253\n",
      "Training loss: 0.00802856295780434, Validation loss: 0.016714121624682998\n",
      "new best w\n",
      "iterations:  1254\n",
      "Training loss: 0.008019275455416721, Validation loss: 0.01670392317613539\n",
      "new best w\n",
      "iterations:  1255\n",
      "Training loss: 0.008009826702531992, Validation loss: 0.016700221706220958\n",
      "new best w\n",
      "iterations:  1256\n",
      "Training loss: 0.008000920236247419, Validation loss: 0.0166733683898802\n",
      "new best w\n",
      "iterations:  1257\n",
      "Training loss: 0.007991321182126506, Validation loss: 0.016670929917188916\n",
      "new best w\n",
      "iterations:  1258\n",
      "Training loss: 0.007982015339766346, Validation loss: 0.01663094167676181\n",
      "new best w\n",
      "iterations:  1259\n",
      "Training loss: 0.00797297821731711, Validation loss: 0.016612955033778912\n",
      "new best w\n",
      "iterations:  1260\n",
      "Training loss: 0.007963623284957538, Validation loss: 0.016630900932139224\n",
      "iterations:  1261\n",
      "Training loss: 0.007954698059252153, Validation loss: 0.016565065767436056\n",
      "new best w\n",
      "iterations:  1262\n",
      "Training loss: 0.00794502024368956, Validation loss: 0.01658564283804606\n",
      "iterations:  1263\n",
      "Training loss: 0.007935931895887354, Validation loss: 0.016531600008056355\n",
      "new best w\n",
      "iterations:  1264\n",
      "Training loss: 0.007926837391688896, Validation loss: 0.016548591576920277\n",
      "iterations:  1265\n",
      "Training loss: 0.007917322033781413, Validation loss: 0.01651579371771026\n",
      "new best w\n",
      "iterations:  1266\n",
      "Training loss: 0.007908557164937737, Validation loss: 0.016490220418865074\n",
      "new best w\n",
      "iterations:  1267\n",
      "Training loss: 0.007898971674966926, Validation loss: 0.016489967741427664\n",
      "new best w\n",
      "iterations:  1268\n",
      "Training loss: 0.007889917967202193, Validation loss: 0.016474613848331203\n",
      "new best w\n",
      "iterations:  1269\n",
      "Training loss: 0.007880887035113728, Validation loss: 0.016443067195930267\n",
      "new best w\n",
      "iterations:  1270\n",
      "Training loss: 0.00787132552945039, Validation loss: 0.016434279185635847\n",
      "new best w\n",
      "iterations:  1271\n",
      "Training loss: 0.00786271633363338, Validation loss: 0.01639853806705139\n",
      "new best w\n",
      "iterations:  1272\n",
      "Training loss: 0.007853353126629882, Validation loss: 0.01639868253875161\n",
      "iterations:  1273\n",
      "Training loss: 0.007844214809113707, Validation loss: 0.0163834987613065\n",
      "new best w\n",
      "iterations:  1274\n",
      "Training loss: 0.007835141103680648, Validation loss: 0.01636178307552824\n",
      "new best w\n",
      "iterations:  1275\n",
      "Training loss: 0.007825693362901941, Validation loss: 0.016346151951107488\n",
      "new best w\n",
      "iterations:  1276\n",
      "Training loss: 0.007817135916930057, Validation loss: 0.01632741330584965\n",
      "new best w\n",
      "iterations:  1277\n",
      "Training loss: 0.007807712295647349, Validation loss: 0.016316071209788053\n",
      "new best w\n",
      "iterations:  1278\n",
      "Training loss: 0.00779883108657983, Validation loss: 0.01627875364709407\n",
      "new best w\n",
      "iterations:  1279\n",
      "Training loss: 0.0077896173410975265, Validation loss: 0.016279984848965597\n",
      "iterations:  1280\n",
      "Training loss: 0.007780321688493956, Validation loss: 0.01627826358748003\n",
      "new best w\n",
      "iterations:  1281\n",
      "Training loss: 0.007772123914515078, Validation loss: 0.016215711899125764\n",
      "new best w\n",
      "iterations:  1282\n",
      "Training loss: 0.0077625504144030666, Validation loss: 0.01624480434144434\n",
      "iterations:  1283\n",
      "Training loss: 0.0077536597919940905, Validation loss: 0.01618717648755096\n",
      "new best w\n",
      "iterations:  1284\n",
      "Training loss: 0.007744468221235856, Validation loss: 0.01619538958878456\n",
      "iterations:  1285\n",
      "Training loss: 0.0077351179581706675, Validation loss: 0.016189476128575304\n",
      "iterations:  1286\n",
      "Training loss: 0.007726729283844475, Validation loss: 0.01615404518216381\n",
      "new best w\n",
      "iterations:  1287\n",
      "Training loss: 0.007717310494838658, Validation loss: 0.01614350858471418\n",
      "new best w\n",
      "iterations:  1288\n",
      "Training loss: 0.00770859427635742, Validation loss: 0.016117170905637253\n",
      "new best w\n",
      "iterations:  1289\n",
      "Training loss: 0.007699577806432729, Validation loss: 0.016117199351245497\n",
      "iterations:  1290\n",
      "Training loss: 0.007690363815659762, Validation loss: 0.016067096911724157\n",
      "new best w\n",
      "iterations:  1291\n",
      "Training loss: 0.007681791378746361, Validation loss: 0.016072702288255317\n",
      "iterations:  1292\n",
      "Training loss: 0.007672378474678145, Validation loss: 0.016079094547343634\n",
      "iterations:  1293\n",
      "Training loss: 0.007664117246323863, Validation loss: 0.016018362953656516\n",
      "new best w\n",
      "iterations:  1294\n",
      "Training loss: 0.007654658833550027, Validation loss: 0.0160355729316718\n",
      "iterations:  1295\n",
      "Training loss: 0.007645610416040797, Validation loss: 0.0159894847912709\n",
      "new best w\n",
      "iterations:  1296\n",
      "Training loss: 0.0076372577782605365, Validation loss: 0.01598209679674042\n",
      "new best w\n",
      "iterations:  1297\n",
      "Training loss: 0.007627717438087813, Validation loss: 0.01597709059729669\n",
      "new best w\n",
      "iterations:  1298\n",
      "Training loss: 0.007619458681817337, Validation loss: 0.015952105776571638\n",
      "new best w\n",
      "iterations:  1299\n",
      "Training loss: 0.0076100469822415005, Validation loss: 0.015932604912583574\n",
      "new best w\n",
      "iterations:  1300\n",
      "Training loss: 0.007601192047290936, Validation loss: 0.015925161656844978\n",
      "new best w\n",
      "iterations:  1301\n",
      "Training loss: 0.007592918614853944, Validation loss: 0.015889492987672026\n",
      "new best w\n",
      "iterations:  1302\n",
      "Training loss: 0.0075833817323386924, Validation loss: 0.015897086646196264\n",
      "iterations:  1303\n",
      "Training loss: 0.007575057872830297, Validation loss: 0.015864723518943108\n",
      "new best w\n",
      "iterations:  1304\n",
      "Training loss: 0.007565828173578988, Validation loss: 0.01587054703258004\n",
      "iterations:  1305\n",
      "Training loss: 0.007557104511714105, Validation loss: 0.015820182111298695\n",
      "new best w\n",
      "iterations:  1306\n",
      "Training loss: 0.007548542893524074, Validation loss: 0.01582191371250465\n",
      "iterations:  1307\n",
      "Training loss: 0.00753925295282798, Validation loss: 0.015795414602862747\n",
      "new best w\n",
      "iterations:  1308\n",
      "Training loss: 0.007531070190130368, Validation loss: 0.015779055670158332\n",
      "new best w\n",
      "iterations:  1309\n",
      "Training loss: 0.007521729843748505, Validation loss: 0.01578434721233432\n",
      "iterations:  1310\n",
      "Training loss: 0.007513019764651887, Validation loss: 0.015745593024837225\n",
      "new best w\n",
      "iterations:  1311\n",
      "Training loss: 0.007504483980703858, Validation loss: 0.01572172549848892\n",
      "new best w\n",
      "iterations:  1312\n",
      "Training loss: 0.0074953472056481715, Validation loss: 0.01573153114503155\n",
      "iterations:  1313\n",
      "Training loss: 0.007487324112604466, Validation loss: 0.01568726918525034\n",
      "new best w\n",
      "iterations:  1314\n",
      "Training loss: 0.0074778891169950245, Validation loss: 0.01568985485291425\n",
      "iterations:  1315\n",
      "Training loss: 0.007469190364571815, Validation loss: 0.015665709008158477\n",
      "new best w\n",
      "iterations:  1316\n",
      "Training loss: 0.007460812993913493, Validation loss: 0.01565914512591892\n",
      "new best w\n",
      "iterations:  1317\n",
      "Training loss: 0.007451778276323219, Validation loss: 0.015628035977079922\n",
      "new best w\n",
      "iterations:  1318\n",
      "Training loss: 0.007443511900134938, Validation loss: 0.015620772750016882\n",
      "new best w\n",
      "iterations:  1319\n",
      "Training loss: 0.007434314693068519, Validation loss: 0.01558933274710452\n",
      "new best w\n",
      "iterations:  1320\n",
      "Training loss: 0.007425764349752807, Validation loss: 0.015582630638474797\n",
      "new best w\n",
      "iterations:  1321\n",
      "Training loss: 0.007417260628017072, Validation loss: 0.015564979761888708\n",
      "new best w\n",
      "iterations:  1322\n",
      "Training loss: 0.007408274405231473, Validation loss: 0.015561326257827962\n",
      "new best w\n",
      "iterations:  1323\n",
      "Training loss: 0.00739999992090559, Validation loss: 0.015520278258655016\n",
      "new best w\n",
      "iterations:  1324\n",
      "Training loss: 0.00739094532254617, Validation loss: 0.01552727068628886\n",
      "iterations:  1325\n",
      "Training loss: 0.007382542702285655, Validation loss: 0.015491845669242929\n",
      "new best w\n",
      "iterations:  1326\n",
      "Training loss: 0.007373958460463666, Validation loss: 0.01548070420020554\n",
      "new best w\n",
      "iterations:  1327\n",
      "Training loss: 0.007364951416718544, Validation loss: 0.015477510038321271\n",
      "new best w\n",
      "iterations:  1328\n",
      "Training loss: 0.00735685003034444, Validation loss: 0.015435880562512777\n",
      "new best w\n",
      "iterations:  1329\n",
      "Training loss: 0.007347861177708107, Validation loss: 0.015448537631149672\n",
      "iterations:  1330\n",
      "Training loss: 0.007339287625055792, Validation loss: 0.01542196120920171\n",
      "new best w\n",
      "iterations:  1331\n",
      "Training loss: 0.007330947124073713, Validation loss: 0.015382377408496855\n",
      "new best w\n",
      "iterations:  1332\n",
      "Training loss: 0.007322004181485547, Validation loss: 0.015394093469181361\n",
      "iterations:  1333\n",
      "Training loss: 0.007313853923884614, Validation loss: 0.015375786433668446\n",
      "new best w\n",
      "iterations:  1334\n",
      "Training loss: 0.007304916488741565, Validation loss: 0.0153567541873768\n",
      "new best w\n",
      "iterations:  1335\n",
      "Training loss: 0.0072963016831305875, Validation loss: 0.015327642417115525\n",
      "new best w\n",
      "iterations:  1336\n",
      "Training loss: 0.007288091688526488, Validation loss: 0.015319850606108226\n",
      "new best w\n",
      "iterations:  1337\n",
      "Training loss: 0.007279349038853764, Validation loss: 0.015304437282746561\n",
      "new best w\n",
      "iterations:  1338\n",
      "Training loss: 0.007271096831224879, Validation loss: 0.01528367429278025\n",
      "new best w\n",
      "iterations:  1339\n",
      "Training loss: 0.007262146676059213, Validation loss: 0.015275938620323073\n",
      "new best w\n",
      "iterations:  1340\n",
      "Training loss: 0.00725368489357965, Validation loss: 0.01524376860667712\n",
      "new best w\n",
      "iterations:  1341\n",
      "Training loss: 0.007245543272664138, Validation loss: 0.015242270959023774\n",
      "new best w\n",
      "iterations:  1342\n",
      "Training loss: 0.007236649872370189, Validation loss: 0.015235563628398455\n",
      "new best w\n",
      "iterations:  1343\n",
      "Training loss: 0.007228616701651638, Validation loss: 0.015186374498431007\n",
      "new best w\n",
      "iterations:  1344\n",
      "Training loss: 0.007219740438422488, Validation loss: 0.015193662587811196\n",
      "iterations:  1345\n",
      "Training loss: 0.007211213087965213, Validation loss: 0.01518491209625355\n",
      "new best w\n",
      "iterations:  1346\n",
      "Training loss: 0.007203146742334672, Validation loss: 0.015151460275717807\n",
      "new best w\n",
      "iterations:  1347\n",
      "Training loss: 0.007194198291686997, Validation loss: 0.015142456798299269\n",
      "new best w\n",
      "iterations:  1348\n",
      "Training loss: 0.00718628128390126, Validation loss: 0.01512490412604654\n",
      "new best w\n",
      "iterations:  1349\n",
      "Training loss: 0.007177628928636715, Validation loss: 0.01510497916559565\n",
      "new best w\n",
      "iterations:  1350\n",
      "Training loss: 0.007168991873249209, Validation loss: 0.015093758434352424\n",
      "new best w\n",
      "iterations:  1351\n",
      "Training loss: 0.007160908249738375, Validation loss: 0.015071759986056274\n",
      "new best w\n",
      "iterations:  1352\n",
      "Training loss: 0.007152121162144158, Validation loss: 0.015059653205540962\n",
      "new best w\n",
      "iterations:  1353\n",
      "Training loss: 0.007144273663741766, Validation loss: 0.015048296540072609\n",
      "new best w\n",
      "iterations:  1354\n",
      "Training loss: 0.007135456276286194, Validation loss: 0.015037148078934299\n",
      "new best w\n",
      "iterations:  1355\n",
      "Training loss: 0.007127023878351807, Validation loss: 0.014997512401092348\n",
      "new best w\n",
      "iterations:  1356\n",
      "Training loss: 0.007119043489752549, Validation loss: 0.01499059756157819\n",
      "new best w\n",
      "iterations:  1357\n",
      "Training loss: 0.007110200514491497, Validation loss: 0.01500178780568642\n",
      "iterations:  1358\n",
      "Training loss: 0.0071023880876196635, Validation loss: 0.014958457294049885\n",
      "new best w\n",
      "iterations:  1359\n",
      "Training loss: 0.007093556133837112, Validation loss: 0.014945167944498803\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1360\n",
      "Training loss: 0.0070852184543503584, Validation loss: 0.014937121600560871\n",
      "new best w\n",
      "iterations:  1361\n",
      "Training loss: 0.007077469421112231, Validation loss: 0.014902780328097609\n",
      "new best w\n",
      "iterations:  1362\n",
      "Training loss: 0.007068512556155035, Validation loss: 0.014911825160293027\n",
      "iterations:  1363\n",
      "Training loss: 0.007060702378638222, Validation loss: 0.014879768249198847\n",
      "new best w\n",
      "iterations:  1364\n",
      "Training loss: 0.0070520179291846145, Validation loss: 0.01488713873003436\n",
      "iterations:  1365\n",
      "Training loss: 0.007043811052523219, Validation loss: 0.014837374727045632\n",
      "new best w\n",
      "iterations:  1366\n",
      "Training loss: 0.007035788327068466, Validation loss: 0.014841116833417756\n",
      "iterations:  1367\n",
      "Training loss: 0.007027065394364148, Validation loss: 0.014815168241983128\n",
      "new best w\n",
      "iterations:  1368\n",
      "Training loss: 0.0070193915637500656, Validation loss: 0.014800035786441897\n",
      "new best w\n",
      "iterations:  1369\n",
      "Training loss: 0.007010608473773371, Validation loss: 0.014806198943698456\n",
      "iterations:  1370\n",
      "Training loss: 0.00700238312414301, Validation loss: 0.014772449293862677\n",
      "new best w\n",
      "iterations:  1371\n",
      "Training loss: 0.006994446292076232, Validation loss: 0.014744911913470743\n",
      "new best w\n",
      "iterations:  1372\n",
      "Training loss: 0.006985846083607576, Validation loss: 0.01475724139216152\n",
      "iterations:  1373\n",
      "Training loss: 0.0069782426566274616, Validation loss: 0.014731462479894337\n",
      "new best w\n",
      "iterations:  1374\n",
      "Training loss: 0.006969493344722404, Validation loss: 0.014698284169416247\n",
      "new best w\n",
      "iterations:  1375\n",
      "Training loss: 0.00696127828583876, Validation loss: 0.014699216016990988\n",
      "iterations:  1376\n",
      "Training loss: 0.006953390758114908, Validation loss: 0.014686524074342075\n",
      "new best w\n",
      "iterations:  1377\n",
      "Training loss: 0.00694491333433624, Validation loss: 0.014665833095814481\n",
      "new best w\n",
      "iterations:  1378\n",
      "Training loss: 0.0069370908199861405, Validation loss: 0.014644505125359638\n",
      "new best w\n",
      "iterations:  1379\n",
      "Training loss: 0.006928542533174714, Validation loss: 0.014623538487666643\n",
      "new best w\n",
      "iterations:  1380\n",
      "Training loss: 0.0069205242923362454, Validation loss: 0.014615840706909434\n",
      "new best w\n",
      "iterations:  1381\n",
      "Training loss: 0.006912554822725616, Validation loss: 0.0146078242652404\n",
      "new best w\n",
      "iterations:  1382\n",
      "Training loss: 0.006904016187717613, Validation loss: 0.01459533562526109\n",
      "new best w\n",
      "iterations:  1383\n",
      "Training loss: 0.0068963717480980125, Validation loss: 0.01455604563965186\n",
      "new best w\n",
      "iterations:  1384\n",
      "Training loss: 0.006887895581265128, Validation loss: 0.014570030340803572\n",
      "iterations:  1385\n",
      "Training loss: 0.006879781663722827, Validation loss: 0.01454349605980427\n",
      "new best w\n",
      "iterations:  1386\n",
      "Training loss: 0.006871950960705943, Validation loss: 0.014505447709628883\n",
      "new best w\n",
      "iterations:  1387\n",
      "Training loss: 0.006863514998379253, Validation loss: 0.014518663786725092\n",
      "iterations:  1388\n",
      "Training loss: 0.006855823509825951, Validation loss: 0.014499923068247747\n",
      "new best w\n",
      "iterations:  1389\n",
      "Training loss: 0.006847373355606799, Validation loss: 0.014482744665430581\n",
      "new best w\n",
      "iterations:  1390\n",
      "Training loss: 0.006839278208369979, Validation loss: 0.014454156525889897\n",
      "new best w\n",
      "iterations:  1391\n",
      "Training loss: 0.006831505099927514, Validation loss: 0.014447675838007115\n",
      "new best w\n",
      "iterations:  1392\n",
      "Training loss: 0.006823274780833199, Validation loss: 0.014451833615361145\n",
      "iterations:  1393\n",
      "Training loss: 0.006815568300097085, Validation loss: 0.014393430987930092\n",
      "new best w\n",
      "iterations:  1394\n",
      "Training loss: 0.006807056820057255, Validation loss: 0.014411206815411626\n",
      "iterations:  1395\n",
      "Training loss: 0.006799092681222673, Validation loss: 0.014396511636903997\n",
      "iterations:  1396\n",
      "Training loss: 0.006791386229711753, Validation loss: 0.01435943830289962\n",
      "new best w\n",
      "iterations:  1397\n",
      "Training loss: 0.006782935594413273, Validation loss: 0.01436558820700613\n",
      "iterations:  1398\n",
      "Training loss: 0.006775440417247957, Validation loss: 0.01432072005653505\n",
      "new best w\n",
      "iterations:  1399\n",
      "Training loss: 0.006767136458162492, Validation loss: 0.014329059094301743\n",
      "iterations:  1400\n",
      "Training loss: 0.0067591216672910626, Validation loss: 0.01431950468516264\n",
      "new best w\n",
      "iterations:  1401\n",
      "Training loss: 0.006751303349602278, Validation loss: 0.014290301331568927\n",
      "new best w\n",
      "iterations:  1402\n",
      "Training loss: 0.0067430461952795095, Validation loss: 0.01427872244052294\n",
      "new best w\n",
      "iterations:  1403\n",
      "Training loss: 0.0067356055669707685, Validation loss: 0.014268548467164527\n",
      "new best w\n",
      "iterations:  1404\n",
      "Training loss: 0.006727226483701704, Validation loss: 0.014258025188646653\n",
      "new best w\n",
      "iterations:  1405\n",
      "Training loss: 0.00671938048474087, Validation loss: 0.01421881145906477\n",
      "new best w\n",
      "iterations:  1406\n",
      "Training loss: 0.006711627020696229, Validation loss: 0.014215313002693589\n",
      "new best w\n",
      "iterations:  1407\n",
      "Training loss: 0.006703361051538827, Validation loss: 0.014223706693374253\n",
      "iterations:  1408\n",
      "Training loss: 0.0066959034088491486, Validation loss: 0.014182709476997148\n",
      "new best w\n",
      "iterations:  1409\n",
      "Training loss: 0.006687554736321897, Validation loss: 0.01417034541157255\n",
      "new best w\n",
      "iterations:  1410\n",
      "Training loss: 0.006679834733096377, Validation loss: 0.014162471529803424\n",
      "new best w\n",
      "iterations:  1411\n",
      "Training loss: 0.006672147526137731, Validation loss: 0.01414958139228388\n",
      "new best w\n",
      "iterations:  1412\n",
      "Training loss: 0.006663925062213639, Validation loss: 0.014119127470717105\n",
      "new best w\n",
      "iterations:  1413\n",
      "Training loss: 0.006656466350856826, Validation loss: 0.014112707523544469\n",
      "new best w\n",
      "iterations:  1414\n",
      "Training loss: 0.006648177079863308, Validation loss: 0.014114445342192522\n",
      "iterations:  1415\n",
      "Training loss: 0.006640539442335455, Validation loss: 0.014082292190474488\n",
      "new best w\n",
      "iterations:  1416\n",
      "Training loss: 0.006632686699302945, Validation loss: 0.01405731498540511\n",
      "new best w\n",
      "iterations:  1417\n",
      "Training loss: 0.006624586864640039, Validation loss: 0.014067409919655573\n",
      "iterations:  1418\n",
      "Training loss: 0.006617391738049166, Validation loss: 0.01402692939883848\n",
      "new best w\n",
      "iterations:  1419\n",
      "Training loss: 0.006608991239073326, Validation loss: 0.014029196877200206\n",
      "iterations:  1420\n",
      "Training loss: 0.006601421269789588, Validation loss: 0.014008652856588722\n",
      "new best w\n",
      "iterations:  1421\n",
      "Training loss: 0.006593611314498643, Validation loss: 0.014002821087830703\n",
      "new best w\n",
      "iterations:  1422\n",
      "Training loss: 0.006585677961680486, Validation loss: 0.013979106919258816\n",
      "new best w\n",
      "iterations:  1423\n",
      "Training loss: 0.0065783683500330635, Validation loss: 0.01395923596987155\n",
      "new best w\n",
      "iterations:  1424\n",
      "Training loss: 0.0065702654263856556, Validation loss: 0.013969319082522353\n",
      "iterations:  1425\n",
      "Training loss: 0.006562777106950194, Validation loss: 0.013918123392429843\n",
      "new best w\n",
      "iterations:  1426\n",
      "Training loss: 0.006554638836383368, Validation loss: 0.013925127637897198\n",
      "iterations:  1427\n",
      "Training loss: 0.00654674584979543, Validation loss: 0.013909856047574895\n",
      "new best w\n",
      "iterations:  1428\n",
      "Training loss: 0.006539524307680666, Validation loss: 0.0138987922998693\n",
      "new best w\n",
      "iterations:  1429\n",
      "Training loss: 0.006531632321956237, Validation loss: 0.0138751610884424\n",
      "new best w\n",
      "iterations:  1430\n",
      "Training loss: 0.0065242006409258375, Validation loss: 0.013859283245081445\n",
      "new best w\n",
      "iterations:  1431\n",
      "Training loss: 0.006516014347607782, Validation loss: 0.013841770052557285\n",
      "new best w\n",
      "iterations:  1432\n",
      "Training loss: 0.006508297503335466, Validation loss: 0.013830213597556953\n",
      "new best w\n",
      "iterations:  1433\n",
      "Training loss: 0.006500881011786006, Validation loss: 0.013825474364424117\n",
      "new best w\n",
      "iterations:  1434\n",
      "Training loss: 0.0064927702201514215, Validation loss: 0.013812320196299594\n",
      "new best w\n",
      "iterations:  1435\n",
      "Training loss: 0.006485597587359643, Validation loss: 0.01379751704293665\n",
      "new best w\n",
      "iterations:  1436\n",
      "Training loss: 0.006477653963721253, Validation loss: 0.013765007873537701\n",
      "new best w\n",
      "iterations:  1437\n",
      "Training loss: 0.00646988126155699, Validation loss: 0.013768898058988219\n",
      "iterations:  1438\n",
      "Training loss: 0.00646255187280777, Validation loss: 0.013724413079201142\n",
      "new best w\n",
      "iterations:  1439\n",
      "Training loss: 0.006454657241351317, Validation loss: 0.01374116146138676\n",
      "iterations:  1440\n",
      "Training loss: 0.0064473369854717785, Validation loss: 0.013720778778450158\n",
      "new best w\n",
      "iterations:  1441\n",
      "Training loss: 0.006439284315410555, Validation loss: 0.013706603511323218\n",
      "new best w\n",
      "iterations:  1442\n",
      "Training loss: 0.006431790747930603, Validation loss: 0.013699122098040853\n",
      "new best w\n",
      "iterations:  1443\n",
      "Training loss: 0.006424467629852211, Validation loss: 0.01365457000701814\n",
      "new best w\n",
      "iterations:  1444\n",
      "Training loss: 0.006416469382788523, Validation loss: 0.013676621525674077\n",
      "iterations:  1445\n",
      "Training loss: 0.006409315758415501, Validation loss: 0.013622319239771832\n",
      "new best w\n",
      "iterations:  1446\n",
      "Training loss: 0.006401497368548043, Validation loss: 0.013647314251514848\n",
      "iterations:  1447\n",
      "Training loss: 0.006393890906159533, Validation loss: 0.01361055414937506\n",
      "new best w\n",
      "iterations:  1448\n",
      "Training loss: 0.00638634889736794, Validation loss: 0.01359953922694353\n",
      "new best w\n",
      "iterations:  1449\n",
      "Training loss: 0.006378578723552562, Validation loss: 0.013583633013849041\n",
      "new best w\n",
      "iterations:  1450\n",
      "Training loss: 0.006371703645602538, Validation loss: 0.013574774813519597\n",
      "new best w\n",
      "iterations:  1451\n",
      "Training loss: 0.006363666026699069, Validation loss: 0.01357488233207568\n",
      "iterations:  1452\n",
      "Training loss: 0.006356312430756223, Validation loss: 0.013520355941979895\n",
      "new best w\n",
      "iterations:  1453\n",
      "Training loss: 0.006348770692531227, Validation loss: 0.013543589665470455\n",
      "iterations:  1454\n",
      "Training loss: 0.006340946523706224, Validation loss: 0.013515458294976593\n",
      "new best w\n",
      "iterations:  1455\n",
      "Training loss: 0.006333894662260587, Validation loss: 0.013497408105863551\n",
      "new best w\n",
      "iterations:  1456\n",
      "Training loss: 0.006326000387571958, Validation loss: 0.013510664869097386\n",
      "iterations:  1457\n",
      "Training loss: 0.006319010109578692, Validation loss: 0.013466897877893614\n",
      "new best w\n",
      "iterations:  1458\n",
      "Training loss: 0.006311054042411438, Validation loss: 0.013457840273805676\n",
      "new best w\n",
      "iterations:  1459\n",
      "Training loss: 0.006303583216983491, Validation loss: 0.013449477717369365\n",
      "new best w\n",
      "iterations:  1460\n",
      "Training loss: 0.006296586349610849, Validation loss: 0.01343544972239634\n",
      "new best w\n",
      "iterations:  1461\n",
      "Training loss: 0.006288690834317679, Validation loss: 0.013408442035183796\n",
      "new best w\n",
      "iterations:  1462\n",
      "Training loss: 0.006281627043125348, Validation loss: 0.013401212988694561\n",
      "new best w\n",
      "iterations:  1463\n",
      "Training loss: 0.006273828350476935, Validation loss: 0.013404007350478132\n",
      "iterations:  1464\n",
      "Training loss: 0.0062665178692451395, Validation loss: 0.013371950667587512\n",
      "new best w\n",
      "iterations:  1465\n",
      "Training loss: 0.006259164073357989, Validation loss: 0.013348470614374889\n",
      "new best w\n",
      "iterations:  1466\n",
      "Training loss: 0.006251562248295377, Validation loss: 0.013359567250261167\n",
      "iterations:  1467\n",
      "Training loss: 0.006244614957957061, Validation loss: 0.013334766483503981\n",
      "new best w\n",
      "iterations:  1468\n",
      "Training loss: 0.006236809976533054, Validation loss: 0.013304258151795359\n",
      "new best w\n",
      "iterations:  1469\n",
      "Training loss: 0.006229637577954821, Validation loss: 0.013305278811354316\n",
      "iterations:  1470\n",
      "Training loss: 0.0062221803439192255, Validation loss: 0.01329848603140196\n",
      "new best w\n",
      "iterations:  1471\n",
      "Training loss: 0.006214634954576057, Validation loss: 0.013280894455191001\n",
      "new best w\n",
      "iterations:  1472\n",
      "Training loss: 0.00620777009531691, Validation loss: 0.01324598502964142\n",
      "new best w\n",
      "iterations:  1473\n",
      "Training loss: 0.0062000199559368915, Validation loss: 0.013265431172997466\n",
      "iterations:  1474\n",
      "Training loss: 0.006193084985999587, Validation loss: 0.0132359319774143\n",
      "new best w\n",
      "iterations:  1475\n",
      "Training loss: 0.006185392516415788, Validation loss: 0.01321167847163816\n",
      "new best w\n",
      "iterations:  1476\n",
      "Training loss: 0.006178066617545786, Validation loss: 0.01320560560480292\n",
      "new best w\n",
      "iterations:  1477\n",
      "Training loss: 0.006171029835255846, Validation loss: 0.013198059771421439\n",
      "new best w\n",
      "iterations:  1478\n",
      "Training loss: 0.006163316816712652, Validation loss: 0.01318741254976527\n",
      "new best w\n",
      "iterations:  1479\n",
      "Training loss: 0.006156500751565562, Validation loss: 0.013149052700513877\n",
      "new best w\n",
      "iterations:  1480\n",
      "Training loss: 0.006148899623376145, Validation loss: 0.013165326185428806\n",
      "iterations:  1481\n",
      "Training loss: 0.006141603552705031, Validation loss: 0.013137706929423875\n",
      "new best w\n",
      "iterations:  1482\n",
      "Training loss: 0.0061344706379210055, Validation loss: 0.013123354519564492\n",
      "new best w\n",
      "iterations:  1483\n",
      "Training loss: 0.00612708382724009, Validation loss: 0.01310905842844019\n",
      "new best w\n",
      "iterations:  1484\n",
      "Training loss: 0.006120290344922766, Validation loss: 0.013090257637783594\n",
      "new best w\n",
      "iterations:  1485\n",
      "Training loss: 0.006112672450613371, Validation loss: 0.013096030398493339\n",
      "iterations:  1486\n",
      "Training loss: 0.006105552857347839, Validation loss: 0.013070507867558385\n",
      "new best w\n",
      "iterations:  1487\n",
      "Training loss: 0.006098167782987326, Validation loss: 0.013056062795833004\n",
      "new best w\n",
      "iterations:  1488\n",
      "Training loss: 0.006090829962766536, Validation loss: 0.013035942249252488\n",
      "new best w\n",
      "iterations:  1489\n",
      "Training loss: 0.0060840576876156904, Validation loss: 0.013026323181162864\n",
      "new best w\n",
      "iterations:  1490\n",
      "Training loss: 0.006076508132213839, Validation loss: 0.013033716042792515\n",
      "iterations:  1491\n",
      "Training loss: 0.006069710091350991, Validation loss: 0.012978303116783496\n",
      "new best w\n",
      "iterations:  1492\n",
      "Training loss: 0.006062158412840344, Validation loss: 0.012993590263889094\n",
      "iterations:  1493\n",
      "Training loss: 0.006055016984345519, Validation loss: 0.01298075841620237\n",
      "iterations:  1494\n",
      "Training loss: 0.006047932427428248, Validation loss: 0.012955120838158604\n",
      "new best w\n",
      "iterations:  1495\n",
      "Training loss: 0.006040582335557494, Validation loss: 0.012944534424198791\n",
      "new best w\n",
      "iterations:  1496\n",
      "Training loss: 0.006033876025186042, Validation loss: 0.012934766808721455\n",
      "new best w\n",
      "iterations:  1497\n",
      "Training loss: 0.00602626229776867, Validation loss: 0.012926082470077075\n",
      "new best w\n",
      "iterations:  1498\n",
      "Training loss: 0.006019382517260574, Validation loss: 0.012905175201442836\n",
      "new best w\n",
      "iterations:  1499\n",
      "Training loss: 0.006012318001693253, Validation loss: 0.012882311372706637\n",
      "new best w\n",
      "iterations:  1500\n",
      "Training loss: 0.0060049787108400485, Validation loss: 0.0128844957999872\n",
      "iterations:  1501\n",
      "Training loss: 0.005998299855586643, Validation loss: 0.01285724311153695\n",
      "new best w\n",
      "iterations:  1502\n",
      "Training loss: 0.0059908057766604695, Validation loss: 0.012874851261866022\n",
      "iterations:  1503\n",
      "Training loss: 0.005984083401586122, Validation loss: 0.012829432650854325\n",
      "new best w\n",
      "iterations:  1504\n",
      "Training loss: 0.005976622226361664, Validation loss: 0.01282312267131477\n",
      "new best w\n",
      "iterations:  1505\n",
      "Training loss: 0.005969653428768833, Validation loss: 0.012813364751075315\n",
      "new best w\n",
      "iterations:  1506\n",
      "Training loss: 0.005962713950745779, Validation loss: 0.012802552357965097\n",
      "new best w\n",
      "iterations:  1507\n",
      "Training loss: 0.005955414451590548, Validation loss: 0.012776878440294015\n",
      "new best w\n",
      "iterations:  1508\n",
      "Training loss: 0.005948811097720136, Validation loss: 0.01276506576639506\n",
      "new best w\n",
      "iterations:  1509\n",
      "Training loss: 0.00594136170553197, Validation loss: 0.012771574122982651\n",
      "iterations:  1510\n",
      "Training loss: 0.00593453128742031, Validation loss: 0.012739153895634934\n",
      "new best w\n",
      "iterations:  1511\n",
      "Training loss: 0.005927437197356326, Validation loss: 0.012741877996382947\n",
      "iterations:  1512\n",
      "Training loss: 0.0059203497326552525, Validation loss: 0.012721419997689923\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1513\n",
      "Training loss: 0.0059136958826647885, Validation loss: 0.012689369413432742\n",
      "new best w\n",
      "iterations:  1514\n",
      "Training loss: 0.005906358341805596, Validation loss: 0.012708378697462195\n",
      "iterations:  1515\n",
      "Training loss: 0.005899761468169578, Validation loss: 0.012678680723214986\n",
      "new best w\n",
      "iterations:  1516\n",
      "Training loss: 0.005892400406673714, Validation loss: 0.012656072739580493\n",
      "new best w\n",
      "iterations:  1517\n",
      "Training loss: 0.005885575741040545, Validation loss: 0.0126495454433349\n",
      "new best w\n",
      "iterations:  1518\n",
      "Training loss: 0.005878657947744276, Validation loss: 0.012644425391922265\n",
      "new best w\n",
      "iterations:  1519\n",
      "Training loss: 0.005871437293680321, Validation loss: 0.012630967554323358\n",
      "new best w\n",
      "iterations:  1520\n",
      "Training loss: 0.005865076975477834, Validation loss: 0.012616255441854435\n",
      "new best w\n",
      "iterations:  1521\n",
      "Training loss: 0.005857882261282466, Validation loss: 0.012613176711768256\n",
      "new best w\n",
      "iterations:  1522\n",
      "Training loss: 0.005851276166680987, Validation loss: 0.012564336518260607\n",
      "new best w\n",
      "iterations:  1523\n",
      "Training loss: 0.005843912503923458, Validation loss: 0.012589698007254754\n",
      "iterations:  1524\n",
      "Training loss: 0.0058369555251412296, Validation loss: 0.0125626178801172\n",
      "new best w\n",
      "iterations:  1525\n",
      "Training loss: 0.0058302353475623925, Validation loss: 0.012530306115117188\n",
      "new best w\n",
      "iterations:  1526\n",
      "Training loss: 0.0058231273242598, Validation loss: 0.012541436041170185\n",
      "iterations:  1527\n",
      "Training loss: 0.005816588241850011, Validation loss: 0.012524075191482506\n",
      "new best w\n",
      "iterations:  1528\n",
      "Training loss: 0.005809325798652097, Validation loss: 0.012510472979532112\n",
      "new best w\n",
      "iterations:  1529\n",
      "Training loss: 0.005802717650338887, Validation loss: 0.012503050093281388\n",
      "new best w\n",
      "iterations:  1530\n",
      "Training loss: 0.005795776545815452, Validation loss: 0.012479018829737362\n",
      "new best w\n",
      "iterations:  1531\n",
      "Training loss: 0.005788823088524912, Validation loss: 0.012465605023139797\n",
      "new best w\n",
      "iterations:  1532\n",
      "Training loss: 0.005782378013804257, Validation loss: 0.012452743159645384\n",
      "new best w\n",
      "iterations:  1533\n",
      "Training loss: 0.005775195598434368, Validation loss: 0.012462599577813066\n",
      "iterations:  1534\n",
      "Training loss: 0.005768732884799081, Validation loss: 0.012406376849034291\n",
      "new best w\n",
      "iterations:  1535\n",
      "Training loss: 0.005761595449059773, Validation loss: 0.012423927893913079\n",
      "iterations:  1536\n",
      "Training loss: 0.00575484765446582, Validation loss: 0.012409567174747699\n",
      "iterations:  1537\n",
      "Training loss: 0.005748084134589141, Validation loss: 0.012385749435244817\n",
      "new best w\n",
      "iterations:  1538\n",
      "Training loss: 0.00574116283211811, Validation loss: 0.012396590227013414\n",
      "iterations:  1539\n",
      "Training loss: 0.0057348296734369185, Validation loss: 0.012355381767778605\n",
      "new best w\n",
      "iterations:  1540\n",
      "Training loss: 0.0057276085137718535, Validation loss: 0.012357722722608055\n",
      "iterations:  1541\n",
      "Training loss: 0.005721257035540307, Validation loss: 0.01233611747547608\n",
      "new best w\n",
      "iterations:  1542\n",
      "Training loss: 0.005714110747455747, Validation loss: 0.0123414484961358\n",
      "iterations:  1543\n",
      "Training loss: 0.0057074567970881295, Validation loss: 0.01229522923907968\n",
      "new best w\n",
      "iterations:  1544\n",
      "Training loss: 0.005701084786116414, Validation loss: 0.012328138180213513\n",
      "iterations:  1545\n",
      "Training loss: 0.005693908792984616, Validation loss: 0.012271370094143788\n",
      "new best w\n",
      "iterations:  1546\n",
      "Training loss: 0.00568761528259531, Validation loss: 0.012275093197999753\n",
      "iterations:  1547\n",
      "Training loss: 0.0056804940945729574, Validation loss: 0.012284049765556432\n",
      "iterations:  1548\n",
      "Training loss: 0.0056741056039418414, Validation loss: 0.012242276782467613\n",
      "new best w\n",
      "iterations:  1549\n",
      "Training loss: 0.0056671110270922476, Validation loss: 0.01223537665894868\n",
      "new best w\n",
      "iterations:  1550\n",
      "Training loss: 0.005660540851174598, Validation loss: 0.012237548294271223\n",
      "iterations:  1551\n",
      "Training loss: 0.005653898201605937, Validation loss: 0.012203675285940995\n",
      "new best w\n",
      "iterations:  1552\n",
      "Training loss: 0.005647008621838539, Validation loss: 0.012211929267758583\n",
      "iterations:  1553\n",
      "Training loss: 0.0056408925275302614, Validation loss: 0.012187322499236505\n",
      "new best w\n",
      "iterations:  1554\n",
      "Training loss: 0.005633932075395412, Validation loss: 0.012173357296730362\n",
      "new best w\n",
      "iterations:  1555\n",
      "Training loss: 0.005627676343751216, Validation loss: 0.012149230958948622\n",
      "new best w\n",
      "iterations:  1556\n",
      "Training loss: 0.005620589286288148, Validation loss: 0.012169575933983318\n",
      "iterations:  1557\n",
      "Training loss: 0.005613880939868695, Validation loss: 0.012129543186898996\n",
      "new best w\n",
      "iterations:  1558\n",
      "Training loss: 0.005607441842445904, Validation loss: 0.012134491494943266\n",
      "iterations:  1559\n",
      "Training loss: 0.005600529887425407, Validation loss: 0.012116150259077536\n",
      "new best w\n",
      "iterations:  1560\n",
      "Training loss: 0.0055944767002946635, Validation loss: 0.012084210714174735\n",
      "new best w\n",
      "iterations:  1561\n",
      "Training loss: 0.0055876529694608434, Validation loss: 0.012108681236153676\n",
      "iterations:  1562\n",
      "Training loss: 0.00558136973370513, Validation loss: 0.012063299298851362\n",
      "new best w\n",
      "iterations:  1563\n",
      "Training loss: 0.005574362252329813, Validation loss: 0.012076209705906492\n",
      "iterations:  1564\n",
      "Training loss: 0.005567821849999069, Validation loss: 0.01205278098282849\n",
      "new best w\n",
      "iterations:  1565\n",
      "Training loss: 0.005561374126207819, Validation loss: 0.012020587535182493\n",
      "new best w\n",
      "iterations:  1566\n",
      "Training loss: 0.005554596069397566, Validation loss: 0.012031741052445333\n",
      "iterations:  1567\n",
      "Training loss: 0.005548546422918148, Validation loss: 0.01201340467929632\n",
      "new best w\n",
      "iterations:  1568\n",
      "Training loss: 0.00554169174209473, Validation loss: 0.012011648792152125\n",
      "new best w\n",
      "iterations:  1569\n",
      "Training loss: 0.005535571072965179, Validation loss: 0.011985206484934575\n",
      "new best w\n",
      "iterations:  1570\n",
      "Training loss: 0.005528585486258979, Validation loss: 0.01198476641012031\n",
      "new best w\n",
      "iterations:  1571\n",
      "Training loss: 0.005522075619149161, Validation loss: 0.011949040485371716\n",
      "new best w\n",
      "iterations:  1572\n",
      "Training loss: 0.005515720669642396, Validation loss: 0.011957405915260729\n",
      "iterations:  1573\n",
      "Training loss: 0.005508952162252957, Validation loss: 0.011946064641942797\n",
      "new best w\n",
      "iterations:  1574\n",
      "Training loss: 0.0055029853650283775, Validation loss: 0.011920842977312253\n",
      "new best w\n",
      "iterations:  1575\n",
      "Training loss: 0.00549616573383415, Validation loss: 0.011935112406564415\n",
      "iterations:  1576\n",
      "Training loss: 0.005490034981038363, Validation loss: 0.011877775699817853\n",
      "new best w\n",
      "iterations:  1577\n",
      "Training loss: 0.005483226970647055, Validation loss: 0.011897407015604603\n",
      "iterations:  1578\n",
      "Training loss: 0.0054768147289602056, Validation loss: 0.011881564693085623\n",
      "iterations:  1579\n",
      "Training loss: 0.005470429360327601, Validation loss: 0.01185956386582336\n",
      "new best w\n",
      "iterations:  1580\n",
      "Training loss: 0.0054638765681271125, Validation loss: 0.011869882860861141\n",
      "iterations:  1581\n",
      "Training loss: 0.005457838306365652, Validation loss: 0.011830115954588985\n",
      "new best w\n",
      "iterations:  1582\n",
      "Training loss: 0.005451055268550442, Validation loss: 0.01183221729590397\n",
      "iterations:  1583\n",
      "Training loss: 0.005445019366909468, Validation loss: 0.011815136967023602\n",
      "new best w\n",
      "iterations:  1584\n",
      "Training loss: 0.005438213071453256, Validation loss: 0.011812668653144665\n",
      "new best w\n",
      "iterations:  1585\n",
      "Training loss: 0.005431937608800243, Validation loss: 0.011790125692884798\n",
      "new best w\n",
      "iterations:  1586\n",
      "Training loss: 0.005425613099036469, Validation loss: 0.011784371537013005\n",
      "new best w\n",
      "iterations:  1587\n",
      "Training loss: 0.005419123226950148, Validation loss: 0.011757958660809443\n",
      "new best w\n",
      "iterations:  1588\n",
      "Training loss: 0.005413212972137465, Validation loss: 0.011758503733395185\n",
      "iterations:  1589\n",
      "Training loss: 0.005406393575373497, Validation loss: 0.011750847569435023\n",
      "new best w\n",
      "iterations:  1590\n",
      "Training loss: 0.005400370829746157, Validation loss: 0.011723677097524176\n",
      "new best w\n",
      "iterations:  1591\n",
      "Training loss: 0.0053937106462456684, Validation loss: 0.011735012153194346\n",
      "iterations:  1592\n",
      "Training loss: 0.005387489130201793, Validation loss: 0.011700506376197822\n",
      "new best w\n",
      "iterations:  1593\n",
      "Training loss: 0.0053811366258803076, Validation loss: 0.011706096933459963\n",
      "iterations:  1594\n",
      "Training loss: 0.005374757583807991, Validation loss: 0.011685199865670482\n",
      "new best w\n",
      "iterations:  1595\n",
      "Training loss: 0.005368729725678628, Validation loss: 0.011655480671039515\n",
      "new best w\n",
      "iterations:  1596\n",
      "Training loss: 0.005362205491898112, Validation loss: 0.011679024846085736\n",
      "iterations:  1597\n",
      "Training loss: 0.005356486492139934, Validation loss: 0.011636470554100999\n",
      "new best w\n",
      "iterations:  1598\n",
      "Training loss: 0.005349876624427536, Validation loss: 0.01165699080744894\n",
      "iterations:  1599\n",
      "Training loss: 0.005343910192117116, Validation loss: 0.011614644135044205\n",
      "new best w\n",
      "iterations:  1600\n",
      "Training loss: 0.005337141833965745, Validation loss: 0.01161011813364515\n",
      "new best w\n",
      "iterations:  1601\n",
      "Training loss: 0.005330870904328619, Validation loss: 0.011593043183781635\n",
      "new best w\n",
      "iterations:  1602\n",
      "Training loss: 0.005324680225648738, Validation loss: 0.011595309460380166\n",
      "iterations:  1603\n",
      "Training loss: 0.00531825084272601, Validation loss: 0.011578452457316285\n",
      "new best w\n",
      "iterations:  1604\n",
      "Training loss: 0.005312477360008689, Validation loss: 0.011566722719185544\n",
      "new best w\n",
      "iterations:  1605\n",
      "Training loss: 0.005305813588196301, Validation loss: 0.011562807944902633\n",
      "new best w\n",
      "iterations:  1606\n",
      "Training loss: 0.005299994211192241, Validation loss: 0.011543325511799127\n",
      "new best w\n",
      "iterations:  1607\n",
      "Training loss: 0.005293406656935946, Validation loss: 0.011534021410362734\n",
      "new best w\n",
      "iterations:  1608\n",
      "Training loss: 0.005287389114549214, Validation loss: 0.011504173949640718\n",
      "new best w\n",
      "iterations:  1609\n",
      "Training loss: 0.005281140706045356, Validation loss: 0.011512886995566001\n",
      "iterations:  1610\n",
      "Training loss: 0.005274877664683423, Validation loss: 0.01149890310627834\n",
      "new best w\n",
      "iterations:  1611\n",
      "Training loss: 0.005268993915494494, Validation loss: 0.011476225547819916\n",
      "new best w\n",
      "iterations:  1612\n",
      "Training loss: 0.005262592243179721, Validation loss: 0.011489594961686164\n",
      "iterations:  1613\n",
      "Training loss: 0.005257007752691377, Validation loss: 0.011430209503707916\n",
      "new best w\n",
      "iterations:  1614\n",
      "Training loss: 0.005250633775329655, Validation loss: 0.011476148165522356\n",
      "iterations:  1615\n",
      "Training loss: 0.00524467995631401, Validation loss: 0.011412816020649575\n",
      "new best w\n",
      "iterations:  1616\n",
      "Training loss: 0.005238074621457784, Validation loss: 0.011438297578245878\n",
      "iterations:  1617\n",
      "Training loss: 0.005231918259203026, Validation loss: 0.011412210809542803\n",
      "new best w\n",
      "iterations:  1618\n",
      "Training loss: 0.0052258185781056685, Validation loss: 0.011397865547791142\n",
      "new best w\n",
      "iterations:  1619\n",
      "Training loss: 0.0052196383065865794, Validation loss: 0.01140367576028954\n",
      "iterations:  1620\n",
      "Training loss: 0.005213850822880175, Validation loss: 0.011367296647013208\n",
      "new best w\n",
      "iterations:  1621\n",
      "Training loss: 0.005207509789990964, Validation loss: 0.011389361440786984\n",
      "iterations:  1622\n",
      "Training loss: 0.005201735827651838, Validation loss: 0.011335469040066295\n",
      "new best w\n",
      "iterations:  1623\n",
      "Training loss: 0.005195266486287063, Validation loss: 0.011350349790122272\n",
      "iterations:  1624\n",
      "Training loss: 0.00518941251624302, Validation loss: 0.011326423945325447\n",
      "new best w\n",
      "iterations:  1625\n",
      "Training loss: 0.005183190789570038, Validation loss: 0.011325945169538398\n",
      "new best w\n",
      "iterations:  1626\n",
      "Training loss: 0.005177186066696935, Validation loss: 0.011311435056954092\n",
      "new best w\n",
      "iterations:  1627\n",
      "Training loss: 0.005171351731195572, Validation loss: 0.011296460001933527\n",
      "new best w\n",
      "iterations:  1628\n",
      "Training loss: 0.0051651623520410665, Validation loss: 0.011280078203488223\n",
      "new best w\n",
      "iterations:  1629\n",
      "Training loss: 0.005159644047463251, Validation loss: 0.011271754554813825\n",
      "new best w\n",
      "iterations:  1630\n",
      "Training loss: 0.005153099243070394, Validation loss: 0.011272511241236401\n",
      "iterations:  1631\n",
      "Training loss: 0.005147391149141011, Validation loss: 0.011240227405971647\n",
      "new best w\n",
      "iterations:  1632\n",
      "Training loss: 0.005141042728467667, Validation loss: 0.011255727477360927\n",
      "iterations:  1633\n",
      "Training loss: 0.005135167136000072, Validation loss: 0.011218923845473513\n",
      "new best w\n",
      "iterations:  1634\n",
      "Training loss: 0.0051291091011138, Validation loss: 0.01122685479765673\n",
      "iterations:  1635\n",
      "Training loss: 0.005123070063215758, Validation loss: 0.011205665010241304\n",
      "new best w\n",
      "iterations:  1636\n",
      "Training loss: 0.005117384365656525, Validation loss: 0.011196858876346475\n",
      "new best w\n",
      "iterations:  1637\n",
      "Training loss: 0.005111206353251587, Validation loss: 0.011191254217180084\n",
      "new best w\n",
      "iterations:  1638\n",
      "Training loss: 0.005105742833641263, Validation loss: 0.011149648425838297\n",
      "new best w\n",
      "iterations:  1639\n",
      "Training loss: 0.0050993975142744445, Validation loss: 0.011183778756202366\n",
      "iterations:  1640\n",
      "Training loss: 0.005093683261065737, Validation loss: 0.011137775939960806\n",
      "new best w\n",
      "iterations:  1641\n",
      "Training loss: 0.005087417490809849, Validation loss: 0.011148056931497109\n",
      "iterations:  1642\n",
      "Training loss: 0.005081616457814995, Validation loss: 0.011126060839197606\n",
      "new best w\n",
      "iterations:  1643\n",
      "Training loss: 0.005075598636234814, Validation loss: 0.011115910478201759\n",
      "new best w\n",
      "iterations:  1644\n",
      "Training loss: 0.005069747867706554, Validation loss: 0.011114362857859833\n",
      "new best w\n",
      "iterations:  1645\n",
      "Training loss: 0.0050640630549148805, Validation loss: 0.011068472437058056\n",
      "new best w\n",
      "iterations:  1646\n",
      "Training loss: 0.005058108706256934, Validation loss: 0.011103973975283201\n",
      "iterations:  1647\n",
      "Training loss: 0.005052594011268909, Validation loss: 0.011049880720005716\n",
      "new best w\n",
      "iterations:  1648\n",
      "Training loss: 0.005046278743941278, Validation loss: 0.011076654732168695\n",
      "iterations:  1649\n",
      "Training loss: 0.0050406796078807615, Validation loss: 0.011042631940114667\n",
      "new best w\n",
      "iterations:  1650\n",
      "Training loss: 0.005034444514230678, Validation loss: 0.011043123211482177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1651\n",
      "Training loss: 0.005028771979940176, Validation loss: 0.011027682452105402\n",
      "new best w\n",
      "iterations:  1652\n",
      "Training loss: 0.005022762154967422, Validation loss: 0.011012172099661783\n",
      "new best w\n",
      "iterations:  1653\n",
      "Training loss: 0.005017032451877719, Validation loss: 0.011015923456976913\n",
      "iterations:  1654\n",
      "Training loss: 0.005011314375405244, Validation loss: 0.010981827398374422\n",
      "new best w\n",
      "iterations:  1655\n",
      "Training loss: 0.005005389116963604, Validation loss: 0.01098205107801144\n",
      "iterations:  1656\n",
      "Training loss: 0.00500002379347959, Validation loss: 0.010979266496785834\n",
      "new best w\n",
      "iterations:  1657\n",
      "Training loss: 0.004993777822299263, Validation loss: 0.01095783978076419\n",
      "new best w\n",
      "iterations:  1658\n",
      "Training loss: 0.004988572744102106, Validation loss: 0.010943672581927457\n",
      "new best w\n",
      "iterations:  1659\n",
      "Training loss: 0.004982399817013932, Validation loss: 0.010955572373919014\n",
      "iterations:  1660\n",
      "Training loss: 0.004976930156262817, Validation loss: 0.010910324776467303\n",
      "new best w\n",
      "iterations:  1661\n",
      "Training loss: 0.004970711561357494, Validation loss: 0.010935692496481792\n",
      "iterations:  1662\n",
      "Training loss: 0.004964919621239966, Validation loss: 0.010895596170101635\n",
      "new best w\n",
      "iterations:  1663\n",
      "Training loss: 0.004959197770029174, Validation loss: 0.01090099512403667\n",
      "iterations:  1664\n",
      "Training loss: 0.004953415489225106, Validation loss: 0.010869565488437647\n",
      "new best w\n",
      "iterations:  1665\n",
      "Training loss: 0.004948119057109467, Validation loss: 0.010890913521227072\n",
      "iterations:  1666\n",
      "Training loss: 0.004942038323635831, Validation loss: 0.010846191035958264\n",
      "new best w\n",
      "iterations:  1667\n",
      "Training loss: 0.004936807141422504, Validation loss: 0.010853831838414122\n",
      "iterations:  1668\n",
      "Training loss: 0.004930586745379371, Validation loss: 0.010846602206597371\n",
      "iterations:  1669\n",
      "Training loss: 0.004925148169147897, Validation loss: 0.010817693079476198\n",
      "new best w\n",
      "iterations:  1670\n",
      "Training loss: 0.004919149646875873, Validation loss: 0.010832502679977356\n",
      "iterations:  1671\n",
      "Training loss: 0.0049135712636487385, Validation loss: 0.010796218036445189\n",
      "new best w\n",
      "iterations:  1672\n",
      "Training loss: 0.004907810287765182, Validation loss: 0.010804718134223056\n",
      "iterations:  1673\n",
      "Training loss: 0.0049021326337953365, Validation loss: 0.010783391101966489\n",
      "new best w\n",
      "iterations:  1674\n",
      "Training loss: 0.004896673862993067, Validation loss: 0.010775333856463119\n",
      "new best w\n",
      "iterations:  1675\n",
      "Training loss: 0.0048908237115638615, Validation loss: 0.010770004211011404\n",
      "new best w\n",
      "iterations:  1676\n",
      "Training loss: 0.004885573129728793, Validation loss: 0.010750481743862427\n",
      "new best w\n",
      "iterations:  1677\n",
      "Training loss: 0.00487958980514164, Validation loss: 0.010753021628748608\n",
      "iterations:  1678\n",
      "Training loss: 0.0048745526731149395, Validation loss: 0.01070674997250952\n",
      "new best w\n",
      "iterations:  1679\n",
      "Training loss: 0.004868790462500098, Validation loss: 0.010760838158191618\n",
      "iterations:  1680\n",
      "Training loss: 0.004863366603768281, Validation loss: 0.010673620155757712\n",
      "new best w\n",
      "iterations:  1681\n",
      "Training loss: 0.004857438915200757, Validation loss: 0.010739483807105656\n",
      "iterations:  1682\n",
      "Training loss: 0.00485180251069146, Validation loss: 0.010659350148987088\n",
      "new best w\n",
      "iterations:  1683\n",
      "Training loss: 0.00484603408380189, Validation loss: 0.010693980732463238\n",
      "iterations:  1684\n",
      "Training loss: 0.004840469203615893, Validation loss: 0.010665464561755525\n",
      "iterations:  1685\n",
      "Training loss: 0.0048350672513534695, Validation loss: 0.010653766115115317\n",
      "new best w\n",
      "iterations:  1686\n",
      "Training loss: 0.004829431831184005, Validation loss: 0.010659914444019338\n",
      "iterations:  1687\n",
      "Training loss: 0.004824213536968722, Validation loss: 0.010625987379223227\n",
      "new best w\n",
      "iterations:  1688\n",
      "Training loss: 0.004818444648091144, Validation loss: 0.010645628469606056\n",
      "iterations:  1689\n",
      "Training loss: 0.004813448660828007, Validation loss: 0.010600763823041738\n",
      "new best w\n",
      "iterations:  1690\n",
      "Training loss: 0.004807706257408651, Validation loss: 0.010631608326388913\n",
      "iterations:  1691\n",
      "Training loss: 0.004802509815724941, Validation loss: 0.010559326353676356\n",
      "new best w\n",
      "iterations:  1692\n",
      "Training loss: 0.004796653860931252, Validation loss: 0.010625301649823563\n",
      "iterations:  1693\n",
      "Training loss: 0.004791261227774821, Validation loss: 0.010535323383187585\n",
      "new best w\n",
      "iterations:  1694\n",
      "Training loss: 0.004785667535074425, Validation loss: 0.010601484812992541\n",
      "iterations:  1695\n",
      "Training loss: 0.004780171921189897, Validation loss: 0.010522905908540194\n",
      "new best w\n",
      "iterations:  1696\n",
      "Training loss: 0.00477464368317535, Validation loss: 0.010555680138638545\n",
      "iterations:  1697\n",
      "Training loss: 0.004769167191783085, Validation loss: 0.010529007781342666\n",
      "iterations:  1698\n",
      "Training loss: 0.004763839522450452, Validation loss: 0.010513228035751124\n",
      "new best w\n",
      "iterations:  1699\n",
      "Training loss: 0.004758371740638013, Validation loss: 0.010528849336827186\n",
      "iterations:  1700\n",
      "Training loss: 0.004753245982265286, Validation loss: 0.010482998738730825\n",
      "new best w\n",
      "iterations:  1701\n",
      "Training loss: 0.004747580044344602, Validation loss: 0.010515191732170403\n",
      "iterations:  1702\n",
      "Training loss: 0.004742661213242277, Validation loss: 0.010459807242249524\n",
      "new best w\n",
      "iterations:  1703\n",
      "Training loss: 0.004737010355227287, Validation loss: 0.010499931866688347\n",
      "iterations:  1704\n",
      "Training loss: 0.004731762782813132, Validation loss: 0.010436040223102192\n",
      "new best w\n",
      "iterations:  1705\n",
      "Training loss: 0.0047259325946941225, Validation loss: 0.010473831001419331\n",
      "iterations:  1706\n",
      "Training loss: 0.0047205680657865885, Validation loss: 0.010424351103369091\n",
      "new best w\n",
      "iterations:  1707\n",
      "Training loss: 0.004715039503017615, Validation loss: 0.010441395119878098\n",
      "iterations:  1708\n",
      "Training loss: 0.0047097068373824, Validation loss: 0.010415749449692242\n",
      "new best w\n",
      "iterations:  1709\n",
      "Training loss: 0.004704465945271661, Validation loss: 0.010410262640762124\n",
      "new best w\n",
      "iterations:  1710\n",
      "Training loss: 0.0046990602192440055, Validation loss: 0.01040459271168958\n",
      "new best w\n",
      "iterations:  1711\n",
      "Training loss: 0.004693962471135327, Validation loss: 0.010384815860936817\n",
      "new best w\n",
      "iterations:  1712\n",
      "Training loss: 0.00468847325218402, Validation loss: 0.010389898264172933\n",
      "iterations:  1713\n",
      "Training loss: 0.004683535442777435, Validation loss: 0.010360117180539646\n",
      "new best w\n",
      "iterations:  1714\n",
      "Training loss: 0.004677881152688899, Validation loss: 0.010374102634268408\n",
      "iterations:  1715\n",
      "Training loss: 0.004673099966649209, Validation loss: 0.010337946382365282\n",
      "new best w\n",
      "iterations:  1716\n",
      "Training loss: 0.0046675198725103575, Validation loss: 0.010358446264404821\n",
      "iterations:  1717\n",
      "Training loss: 0.004663006895303134, Validation loss: 0.010310776723867921\n",
      "new best w\n",
      "iterations:  1718\n",
      "Training loss: 0.004657138804492679, Validation loss: 0.010344797035579334\n",
      "iterations:  1719\n",
      "Training loss: 0.004652675976506903, Validation loss: 0.010287615917008676\n",
      "new best w\n",
      "iterations:  1720\n",
      "Training loss: 0.004646609095119784, Validation loss: 0.010326689361812847\n",
      "iterations:  1721\n",
      "Training loss: 0.004641587201725216, Validation loss: 0.01027049031793642\n",
      "new best w\n",
      "iterations:  1722\n",
      "Training loss: 0.004635649416613792, Validation loss: 0.01029692237176922\n",
      "iterations:  1723\n",
      "Training loss: 0.004630475731537095, Validation loss: 0.010261519633841736\n",
      "new best w\n",
      "iterations:  1724\n",
      "Training loss: 0.004624979093117815, Validation loss: 0.010262758494531888\n",
      "iterations:  1725\n",
      "Training loss: 0.004619847707498498, Validation loss: 0.010254450628527301\n",
      "new best w\n",
      "iterations:  1726\n",
      "Training loss: 0.0046146678195648876, Validation loss: 0.010230758711825851\n",
      "new best w\n",
      "iterations:  1727\n",
      "Training loss: 0.004609495579251843, Validation loss: 0.010244297428351878\n",
      "iterations:  1728\n",
      "Training loss: 0.004604446653377143, Validation loss: 0.010204845033884824\n",
      "new best w\n",
      "iterations:  1729\n",
      "Training loss: 0.004599198757324687, Validation loss: 0.010230301892412834\n",
      "iterations:  1730\n",
      "Training loss: 0.0045942953645106855, Validation loss: 0.010179967103809829\n",
      "new best w\n",
      "iterations:  1731\n",
      "Training loss: 0.0045888978938331835, Validation loss: 0.010215029688015135\n",
      "iterations:  1732\n",
      "Training loss: 0.0045841270211655, Validation loss: 0.010157763008316872\n",
      "new best w\n",
      "iterations:  1733\n",
      "Training loss: 0.004578833233829741, Validation loss: 0.010199790408330153\n",
      "iterations:  1734\n",
      "Training loss: 0.0045743042710415785, Validation loss: 0.01013070615588232\n",
      "new best w\n",
      "iterations:  1735\n",
      "Training loss: 0.004568748117400389, Validation loss: 0.010186543510667034\n",
      "iterations:  1736\n",
      "Training loss: 0.00456434052485871, Validation loss: 0.010107178496266914\n",
      "new best w\n",
      "iterations:  1737\n",
      "Training loss: 0.004558557798730679, Validation loss: 0.010170384521779399\n",
      "iterations:  1738\n",
      "Training loss: 0.004553484663180801, Validation loss: 0.010089513967698175\n",
      "new best w\n",
      "iterations:  1739\n",
      "Training loss: 0.004547737715988789, Validation loss: 0.010140291922721327\n",
      "iterations:  1740\n",
      "Training loss: 0.004542588481026573, Validation loss: 0.010081352136251609\n",
      "new best w\n",
      "iterations:  1741\n",
      "Training loss: 0.004537226618580622, Validation loss: 0.010105843785411659\n",
      "iterations:  1742\n",
      "Training loss: 0.0045321960078707, Validation loss: 0.010075006684950597\n",
      "new best w\n",
      "iterations:  1743\n",
      "Training loss: 0.004527115794209352, Validation loss: 0.010073633053887051\n",
      "new best w\n",
      "iterations:  1744\n",
      "Training loss: 0.004522095061272507, Validation loss: 0.010065521932007672\n",
      "new best w\n",
      "iterations:  1745\n",
      "Training loss: 0.0045171086210151045, Validation loss: 0.010047588392303736\n",
      "new best w\n",
      "iterations:  1746\n",
      "Training loss: 0.004512075977440023, Validation loss: 0.01005214834669632\n",
      "iterations:  1747\n",
      "Training loss: 0.004507189359635209, Validation loss: 0.010022684019661571\n",
      "new best w\n",
      "iterations:  1748\n",
      "Training loss: 0.0045020496050852616, Validation loss: 0.010037421210735552\n",
      "iterations:  1749\n",
      "Training loss: 0.004497236446483802, Validation loss: 0.01000053449728006\n",
      "new best w\n",
      "iterations:  1750\n",
      "Training loss: 0.0044920008932850565, Validation loss: 0.01002060609278051\n",
      "iterations:  1751\n",
      "Training loss: 0.004487272040226195, Validation loss: 0.009979913369636807\n",
      "new best w\n",
      "iterations:  1752\n",
      "Training loss: 0.0044822324488298995, Validation loss: 0.010004781470354138\n",
      "iterations:  1753\n",
      "Training loss: 0.004477676598120936, Validation loss: 0.009953784743911304\n",
      "new best w\n",
      "iterations:  1754\n",
      "Training loss: 0.004472435487995471, Validation loss: 0.009991467237476615\n",
      "iterations:  1755\n",
      "Training loss: 0.0044679683783013895, Validation loss: 0.009930763471310982\n",
      "new best w\n",
      "iterations:  1756\n",
      "Training loss: 0.00446256584731003, Validation loss: 0.009985686252945578\n",
      "iterations:  1757\n",
      "Training loss: 0.004458286397439071, Validation loss: 0.009897778298231935\n",
      "new best w\n",
      "iterations:  1758\n",
      "Training loss: 0.004452820754689643, Validation loss: 0.00998845756631289\n",
      "iterations:  1759\n",
      "Training loss: 0.0044487868474107975, Validation loss: 0.009859458382607472\n",
      "new best w\n",
      "iterations:  1760\n",
      "Training loss: 0.004442989842417001, Validation loss: 0.009976480793332845\n",
      "iterations:  1761\n",
      "Training loss: 0.0044389367914764435, Validation loss: 0.009840617061150811\n",
      "new best w\n",
      "iterations:  1762\n",
      "Training loss: 0.004432945283824511, Validation loss: 0.009954156905151848\n",
      "iterations:  1763\n",
      "Training loss: 0.0044280638636649515, Validation loss: 0.009841881459150449\n",
      "iterations:  1764\n",
      "Training loss: 0.004422301639979692, Validation loss: 0.009904794761438598\n",
      "iterations:  1765\n",
      "Training loss: 0.00441739058512251, Validation loss: 0.0098415980525208\n",
      "iterations:  1766\n",
      "Training loss: 0.0044120888634945656, Validation loss: 0.00986716535638388\n",
      "iterations:  1767\n",
      "Training loss: 0.004407316503882993, Validation loss: 0.009840585009119248\n",
      "new best w\n",
      "iterations:  1768\n",
      "Training loss: 0.004402325968426389, Validation loss: 0.009830207250424602\n",
      "new best w\n",
      "iterations:  1769\n",
      "Training loss: 0.004397615177458874, Validation loss: 0.009840042149319533\n",
      "iterations:  1770\n",
      "Training loss: 0.00439277286540767, Validation loss: 0.009795706185731345\n",
      "new best w\n",
      "iterations:  1771\n",
      "Training loss: 0.004387978606634947, Validation loss: 0.009836883005414751\n",
      "iterations:  1772\n",
      "Training loss: 0.004383284457458431, Validation loss: 0.009763541235817621\n",
      "new best w\n",
      "iterations:  1773\n",
      "Training loss: 0.004378390269881814, Validation loss: 0.009826243522907518\n",
      "iterations:  1774\n",
      "Training loss: 0.004373836272656873, Validation loss: 0.009737996180063869\n",
      "new best w\n",
      "iterations:  1775\n",
      "Training loss: 0.00436876555675209, Validation loss: 0.009812037929494354\n",
      "iterations:  1776\n",
      "Training loss: 0.00436433372768216, Validation loss: 0.00971623547100689\n",
      "new best w\n",
      "iterations:  1777\n",
      "Training loss: 0.004359115136341708, Validation loss: 0.00979549454364706\n",
      "iterations:  1778\n",
      "Training loss: 0.004354576053422109, Validation loss: 0.009714132979507285\n",
      "new best w\n",
      "iterations:  1779\n",
      "Training loss: 0.004349372721921398, Validation loss: 0.00975600102887295\n",
      "iterations:  1780\n",
      "Training loss: 0.00434494034874176, Validation loss: 0.009703251027842781\n",
      "new best w\n",
      "iterations:  1781\n",
      "Training loss: 0.004339723743016239, Validation loss: 0.009734073722152162\n",
      "iterations:  1782\n",
      "Training loss: 0.004335338281490988, Validation loss: 0.009685926862854764\n",
      "new best w\n",
      "iterations:  1783\n",
      "Training loss: 0.0043301130523113055, Validation loss: 0.009714736954625604\n",
      "iterations:  1784\n",
      "Training loss: 0.004325782327043114, Validation loss: 0.009667603654305652\n",
      "new best w\n",
      "iterations:  1785\n",
      "Training loss: 0.0043208546420337455, Validation loss: 0.009707858983373972\n",
      "iterations:  1786\n",
      "Training loss: 0.00431669923023688, Validation loss: 0.009635628288436619\n",
      "new best w\n",
      "iterations:  1787\n",
      "Training loss: 0.004311684317291905, Validation loss: 0.00970869654405632\n",
      "iterations:  1788\n",
      "Training loss: 0.004307468774140161, Validation loss: 0.009611000686387088\n",
      "new best w\n",
      "iterations:  1789\n",
      "Training loss: 0.004302275180811607, Validation loss: 0.009681724913401041\n",
      "iterations:  1790\n",
      "Training loss: 0.0042980984424295186, Validation loss: 0.00959765262071411\n",
      "new best w\n",
      "iterations:  1791\n",
      "Training loss: 0.00429284847652166, Validation loss: 0.00965919028659041\n",
      "iterations:  1792\n",
      "Training loss: 0.0042886830797651465, Validation loss: 0.009586249864420162\n",
      "new best w\n",
      "iterations:  1793\n",
      "Training loss: 0.004283459770850144, Validation loss: 0.00963314027245279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1794\n",
      "Training loss: 0.004279234430451469, Validation loss: 0.009577106578919367\n",
      "new best w\n",
      "iterations:  1795\n",
      "Training loss: 0.004274100680920563, Validation loss: 0.009605605278063475\n",
      "iterations:  1796\n",
      "Training loss: 0.004269795042922622, Validation loss: 0.00956272857613989\n",
      "new best w\n",
      "iterations:  1797\n",
      "Training loss: 0.004264739559370852, Validation loss: 0.009584850850567369\n",
      "iterations:  1798\n",
      "Training loss: 0.004260415568165582, Validation loss: 0.009545179127710298\n",
      "new best w\n",
      "iterations:  1799\n",
      "Training loss: 0.004255303456855123, Validation loss: 0.009583762624792866\n",
      "iterations:  1800\n",
      "Training loss: 0.0042511322753735654, Validation loss: 0.009505447298021377\n",
      "new best w\n",
      "iterations:  1801\n",
      "Training loss: 0.004246037838173114, Validation loss: 0.009573997462595695\n",
      "iterations:  1802\n",
      "Training loss: 0.004241919865014118, Validation loss: 0.009483204843964787\n",
      "new best w\n",
      "iterations:  1803\n",
      "Training loss: 0.0042367768656484195, Validation loss: 0.009557489668975083\n",
      "iterations:  1804\n",
      "Training loss: 0.004232671728352202, Validation loss: 0.009473231720359911\n",
      "new best w\n",
      "iterations:  1805\n",
      "Training loss: 0.0042276140580085894, Validation loss: 0.00954790426310247\n",
      "iterations:  1806\n",
      "Training loss: 0.004223455319153078, Validation loss: 0.009450291556609506\n",
      "new best w\n",
      "iterations:  1807\n",
      "Training loss: 0.0042183683916746754, Validation loss: 0.009522823006919392\n",
      "iterations:  1808\n",
      "Training loss: 0.004214267625586424, Validation loss: 0.009431825036952621\n",
      "new best w\n",
      "iterations:  1809\n",
      "Training loss: 0.00420923646581474, Validation loss: 0.009517145569417382\n",
      "iterations:  1810\n",
      "Training loss: 0.004205297464489407, Validation loss: 0.009398327639434892\n",
      "new best w\n",
      "iterations:  1811\n",
      "Training loss: 0.00420011336479154, Validation loss: 0.009505739935730489\n",
      "iterations:  1812\n",
      "Training loss: 0.004196283174548676, Validation loss: 0.009376480606145042\n",
      "new best w\n",
      "iterations:  1813\n",
      "Training loss: 0.0041909526532735, Validation loss: 0.00948915249188213\n",
      "iterations:  1814\n",
      "Training loss: 0.004187254016498377, Validation loss: 0.009357280253078077\n",
      "new best w\n",
      "iterations:  1815\n",
      "Training loss: 0.004181861311079748, Validation loss: 0.009471466164878175\n",
      "iterations:  1816\n",
      "Training loss: 0.004177909000559857, Validation loss: 0.009356067976058159\n",
      "new best w\n",
      "iterations:  1817\n",
      "Training loss: 0.004172629125606511, Validation loss: 0.009432178615949418\n",
      "iterations:  1818\n",
      "Training loss: 0.004168755892079564, Validation loss: 0.009346300671902088\n",
      "new best w\n",
      "iterations:  1819\n",
      "Training loss: 0.004163568844036105, Validation loss: 0.00941977331754907\n",
      "iterations:  1820\n",
      "Training loss: 0.004159663369601588, Validation loss: 0.009337736539015233\n",
      "new best w\n",
      "iterations:  1821\n",
      "Training loss: 0.004154521738809543, Validation loss: 0.009396401958870267\n",
      "iterations:  1822\n",
      "Training loss: 0.0041505943826638855, Validation loss: 0.009313171853250429\n",
      "new best w\n",
      "iterations:  1823\n",
      "Training loss: 0.00414554449331314, Validation loss: 0.009378168183505177\n",
      "iterations:  1824\n",
      "Training loss: 0.00414150726610576, Validation loss: 0.009307357315417862\n",
      "new best w\n",
      "iterations:  1825\n",
      "Training loss: 0.004136641588420453, Validation loss: 0.009345014368198618\n",
      "iterations:  1826\n",
      "Training loss: 0.004132461122060144, Validation loss: 0.009295915056456343\n",
      "new best w\n",
      "iterations:  1827\n",
      "Training loss: 0.004127659556847886, Validation loss: 0.00934081294066438\n",
      "iterations:  1828\n",
      "Training loss: 0.004123530851711359, Validation loss: 0.009258257864427012\n",
      "new best w\n",
      "iterations:  1829\n",
      "Training loss: 0.004118793784883248, Validation loss: 0.009330779131521248\n",
      "iterations:  1830\n",
      "Training loss: 0.004114682835207665, Validation loss: 0.009236624436510781\n",
      "new best w\n",
      "iterations:  1831\n",
      "Training loss: 0.004109966833560555, Validation loss: 0.009318935804245861\n",
      "iterations:  1832\n",
      "Training loss: 0.00410588402779655, Validation loss: 0.009221709322112491\n",
      "new best w\n",
      "iterations:  1833\n",
      "Training loss: 0.0041011836482413916, Validation loss: 0.009307199565725798\n",
      "iterations:  1834\n",
      "Training loss: 0.004097052991995937, Validation loss: 0.009203367821079545\n",
      "new best w\n",
      "iterations:  1835\n",
      "Training loss: 0.00409242090639699, Validation loss: 0.009293443404067181\n",
      "iterations:  1836\n",
      "Training loss: 0.004088428738201272, Validation loss: 0.009171283432270902\n",
      "new best w\n",
      "iterations:  1837\n",
      "Training loss: 0.004083666499757373, Validation loss: 0.009282075115646844\n",
      "iterations:  1838\n",
      "Training loss: 0.004079767658881883, Validation loss: 0.009149883596301662\n",
      "new best w\n",
      "iterations:  1839\n",
      "Training loss: 0.004074911880741806, Validation loss: 0.009265853249753136\n",
      "iterations:  1840\n",
      "Training loss: 0.004070804156153589, Validation loss: 0.009148044649603376\n",
      "new best w\n",
      "iterations:  1841\n",
      "Training loss: 0.004066044839975424, Validation loss: 0.009227525823172801\n",
      "iterations:  1842\n",
      "Training loss: 0.004061955797117233, Validation loss: 0.00913846450325002\n",
      "new best w\n",
      "iterations:  1843\n",
      "Training loss: 0.004057360051883554, Validation loss: 0.009215290491540103\n",
      "iterations:  1844\n",
      "Training loss: 0.004053173680872676, Validation loss: 0.009130005397216891\n",
      "new best w\n",
      "iterations:  1845\n",
      "Training loss: 0.0040486875715114315, Validation loss: 0.009192322594996161\n",
      "iterations:  1846\n",
      "Training loss: 0.004044427364254832, Validation loss: 0.009105956039200743\n",
      "new best w\n",
      "iterations:  1847\n",
      "Training loss: 0.004040075804886129, Validation loss: 0.009174561107389168\n",
      "iterations:  1848\n",
      "Training loss: 0.0040356507326373605, Validation loss: 0.009100192290603635\n",
      "new best w\n",
      "iterations:  1849\n",
      "Training loss: 0.004031065531508417, Validation loss: 0.00915700899376433\n",
      "iterations:  1850\n",
      "Training loss: 0.004026622500925866, Validation loss: 0.00907441596533232\n",
      "new best w\n",
      "iterations:  1851\n",
      "Training loss: 0.004022192189793527, Validation loss: 0.009139346395249198\n",
      "iterations:  1852\n",
      "Training loss: 0.004017841611234957, Validation loss: 0.009058135455341329\n",
      "new best w\n",
      "iterations:  1853\n",
      "Training loss: 0.0040134953197817035, Validation loss: 0.009123963036775862\n",
      "iterations:  1854\n",
      "Training loss: 0.004009294977065229, Validation loss: 0.009036823623577004\n",
      "new best w\n",
      "iterations:  1855\n",
      "Training loss: 0.0040050071871898536, Validation loss: 0.009121779674070714\n",
      "iterations:  1856\n",
      "Training loss: 0.0040010398688580615, Validation loss: 0.009022679323356046\n",
      "new best w\n",
      "iterations:  1857\n",
      "Training loss: 0.003996793647006599, Validation loss: 0.00910407910423012\n",
      "iterations:  1858\n",
      "Training loss: 0.003992921975973213, Validation loss: 0.008990746383148689\n",
      "new best w\n",
      "iterations:  1859\n",
      "Training loss: 0.003988522112743403, Validation loss: 0.009094794169272922\n",
      "iterations:  1860\n",
      "Training loss: 0.003984695765023976, Validation loss: 0.008967780080210336\n",
      "new best w\n",
      "iterations:  1861\n",
      "Training loss: 0.003980239937602373, Validation loss: 0.009080620317709832\n",
      "iterations:  1862\n",
      "Training loss: 0.003976075733871213, Validation loss: 0.008964502159251327\n",
      "new best w\n",
      "iterations:  1863\n",
      "Training loss: 0.0039717665261068, Validation loss: 0.009044129376222405\n",
      "iterations:  1864\n",
      "Training loss: 0.00396757211348747, Validation loss: 0.00895872209889316\n",
      "new best w\n",
      "iterations:  1865\n",
      "Training loss: 0.00396337307424746, Validation loss: 0.009040484687466068\n",
      "iterations:  1866\n",
      "Training loss: 0.003959167888646868, Validation loss: 0.008932530965642303\n",
      "new best w\n",
      "iterations:  1867\n",
      "Training loss: 0.003955077095892122, Validation loss: 0.00901623394672347\n",
      "iterations:  1868\n",
      "Training loss: 0.003950655583643311, Validation loss: 0.008932033073910033\n",
      "new best w\n",
      "iterations:  1869\n",
      "Training loss: 0.003946812182300487, Validation loss: 0.008996549745078273\n",
      "iterations:  1870\n",
      "Training loss: 0.003942291628018806, Validation loss: 0.00890244049437922\n",
      "new best w\n",
      "iterations:  1871\n",
      "Training loss: 0.003938521719020256, Validation loss: 0.00898380057135326\n",
      "iterations:  1872\n",
      "Training loss: 0.003934008318739054, Validation loss: 0.008882735099322732\n",
      "new best w\n",
      "iterations:  1873\n",
      "Training loss: 0.003930410582034666, Validation loss: 0.008986255214879\n",
      "iterations:  1874\n",
      "Training loss: 0.00392600389682217, Validation loss: 0.008851031385697362\n",
      "new best w\n",
      "iterations:  1875\n",
      "Training loss: 0.003922409088201998, Validation loss: 0.00898307068805115\n",
      "iterations:  1876\n",
      "Training loss: 0.0039178731795928064, Validation loss: 0.00882911689230283\n",
      "new best w\n",
      "iterations:  1877\n",
      "Training loss: 0.003914029126946647, Validation loss: 0.008957570380109183\n",
      "iterations:  1878\n",
      "Training loss: 0.003909662220843112, Validation loss: 0.008814241568324183\n",
      "new best w\n",
      "iterations:  1879\n",
      "Training loss: 0.0039056830571601526, Validation loss: 0.008938719284549446\n",
      "iterations:  1880\n",
      "Training loss: 0.0039011537700077096, Validation loss: 0.008814600603769731\n",
      "iterations:  1881\n",
      "Training loss: 0.003897386252298058, Validation loss: 0.008901228400190447\n",
      "iterations:  1882\n",
      "Training loss: 0.003892817034288185, Validation loss: 0.008809968223686822\n",
      "new best w\n",
      "iterations:  1883\n",
      "Training loss: 0.0038891060430235793, Validation loss: 0.008882880971946916\n",
      "iterations:  1884\n",
      "Training loss: 0.003884508772246983, Validation loss: 0.00880462479292244\n",
      "new best w\n",
      "iterations:  1885\n",
      "Training loss: 0.003881087771636246, Validation loss: 0.008861595062222156\n",
      "iterations:  1886\n",
      "Training loss: 0.003876257009802992, Validation loss: 0.00879031975957582\n",
      "new best w\n",
      "iterations:  1887\n",
      "Training loss: 0.003872429478742213, Validation loss: 0.008846809455646457\n",
      "iterations:  1888\n",
      "Training loss: 0.0038681875918628304, Validation loss: 0.008763381905012434\n",
      "new best w\n",
      "iterations:  1889\n",
      "Training loss: 0.0038637310688531823, Validation loss: 0.008833333685066797\n",
      "iterations:  1890\n",
      "Training loss: 0.003860313622823591, Validation loss: 0.008749752610267508\n",
      "new best w\n",
      "iterations:  1891\n",
      "Training loss: 0.0038560512595550746, Validation loss: 0.008838551169898652\n",
      "iterations:  1892\n",
      "Training loss: 0.0038527615723448704, Validation loss: 0.008715353902915463\n",
      "new best w\n",
      "iterations:  1893\n",
      "Training loss: 0.0038484315465480068, Validation loss: 0.008838432664187476\n",
      "iterations:  1894\n",
      "Training loss: 0.0038449632738725968, Validation loss: 0.00869080907569943\n",
      "new best w\n",
      "iterations:  1895\n",
      "Training loss: 0.0038404704505684588, Validation loss: 0.00881476389266329\n",
      "iterations:  1896\n",
      "Training loss: 0.0038367316310829224, Validation loss: 0.008691219925462668\n",
      "iterations:  1897\n",
      "Training loss: 0.0038324688286510227, Validation loss: 0.008777320959542277\n",
      "iterations:  1898\n",
      "Training loss: 0.0038285667271766954, Validation loss: 0.008700398294537644\n",
      "iterations:  1899\n",
      "Training loss: 0.003824257705816052, Validation loss: 0.008754222328901402\n",
      "iterations:  1900\n",
      "Training loss: 0.0038204191853322155, Validation loss: 0.008685484781497322\n",
      "new best w\n",
      "iterations:  1901\n",
      "Training loss: 0.00381654626081276, Validation loss: 0.008742914739942619\n",
      "iterations:  1902\n",
      "Training loss: 0.0038125023544105306, Validation loss: 0.008652643285029585\n",
      "new best w\n",
      "iterations:  1903\n",
      "Training loss: 0.003808762787133165, Validation loss: 0.008732583533518563\n",
      "iterations:  1904\n",
      "Training loss: 0.0038046974477447208, Validation loss: 0.008631895203991413\n",
      "new best w\n",
      "iterations:  1905\n",
      "Training loss: 0.003801084632086538, Validation loss: 0.008735699641904156\n",
      "iterations:  1906\n",
      "Training loss: 0.003797346095808463, Validation loss: 0.008591507244281153\n",
      "new best w\n",
      "iterations:  1907\n",
      "Training loss: 0.003793562149018906, Validation loss: 0.008744018014967842\n",
      "iterations:  1908\n",
      "Training loss: 0.0037895879678520403, Validation loss: 0.008574159371654694\n",
      "new best w\n",
      "iterations:  1909\n",
      "Training loss: 0.0037855696321277, Validation loss: 0.008711026940873773\n",
      "iterations:  1910\n",
      "Training loss: 0.003781433458667656, Validation loss: 0.008578816829677795\n",
      "iterations:  1911\n",
      "Training loss: 0.0037776796195466613, Validation loss: 0.00867181498882497\n",
      "iterations:  1912\n",
      "Training loss: 0.003773375502333506, Validation loss: 0.008589163719365762\n",
      "iterations:  1913\n",
      "Training loss: 0.0037699474359804593, Validation loss: 0.008649554310512937\n",
      "iterations:  1914\n",
      "Training loss: 0.0037653860355360887, Validation loss: 0.008572107159753543\n",
      "new best w\n",
      "iterations:  1915\n",
      "Training loss: 0.0037623211318318565, Validation loss: 0.00863760392267035\n",
      "iterations:  1916\n",
      "Training loss: 0.0037576069574642213, Validation loss: 0.00854061891368701\n",
      "new best w\n",
      "iterations:  1917\n",
      "Training loss: 0.0037544634859141414, Validation loss: 0.008625836503515349\n",
      "iterations:  1918\n",
      "Training loss: 0.003749977136041525, Validation loss: 0.008522680256112362\n",
      "new best w\n",
      "iterations:  1919\n",
      "Training loss: 0.0037463804983144167, Validation loss: 0.008625874491980951\n",
      "iterations:  1920\n",
      "Training loss: 0.0037428430822861736, Validation loss: 0.008488506521813103\n",
      "new best w\n",
      "iterations:  1921\n",
      "Training loss: 0.0037385213688856844, Validation loss: 0.00863180118235972\n",
      "iterations:  1922\n",
      "Training loss: 0.0037352864681670516, Validation loss: 0.00847637639868038\n",
      "new best w\n",
      "iterations:  1923\n",
      "Training loss: 0.003730950875377474, Validation loss: 0.008601329642770559\n",
      "iterations:  1924\n",
      "Training loss: 0.003727443647120638, Validation loss: 0.00847858402181488\n",
      "iterations:  1925\n",
      "Training loss: 0.003723460099351404, Validation loss: 0.008564544511077314\n",
      "iterations:  1926\n",
      "Training loss: 0.003719689404351316, Validation loss: 0.008487039801784946\n",
      "iterations:  1927\n",
      "Training loss: 0.0037157125971209537, Validation loss: 0.008542217270361105\n",
      "iterations:  1928\n",
      "Training loss: 0.0037119543008198143, Validation loss: 0.008472548839264245\n",
      "new best w\n",
      "iterations:  1929\n",
      "Training loss: 0.003708321285176914, Validation loss: 0.00853082944419022\n",
      "iterations:  1930\n",
      "Training loss: 0.0037044479915360174, Validation loss: 0.008439989165497264\n",
      "new best w\n",
      "iterations:  1931\n",
      "Training loss: 0.003701037944301398, Validation loss: 0.008539303008709507\n",
      "iterations:  1932\n",
      "Training loss: 0.003697453486827776, Validation loss: 0.008397515158762726\n",
      "new best w\n",
      "iterations:  1933\n",
      "Training loss: 0.003693873818755715, Validation loss: 0.008548986877422535\n",
      "iterations:  1934\n",
      "Training loss: 0.00369008786241655, Validation loss: 0.008379233618293411\n",
      "new best w\n",
      "iterations:  1935\n",
      "Training loss: 0.0036862880762680993, Validation loss: 0.008517532891163668\n",
      "iterations:  1936\n",
      "Training loss: 0.0036822548514363934, Validation loss: 0.008383165892757774\n",
      "iterations:  1937\n",
      "Training loss: 0.003678814071656938, Validation loss: 0.008479456981923112\n",
      "iterations:  1938\n",
      "Training loss: 0.0036745148606079003, Validation loss: 0.008393043516519687\n",
      "iterations:  1939\n",
      "Training loss: 0.003671450520312298, Validation loss: 0.008457505133977535\n",
      "iterations:  1940\n",
      "Training loss: 0.0036668730722825118, Validation loss: 0.008376892880734638\n",
      "new best w\n",
      "iterations:  1941\n",
      "Training loss: 0.003663906493565697, Validation loss: 0.008444184876245727\n",
      "iterations:  1942\n",
      "Training loss: 0.0036594879328670266, Validation loss: 0.008347814671327917\n",
      "new best w\n",
      "iterations:  1943\n",
      "Training loss: 0.003656181630936068, Validation loss: 0.008449041695207037\n",
      "iterations:  1944\n",
      "Training loss: 0.003652663284751277, Validation loss: 0.008311888626482602\n",
      "new best w\n",
      "iterations:  1945\n",
      "Training loss: 0.003648625606604178, Validation loss: 0.008455871343756137\n",
      "iterations:  1946\n",
      "Training loss: 0.00364541541924562, Validation loss: 0.008299146368026849\n",
      "new best w\n",
      "iterations:  1947\n",
      "Training loss: 0.0036413793735817113, Validation loss: 0.008426449184714925\n",
      "iterations:  1948\n",
      "Training loss: 0.0036378714000861556, Validation loss: 0.00830098711966307\n",
      "iterations:  1949\n",
      "Training loss: 0.0036339371261258483, Validation loss: 0.008396864471194292\n",
      "iterations:  1950\n",
      "Training loss: 0.00363037928525855, Validation loss: 0.008302542285954957\n",
      "iterations:  1951\n",
      "Training loss: 0.0036269761654117265, Validation loss: 0.008373770334544606\n",
      "iterations:  1952\n",
      "Training loss: 0.0036230197053315656, Validation loss: 0.008291755493208623\n",
      "new best w\n",
      "iterations:  1953\n",
      "Training loss: 0.0036196674451737356, Validation loss: 0.00835842194240384\n",
      "iterations:  1954\n",
      "Training loss: 0.0036158564099326605, Validation loss: 0.008263179947055627\n",
      "new best w\n",
      "iterations:  1955\n",
      "Training loss: 0.0036126525353718294, Validation loss: 0.008368455904862616\n",
      "iterations:  1956\n",
      "Training loss: 0.003609098830697507, Validation loss: 0.008226777559694263\n",
      "new best w\n",
      "iterations:  1957\n",
      "Training loss: 0.003605742067734595, Validation loss: 0.008367008958944426\n",
      "iterations:  1958\n",
      "Training loss: 0.0036016769152627487, Validation loss: 0.0082210888542268\n",
      "new best w\n",
      "iterations:  1959\n",
      "Training loss: 0.003598387150083367, Validation loss: 0.008323715387450765\n",
      "iterations:  1960\n",
      "Training loss: 0.003594179888183851, Validation loss: 0.008233892729839357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1961\n",
      "Training loss: 0.003591303914525695, Validation loss: 0.008300711971720717\n",
      "iterations:  1962\n",
      "Training loss: 0.0035868803277277223, Validation loss: 0.008218996609363538\n",
      "new best w\n",
      "iterations:  1963\n",
      "Training loss: 0.0035830511386012553, Validation loss: 0.008281916053904265\n",
      "iterations:  1964\n",
      "Training loss: 0.003579689078957173, Validation loss: 0.008201584427555992\n",
      "new best w\n",
      "iterations:  1965\n",
      "Training loss: 0.003576319678689599, Validation loss: 0.008290884760494905\n",
      "iterations:  1966\n",
      "Training loss: 0.003573265373917409, Validation loss: 0.008158278472503961\n",
      "new best w\n",
      "iterations:  1967\n",
      "Training loss: 0.0035694540360201054, Validation loss: 0.008301025487153458\n",
      "iterations:  1968\n",
      "Training loss: 0.003566377434047405, Validation loss: 0.008140590278137096\n",
      "new best w\n",
      "iterations:  1969\n",
      "Training loss: 0.0035625043096927755, Validation loss: 0.0082725977933906\n",
      "iterations:  1970\n",
      "Training loss: 0.0035590747424493387, Validation loss: 0.008142359291385189\n",
      "iterations:  1971\n",
      "Training loss: 0.0035557589109783965, Validation loss: 0.008256922744890661\n",
      "iterations:  1972\n",
      "Training loss: 0.0035517897769096387, Validation loss: 0.00813935507391159\n",
      "new best w\n",
      "iterations:  1973\n",
      "Training loss: 0.003548862041559084, Validation loss: 0.008226835524624299\n",
      "iterations:  1974\n",
      "Training loss: 0.003544821479269142, Validation loss: 0.008117181417566092\n",
      "new best w\n",
      "iterations:  1975\n",
      "Training loss: 0.003541949462685375, Validation loss: 0.00823198735754634\n",
      "iterations:  1976\n",
      "Training loss: 0.0035383957910604565, Validation loss: 0.008076511641605763\n",
      "new best w\n",
      "iterations:  1977\n",
      "Training loss: 0.0035352525718272687, Validation loss: 0.008239025269650972\n",
      "iterations:  1978\n",
      "Training loss: 0.0035314845273643762, Validation loss: 0.00806158144451686\n",
      "new best w\n",
      "iterations:  1979\n",
      "Training loss: 0.003527712644717732, Validation loss: 0.008205101274578127\n",
      "iterations:  1980\n",
      "Training loss: 0.003524165471706211, Validation loss: 0.008072423913663935\n",
      "iterations:  1981\n",
      "Training loss: 0.0035204974363367016, Validation loss: 0.008183328203072314\n",
      "iterations:  1982\n",
      "Training loss: 0.0035168831960739172, Validation loss: 0.008075480720653914\n",
      "iterations:  1983\n",
      "Training loss: 0.0035134726305240827, Validation loss: 0.008153378051601486\n",
      "iterations:  1984\n",
      "Training loss: 0.0035101350602567156, Validation loss: 0.008055781578109298\n",
      "new best w\n",
      "iterations:  1985\n",
      "Training loss: 0.0035068792406779846, Validation loss: 0.008160473016048608\n",
      "iterations:  1986\n",
      "Training loss: 0.0035037832175542523, Validation loss: 0.008020503331402452\n",
      "new best w\n",
      "iterations:  1987\n",
      "Training loss: 0.0035002657011163636, Validation loss: 0.008158643968198268\n",
      "iterations:  1988\n",
      "Training loss: 0.003496712670882666, Validation loss: 0.008015280424684714\n",
      "new best w\n",
      "iterations:  1989\n",
      "Training loss: 0.0034935471172529574, Validation loss: 0.008117960593485954\n",
      "iterations:  1990\n",
      "Training loss: 0.0034895987629350537, Validation loss: 0.00802815209471588\n",
      "iterations:  1991\n",
      "Training loss: 0.0034869356895320665, Validation loss: 0.008093068046627709\n",
      "iterations:  1992\n",
      "Training loss: 0.003482641809086619, Validation loss: 0.008011579319163109\n",
      "new best w\n",
      "iterations:  1993\n",
      "Training loss: 0.0034795844554058658, Validation loss: 0.008096445793919046\n",
      "iterations:  1994\n",
      "Training loss: 0.0034763538346923926, Validation loss: 0.007967311304138308\n",
      "new best w\n",
      "iterations:  1995\n",
      "Training loss: 0.003472713170537073, Validation loss: 0.008108155328490371\n",
      "iterations:  1996\n",
      "Training loss: 0.0034697685559670487, Validation loss: 0.007951693389217375\n",
      "new best w\n",
      "iterations:  1997\n",
      "Training loss: 0.0034661461266918495, Validation loss: 0.008081772419780763\n",
      "iterations:  1998\n",
      "Training loss: 0.0034628440560650663, Validation loss: 0.007954601262470622\n",
      "iterations:  1999\n",
      "Training loss: 0.003459588727217466, Validation loss: 0.008062709528702819\n",
      "iterations:  2000\n",
      "Training loss: 0.003455889884385023, Validation loss: 0.007952149514533161\n",
      "iterations:  2001\n",
      "Training loss: 0.0034531102183243067, Validation loss: 0.008052056713228622\n",
      "iterations:  2002\n",
      "Training loss: 0.0034496712367905686, Validation loss: 0.007909740958473226\n",
      "new best w\n",
      "iterations:  2003\n",
      "Training loss: 0.0034469290147921385, Validation loss: 0.008063516197675617\n",
      "iterations:  2004\n",
      "Training loss: 0.0034438590662691643, Validation loss: 0.007869664359725257\n",
      "new best w\n",
      "iterations:  2005\n",
      "Training loss: 0.003440560402920516, Validation loss: 0.008060260765869446\n",
      "iterations:  2006\n",
      "Training loss: 0.0034365721997366757, Validation loss: 0.007880669097275004\n",
      "iterations:  2007\n",
      "Training loss: 0.0034333865035766605, Validation loss: 0.008021838710825801\n",
      "iterations:  2008\n",
      "Training loss: 0.0034296796156344455, Validation loss: 0.007882092815756848\n",
      "iterations:  2009\n",
      "Training loss: 0.0034263629272461914, Validation loss: 0.007997593257336007\n",
      "iterations:  2010\n",
      "Training loss: 0.003422997887200071, Validation loss: 0.007873815232007201\n",
      "iterations:  2011\n",
      "Training loss: 0.0034196928747114008, Validation loss: 0.00799100904222803\n",
      "iterations:  2012\n",
      "Training loss: 0.0034170194921191747, Validation loss: 0.00784048319417591\n",
      "new best w\n",
      "iterations:  2013\n",
      "Training loss: 0.003413683830309368, Validation loss: 0.007997857299370482\n",
      "iterations:  2014\n",
      "Training loss: 0.0034102570571480934, Validation loss: 0.007836966428822503\n",
      "new best w\n",
      "iterations:  2015\n",
      "Training loss: 0.0034073490122971217, Validation loss: 0.007969476425280533\n",
      "iterations:  2016\n",
      "Training loss: 0.003403548016763472, Validation loss: 0.007829178775846116\n",
      "new best w\n",
      "iterations:  2017\n",
      "Training loss: 0.003401001944043339, Validation loss: 0.007950301129668411\n",
      "iterations:  2018\n",
      "Training loss: 0.0033970707706789637, Validation loss: 0.007814096415605109\n",
      "new best w\n",
      "iterations:  2019\n",
      "Training loss: 0.0033944838160641093, Validation loss: 0.007944236674246978\n",
      "iterations:  2020\n",
      "Training loss: 0.0033912011320167785, Validation loss: 0.007780216289255133\n",
      "new best w\n",
      "iterations:  2021\n",
      "Training loss: 0.003387286953006371, Validation loss: 0.007943324510959459\n",
      "iterations:  2022\n",
      "Training loss: 0.0033845274105960576, Validation loss: 0.007776805748984256\n",
      "new best w\n",
      "iterations:  2023\n",
      "Training loss: 0.0033811982865398073, Validation loss: 0.007931898224002405\n",
      "iterations:  2024\n",
      "Training loss: 0.0033778907370232538, Validation loss: 0.00777343867246486\n",
      "new best w\n",
      "iterations:  2025\n",
      "Training loss: 0.003375339780637367, Validation loss: 0.007920052535404695\n",
      "iterations:  2026\n",
      "Training loss: 0.00337193022463894, Validation loss: 0.007742918019787356\n",
      "new best w\n",
      "iterations:  2027\n",
      "Training loss: 0.003369097413147446, Validation loss: 0.007914947899711368\n",
      "iterations:  2028\n",
      "Training loss: 0.0033662061429672184, Validation loss: 0.007711950701260646\n",
      "new best w\n",
      "iterations:  2029\n",
      "Training loss: 0.003362949168315225, Validation loss: 0.007908364327298112\n",
      "iterations:  2030\n",
      "Training loss: 0.003359188509287012, Validation loss: 0.0077236376280229325\n",
      "iterations:  2031\n",
      "Training loss: 0.0033565323090892006, Validation loss: 0.00787262198242626\n",
      "iterations:  2032\n",
      "Training loss: 0.0033525682660928663, Validation loss: 0.007725086181136062\n",
      "iterations:  2033\n",
      "Training loss: 0.003349323801687095, Validation loss: 0.007854869196840497\n",
      "iterations:  2034\n",
      "Training loss: 0.003346365360582506, Validation loss: 0.007701529630932384\n",
      "new best w\n",
      "iterations:  2035\n",
      "Training loss: 0.00334317264546555, Validation loss: 0.00785708292741859\n",
      "iterations:  2036\n",
      "Training loss: 0.0033402219132437827, Validation loss: 0.007686852252386087\n",
      "new best w\n",
      "iterations:  2037\n",
      "Training loss: 0.003336998029156497, Validation loss: 0.007829351557754797\n",
      "iterations:  2038\n",
      "Training loss: 0.003333524395229435, Validation loss: 0.0077039471355236835\n",
      "iterations:  2039\n",
      "Training loss: 0.0033309384569488476, Validation loss: 0.0077936005750655875\n",
      "iterations:  2040\n",
      "Training loss: 0.00332711548272892, Validation loss: 0.007695508705150013\n",
      "iterations:  2041\n",
      "Training loss: 0.0033246123221000023, Validation loss: 0.007793439347802873\n",
      "iterations:  2042\n",
      "Training loss: 0.0033214036505064266, Validation loss: 0.007651091432181891\n",
      "new best w\n",
      "iterations:  2043\n",
      "Training loss: 0.003317816118155531, Validation loss: 0.0078006438908194906\n",
      "iterations:  2044\n",
      "Training loss: 0.0033151650887573094, Validation loss: 0.007641490332488105\n",
      "new best w\n",
      "iterations:  2045\n",
      "Training loss: 0.0033123506654172685, Validation loss: 0.007794327718385646\n",
      "iterations:  2046\n",
      "Training loss: 0.0033089401156732676, Validation loss: 0.0076326576495288805\n",
      "new best w\n",
      "iterations:  2047\n",
      "Training loss: 0.003306403247025071, Validation loss: 0.007781619161928192\n",
      "iterations:  2048\n",
      "Training loss: 0.0033031313099663364, Validation loss: 0.007606027198570109\n",
      "new best w\n",
      "iterations:  2049\n",
      "Training loss: 0.0033004387957468423, Validation loss: 0.007776894572811302\n",
      "iterations:  2050\n",
      "Training loss: 0.0032970908776881135, Validation loss: 0.007593191446167958\n",
      "new best w\n",
      "iterations:  2051\n",
      "Training loss: 0.0032941866492824236, Validation loss: 0.007746736848798496\n",
      "iterations:  2052\n",
      "Training loss: 0.003290466706853493, Validation loss: 0.007612500177906323\n",
      "iterations:  2053\n",
      "Training loss: 0.003287735089290896, Validation loss: 0.007707369230847141\n",
      "iterations:  2054\n",
      "Training loss: 0.00328419768884834, Validation loss: 0.007610434337184398\n",
      "iterations:  2055\n",
      "Training loss: 0.0032812525661848474, Validation loss: 0.0077043101152549035\n",
      "iterations:  2056\n",
      "Training loss: 0.0032784696274236543, Validation loss: 0.007570902560188932\n",
      "new best w\n",
      "iterations:  2057\n",
      "Training loss: 0.0032754575077504743, Validation loss: 0.007716487422665457\n",
      "iterations:  2058\n",
      "Training loss: 0.0032726166336749644, Validation loss: 0.007551475737593292\n",
      "new best w\n",
      "iterations:  2059\n",
      "Training loss: 0.003270007195733265, Validation loss: 0.007710146805316356\n",
      "iterations:  2060\n",
      "Training loss: 0.003266412222458516, Validation loss: 0.00754629484705129\n",
      "new best w\n",
      "iterations:  2061\n",
      "Training loss: 0.0032643465648940838, Validation loss: 0.0076958166865943745\n",
      "iterations:  2062\n",
      "Training loss: 0.003260873794802242, Validation loss: 0.007516966377652456\n",
      "new best w\n",
      "iterations:  2063\n",
      "Training loss: 0.003257482341139774, Validation loss: 0.007686913225296496\n",
      "iterations:  2064\n",
      "Training loss: 0.0032547164902944938, Validation loss: 0.007515265323905891\n",
      "new best w\n",
      "iterations:  2065\n",
      "Training loss: 0.0032514628756920715, Validation loss: 0.007663315873005745\n",
      "iterations:  2066\n",
      "Training loss: 0.003248365578615242, Validation loss: 0.007526727793932113\n",
      "iterations:  2067\n",
      "Training loss: 0.003245825635399988, Validation loss: 0.007639456277585275\n",
      "iterations:  2068\n",
      "Training loss: 0.0032425575380405193, Validation loss: 0.00750741103616135\n",
      "new best w\n",
      "iterations:  2069\n",
      "Training loss: 0.003240287312139188, Validation loss: 0.00764575276317248\n",
      "iterations:  2070\n",
      "Training loss: 0.003237045109652833, Validation loss: 0.007476863975810484\n",
      "new best w\n",
      "iterations:  2071\n",
      "Training loss: 0.0032342631020298076, Validation loss: 0.007623591267771027\n",
      "iterations:  2072\n",
      "Training loss: 0.0032307543041761883, Validation loss: 0.007493322106090851\n",
      "iterations:  2073\n",
      "Training loss: 0.00322780348547054, Validation loss: 0.007597240043157013\n",
      "iterations:  2074\n",
      "Training loss: 0.003224672081570158, Validation loss: 0.007478534569694777\n",
      "iterations:  2075\n",
      "Training loss: 0.003222046831554446, Validation loss: 0.007599582005620641\n",
      "iterations:  2076\n",
      "Training loss: 0.003219528713918703, Validation loss: 0.007438346306441523\n",
      "new best w\n",
      "iterations:  2077\n",
      "Training loss: 0.0032166531501240923, Validation loss: 0.007605126816878735\n",
      "iterations:  2078\n",
      "Training loss: 0.00321345487577389, Validation loss: 0.007438311118138255\n",
      "new best w\n",
      "iterations:  2079\n",
      "Training loss: 0.00321142831517317, Validation loss: 0.007590869522425855\n",
      "iterations:  2080\n",
      "Training loss: 0.003207702519398568, Validation loss: 0.007423382626548583\n",
      "new best w\n",
      "iterations:  2081\n",
      "Training loss: 0.0032047528813171694, Validation loss: 0.0075755782667916525\n",
      "iterations:  2082\n",
      "Training loss: 0.0032023878088023494, Validation loss: 0.007396461060075932\n",
      "new best w\n",
      "iterations:  2083\n",
      "Training loss: 0.003199133481069894, Validation loss: 0.00757370046778313\n",
      "iterations:  2084\n",
      "Training loss: 0.003196237306285779, Validation loss: 0.007404447996039105\n",
      "iterations:  2085\n",
      "Training loss: 0.0031939225899735076, Validation loss: 0.0075562330516745334\n",
      "iterations:  2086\n",
      "Training loss: 0.003190498209202307, Validation loss: 0.007390961709012564\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2087\n",
      "Training loss: 0.0031884238742408665, Validation loss: 0.007547118520591751\n",
      "iterations:  2088\n",
      "Training loss: 0.0031855681908140926, Validation loss: 0.007351618715619938\n",
      "new best w\n",
      "iterations:  2089\n",
      "Training loss: 0.003182473535119549, Validation loss: 0.007543406106844801\n",
      "iterations:  2090\n",
      "Training loss: 0.003179374987859581, Validation loss: 0.007363822368358963\n",
      "iterations:  2091\n",
      "Training loss: 0.003176752165385934, Validation loss: 0.007521784940184709\n",
      "iterations:  2092\n",
      "Training loss: 0.00317346942256784, Validation loss: 0.007357004038617473\n",
      "iterations:  2093\n",
      "Training loss: 0.0031710941868070388, Validation loss: 0.007510753367765322\n",
      "iterations:  2094\n",
      "Training loss: 0.0031684976388380954, Validation loss: 0.0073204712200551384\n",
      "new best w\n",
      "iterations:  2095\n",
      "Training loss: 0.0031657534550583104, Validation loss: 0.007509929698403567\n",
      "iterations:  2096\n",
      "Training loss: 0.0031624652418385944, Validation loss: 0.007326810747767355\n",
      "iterations:  2097\n",
      "Training loss: 0.0031600074268656048, Validation loss: 0.0074885697902224045\n",
      "iterations:  2098\n",
      "Training loss: 0.003156735023179774, Validation loss: 0.00732065512078073\n",
      "iterations:  2099\n",
      "Training loss: 0.003154064524503849, Validation loss: 0.007475484265889352\n",
      "iterations:  2100\n",
      "Training loss: 0.003151693671929975, Validation loss: 0.0072880764895251705\n",
      "new best w\n",
      "iterations:  2101\n",
      "Training loss: 0.0031486630773350168, Validation loss: 0.007473488037166314\n",
      "iterations:  2102\n",
      "Training loss: 0.0031456897732729146, Validation loss: 0.007295989212691934\n",
      "iterations:  2103\n",
      "Training loss: 0.003143581730947456, Validation loss: 0.0074557797740978675\n",
      "iterations:  2104\n",
      "Training loss: 0.003140128992863305, Validation loss: 0.007283270888008575\n",
      "new best w\n",
      "iterations:  2105\n",
      "Training loss: 0.003137571472909454, Validation loss: 0.0074429164363985405\n",
      "iterations:  2106\n",
      "Training loss: 0.00313475208435767, Validation loss: 0.007268398592735853\n",
      "new best w\n",
      "iterations:  2107\n",
      "Training loss: 0.003131638328428476, Validation loss: 0.007416047589391679\n",
      "iterations:  2108\n",
      "Training loss: 0.00312864493603916, Validation loss: 0.007288506844040898\n",
      "iterations:  2109\n",
      "Training loss: 0.003126348662975019, Validation loss: 0.0073945857599585175\n",
      "iterations:  2110\n",
      "Training loss: 0.00312326003098758, Validation loss: 0.007265466313310431\n",
      "new best w\n",
      "iterations:  2111\n",
      "Training loss: 0.0031210233927958, Validation loss: 0.007401894447709954\n",
      "iterations:  2112\n",
      "Training loss: 0.0031181445938355677, Validation loss: 0.007236846844796252\n",
      "new best w\n",
      "iterations:  2113\n",
      "Training loss: 0.0031149332211537974, Validation loss: 0.0073836344287488715\n",
      "iterations:  2114\n",
      "Training loss: 0.0031121496757728195, Validation loss: 0.007253890724308281\n",
      "iterations:  2115\n",
      "Training loss: 0.0031098188640463482, Validation loss: 0.007373188752397576\n",
      "iterations:  2116\n",
      "Training loss: 0.0031072277857069615, Validation loss: 0.007217143579218187\n",
      "new best w\n",
      "iterations:  2117\n",
      "Training loss: 0.0031047280838447135, Validation loss: 0.007376633922703523\n",
      "iterations:  2118\n",
      "Training loss: 0.003101733593625244, Validation loss: 0.007211781780973947\n",
      "new best w\n",
      "iterations:  2119\n",
      "Training loss: 0.003099837280639542, Validation loss: 0.007364347888609395\n",
      "iterations:  2120\n",
      "Training loss: 0.0030965012522296827, Validation loss: 0.0071968521579818655\n",
      "new best w\n",
      "iterations:  2121\n",
      "Training loss: 0.0030937929979104922, Validation loss: 0.007351629987949579\n",
      "iterations:  2122\n",
      "Training loss: 0.003091551962661096, Validation loss: 0.00716879407945491\n",
      "new best w\n",
      "iterations:  2123\n",
      "Training loss: 0.0030886519749265743, Validation loss: 0.007349883076429331\n",
      "iterations:  2124\n",
      "Training loss: 0.0030858707033708903, Validation loss: 0.007175387512250997\n",
      "iterations:  2125\n",
      "Training loss: 0.0030838425988494, Validation loss: 0.007333103010629648\n",
      "iterations:  2126\n",
      "Training loss: 0.003080605366733289, Validation loss: 0.007162160141456928\n",
      "new best w\n",
      "iterations:  2127\n",
      "Training loss: 0.003078095744553125, Validation loss: 0.007321423605136802\n",
      "iterations:  2128\n",
      "Training loss: 0.00307547774036876, Validation loss: 0.007146425745096536\n",
      "new best w\n",
      "iterations:  2129\n",
      "Training loss: 0.0030729113577363763, Validation loss: 0.007312391805347254\n",
      "iterations:  2130\n",
      "Training loss: 0.003069805381356438, Validation loss: 0.007150162860528808\n",
      "iterations:  2131\n",
      "Training loss: 0.003067664711829008, Validation loss: 0.00729755774590803\n",
      "iterations:  2132\n",
      "Training loss: 0.0030650544566114078, Validation loss: 0.007116007954900648\n",
      "new best w\n",
      "iterations:  2133\n",
      "Training loss: 0.003062604181613588, Validation loss: 0.007296944829273562\n",
      "iterations:  2134\n",
      "Training loss: 0.0030596675315081963, Validation loss: 0.007118353903000037\n",
      "iterations:  2135\n",
      "Training loss: 0.0030570363474414462, Validation loss: 0.007276350166735746\n",
      "iterations:  2136\n",
      "Training loss: 0.003054167252397463, Validation loss: 0.007114729154798711\n",
      "new best w\n",
      "iterations:  2137\n",
      "Training loss: 0.003051811204746114, Validation loss: 0.007266377652833174\n",
      "iterations:  2138\n",
      "Training loss: 0.0030491860123268238, Validation loss: 0.007094960414197946\n",
      "new best w\n",
      "iterations:  2139\n",
      "Training loss: 0.0030468677439096075, Validation loss: 0.007250641945438006\n",
      "iterations:  2140\n",
      "Training loss: 0.0030436813544728247, Validation loss: 0.007106228696633771\n",
      "iterations:  2141\n",
      "Training loss: 0.0030410185651087453, Validation loss: 0.007228615341979766\n",
      "iterations:  2142\n",
      "Training loss: 0.0030386977676844563, Validation loss: 0.0070797526228623025\n",
      "new best w\n",
      "iterations:  2143\n",
      "Training loss: 0.003036105699345818, Validation loss: 0.007233832663294669\n",
      "iterations:  2144\n",
      "Training loss: 0.0030334896782710868, Validation loss: 0.00707412187705882\n",
      "new best w\n",
      "iterations:  2145\n",
      "Training loss: 0.0030316060281964136, Validation loss: 0.007223570332436648\n",
      "iterations:  2146\n",
      "Training loss: 0.0030285115490530463, Validation loss: 0.007057670586675613\n",
      "new best w\n",
      "iterations:  2147\n",
      "Training loss: 0.00302613657187734, Validation loss: 0.007213941387965819\n",
      "iterations:  2148\n",
      "Training loss: 0.00302362784853425, Validation loss: 0.007040854117896367\n",
      "new best w\n",
      "iterations:  2149\n",
      "Training loss: 0.0030212089547044514, Validation loss: 0.007205818076333021\n",
      "iterations:  2150\n",
      "Training loss: 0.0030181220246549624, Validation loss: 0.007047674562612774\n",
      "iterations:  2151\n",
      "Training loss: 0.003016103423122954, Validation loss: 0.007190299942469622\n",
      "iterations:  2152\n",
      "Training loss: 0.003013705874702473, Validation loss: 0.0070072890535889856\n",
      "new best w\n",
      "iterations:  2153\n",
      "Training loss: 0.0030109730946665265, Validation loss: 0.007190163181313403\n",
      "iterations:  2154\n",
      "Training loss: 0.0030083975720797125, Validation loss: 0.007015177437017438\n",
      "iterations:  2155\n",
      "Training loss: 0.003006041856054681, Validation loss: 0.007171640093855206\n",
      "iterations:  2156\n",
      "Training loss: 0.0030032752086696125, Validation loss: 0.007006776359819828\n",
      "new best w\n",
      "iterations:  2157\n",
      "Training loss: 0.003000916029481793, Validation loss: 0.007161925612258491\n",
      "iterations:  2158\n",
      "Training loss: 0.0029981604083451512, Validation loss: 0.006999270324017821\n",
      "new best w\n",
      "iterations:  2159\n",
      "Training loss: 0.002996076469778456, Validation loss: 0.007150014966224529\n",
      "iterations:  2160\n",
      "Training loss: 0.0029932631832362364, Validation loss: 0.006987657542330583\n",
      "new best w\n",
      "iterations:  2161\n",
      "Training loss: 0.0029908630725411266, Validation loss: 0.007140837512319515\n",
      "iterations:  2162\n",
      "Training loss: 0.0029886594290245384, Validation loss: 0.006961246500092562\n",
      "new best w\n",
      "iterations:  2163\n",
      "Training loss: 0.002986197309726189, Validation loss: 0.007137248353615849\n",
      "iterations:  2164\n",
      "Training loss: 0.0029833086449280693, Validation loss: 0.006970274012068761\n",
      "iterations:  2165\n",
      "Training loss: 0.0029811085235063595, Validation loss: 0.007118207803673421\n",
      "iterations:  2166\n",
      "Training loss: 0.0029787914585134175, Validation loss: 0.006944582644044374\n",
      "new best w\n",
      "iterations:  2167\n",
      "Training loss: 0.0029761189711975233, Validation loss: 0.007114047353968736\n",
      "iterations:  2168\n",
      "Training loss: 0.002973359613826357, Validation loss: 0.00695869109995498\n",
      "iterations:  2169\n",
      "Training loss: 0.0029714403595692716, Validation loss: 0.007095822909709937\n",
      "iterations:  2170\n",
      "Training loss: 0.002968851141975552, Validation loss: 0.0069256678680069145\n",
      "new best w\n",
      "iterations:  2171\n",
      "Training loss: 0.0029663965614377557, Validation loss: 0.00709412772022794\n",
      "iterations:  2172\n",
      "Training loss: 0.002963882415230561, Validation loss: 0.0069259339673385795\n",
      "iterations:  2173\n",
      "Training loss: 0.0029617114328252215, Validation loss: 0.007079594979337746\n",
      "iterations:  2174\n",
      "Training loss: 0.002958977510553216, Validation loss: 0.006915526336952245\n",
      "new best w\n",
      "iterations:  2175\n",
      "Training loss: 0.0029567889156605037, Validation loss: 0.007071004573096397\n",
      "iterations:  2176\n",
      "Training loss: 0.0029540832562242785, Validation loss: 0.006907229357447403\n",
      "new best w\n",
      "iterations:  2177\n",
      "Training loss: 0.0029517184523266456, Validation loss: 0.007056724573614881\n",
      "iterations:  2178\n",
      "Training loss: 0.0029491502409791383, Validation loss: 0.006901176879684108\n",
      "new best w\n",
      "iterations:  2179\n",
      "Training loss: 0.0029469180134360827, Validation loss: 0.007048932269202095\n",
      "iterations:  2180\n",
      "Training loss: 0.0029446010150537398, Validation loss: 0.006880595121490122\n",
      "new best w\n",
      "iterations:  2181\n",
      "Training loss: 0.002942704581548998, Validation loss: 0.007044337458284554\n",
      "iterations:  2182\n",
      "Training loss: 0.002939731742428166, Validation loss: 0.006881301413410862\n",
      "iterations:  2183\n",
      "Training loss: 0.0029372392716923157, Validation loss: 0.007025805686993904\n",
      "iterations:  2184\n",
      "Training loss: 0.002935203489892747, Validation loss: 0.006855806039769724\n",
      "new best w\n",
      "iterations:  2185\n",
      "Training loss: 0.002932827835812474, Validation loss: 0.00702482184553681\n",
      "iterations:  2186\n",
      "Training loss: 0.0029301406155534844, Validation loss: 0.006864053774951785\n",
      "iterations:  2187\n",
      "Training loss: 0.00292833098578585, Validation loss: 0.0070090114907609994\n",
      "iterations:  2188\n",
      "Training loss: 0.0029258968123016347, Validation loss: 0.006836740981902019\n",
      "new best w\n",
      "iterations:  2189\n",
      "Training loss: 0.0029233079259501334, Validation loss: 0.007002394825320557\n",
      "iterations:  2190\n",
      "Training loss: 0.002920729607619687, Validation loss: 0.006845072078950222\n",
      "iterations:  2191\n",
      "Training loss: 0.0029187715951971395, Validation loss: 0.006988052237658526\n",
      "iterations:  2192\n",
      "Training loss: 0.0029164276443818427, Validation loss: 0.006815633588698825\n",
      "new best w\n",
      "iterations:  2193\n",
      "Training loss: 0.002914044781282809, Validation loss: 0.006985088562515572\n",
      "iterations:  2194\n",
      "Training loss: 0.0029114472267774993, Validation loss: 0.006828250533226698\n",
      "iterations:  2195\n",
      "Training loss: 0.0029092057661190135, Validation loss: 0.006965033402461893\n",
      "iterations:  2196\n",
      "Training loss: 0.0029069750635840504, Validation loss: 0.0068007932239058655\n",
      "new best w\n",
      "iterations:  2197\n",
      "Training loss: 0.0029048898732386068, Validation loss: 0.006965903520452617\n",
      "iterations:  2198\n",
      "Training loss: 0.0029021799554298684, Validation loss: 0.006807668567582065\n",
      "iterations:  2199\n",
      "Training loss: 0.00289989833848949, Validation loss: 0.006945315311990998\n",
      "iterations:  2200\n",
      "Training loss: 0.0028977267349044188, Validation loss: 0.006782732074237982\n",
      "new best w\n",
      "iterations:  2201\n",
      "Training loss: 0.002895441635431516, Validation loss: 0.006944964009477604\n",
      "iterations:  2202\n",
      "Training loss: 0.002892823722772034, Validation loss: 0.00679147613449381\n",
      "iterations:  2203\n",
      "Training loss: 0.0028909249755516026, Validation loss: 0.006927380795382599\n",
      "iterations:  2204\n",
      "Training loss: 0.0028887132549358903, Validation loss: 0.006761070986441141\n",
      "new best w\n",
      "iterations:  2205\n",
      "Training loss: 0.0028861284271166814, Validation loss: 0.006924795309146951\n",
      "iterations:  2206\n",
      "Training loss: 0.0028836461699023536, Validation loss: 0.006774498216086865\n",
      "iterations:  2207\n",
      "Training loss: 0.0028820058555656335, Validation loss: 0.006909462179357564\n",
      "iterations:  2208\n",
      "Training loss: 0.00287968499387315, Validation loss: 0.00673960254409451\n",
      "new best w\n",
      "iterations:  2209\n",
      "Training loss: 0.0028770368420249063, Validation loss: 0.006905804568489335\n",
      "iterations:  2210\n",
      "Training loss: 0.002874619683542057, Validation loss: 0.006755568791013851\n",
      "iterations:  2211\n",
      "Training loss: 0.0028727775552380825, Validation loss: 0.0068891519784105015\n",
      "iterations:  2212\n",
      "Training loss: 0.002870531703659874, Validation loss: 0.006723027566628447\n",
      "new best w\n",
      "iterations:  2213\n",
      "Training loss: 0.0028682756464661545, Validation loss: 0.0068882569477576715\n",
      "iterations:  2214\n",
      "Training loss: 0.002865817580273058, Validation loss: 0.006733440029263271\n",
      "iterations:  2215\n",
      "Training loss: 0.002863656477811768, Validation loss: 0.006869415804986989\n",
      "iterations:  2216\n",
      "Training loss: 0.002861530627838304, Validation loss: 0.006705938222243508\n",
      "new best w\n",
      "iterations:  2217\n",
      "Training loss: 0.0028594608353143497, Validation loss: 0.0068698899614559215\n",
      "iterations:  2218\n",
      "Training loss: 0.002856952015987667, Validation loss: 0.006713046334063802\n",
      "iterations:  2219\n",
      "Training loss: 0.002854748973661297, Validation loss: 0.006850721592617172\n",
      "iterations:  2220\n",
      "Training loss: 0.0028525811890582355, Validation loss: 0.006692543438242797\n",
      "new best w\n",
      "iterations:  2221\n",
      "Training loss: 0.0028505952147304813, Validation loss: 0.0068487062820265536\n",
      "iterations:  2222\n",
      "Training loss: 0.002847930356471057, Validation loss: 0.006700382848251548\n",
      "iterations:  2223\n",
      "Training loss: 0.0028456601294103855, Validation loss: 0.006829163314260002\n",
      "iterations:  2224\n",
      "Training loss: 0.00284361320969789, Validation loss: 0.006678392063301911\n",
      "new best w\n",
      "iterations:  2225\n",
      "Training loss: 0.002841731437090766, Validation loss: 0.006828753479925348\n",
      "iterations:  2226\n",
      "Training loss: 0.002839152026901345, Validation loss: 0.006676460330681411\n",
      "new best w\n",
      "iterations:  2227\n",
      "Training loss: 0.0028372084495273224, Validation loss: 0.006815995375459785\n",
      "iterations:  2228\n",
      "Training loss: 0.0028349419908410556, Validation loss: 0.006663683453404687\n",
      "new best w\n",
      "iterations:  2229\n",
      "Training loss: 0.0028329491591779156, Validation loss: 0.0068072740982566965\n",
      "iterations:  2230\n",
      "Training loss: 0.0028305191279219145, Validation loss: 0.006651795986470097\n",
      "new best w\n",
      "iterations:  2231\n",
      "Training loss: 0.00282870998815281, Validation loss: 0.00680181781202496\n",
      "iterations:  2232\n",
      "Training loss: 0.0028264123746244087, Validation loss: 0.006640364890370299\n",
      "new best w\n",
      "iterations:  2233\n",
      "Training loss: 0.002824346750879195, Validation loss: 0.006790320502097434\n",
      "iterations:  2234\n",
      "Training loss: 0.002821994805520976, Validation loss: 0.006631342631645072\n",
      "new best w\n",
      "iterations:  2235\n",
      "Training loss: 0.0028200370991723566, Validation loss: 0.006783151496462326\n",
      "iterations:  2236\n",
      "Training loss: 0.00281759546704228, Validation loss: 0.006633651689916817\n",
      "iterations:  2237\n",
      "Training loss: 0.002815534364183581, Validation loss: 0.006766645725648314\n",
      "iterations:  2238\n",
      "Training loss: 0.0028134882452284437, Validation loss: 0.006607471534575973\n",
      "new best w\n",
      "iterations:  2239\n",
      "Training loss: 0.0028114817009858733, Validation loss: 0.006767519017686838\n",
      "iterations:  2240\n",
      "Training loss: 0.0028090774616906945, Validation loss: 0.006613676327196683\n",
      "iterations:  2241\n",
      "Training loss: 0.0028068834458552677, Validation loss: 0.006748305780725815\n",
      "iterations:  2242\n",
      "Training loss: 0.0028048923976854686, Validation loss: 0.006595022319107973\n",
      "new best w\n",
      "iterations:  2243\n",
      "Training loss: 0.002802981689583097, Validation loss: 0.006746578642044247\n",
      "iterations:  2244\n",
      "Training loss: 0.0028004991033469422, Validation loss: 0.006601414885982089\n",
      "iterations:  2245\n",
      "Training loss: 0.002798284183141374, Validation loss: 0.006728046817094648\n",
      "iterations:  2246\n",
      "Training loss: 0.0027962259603693893, Validation loss: 0.00658917967748601\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2247\n",
      "Training loss: 0.0027945523045518118, Validation loss: 0.006724478929122486\n",
      "iterations:  2248\n",
      "Training loss: 0.0027922365873777246, Validation loss: 0.0065708673020042395\n",
      "new best w\n",
      "iterations:  2249\n",
      "Training loss: 0.0027899108052333085, Validation loss: 0.006715884624549746\n",
      "iterations:  2250\n",
      "Training loss: 0.0027877772764653207, Validation loss: 0.006578495604675279\n",
      "iterations:  2251\n",
      "Training loss: 0.0027860856368588675, Validation loss: 0.0067038186376779385\n",
      "iterations:  2252\n",
      "Training loss: 0.002783951950991824, Validation loss: 0.006549995257626823\n",
      "new best w\n",
      "iterations:  2253\n",
      "Training loss: 0.0027816855945753615, Validation loss: 0.006699606914392206\n",
      "iterations:  2254\n",
      "Training loss: 0.0027794668442534414, Validation loss: 0.0065630257046769115\n",
      "iterations:  2255\n",
      "Training loss: 0.0027776944473205055, Validation loss: 0.006685289928751696\n",
      "iterations:  2256\n",
      "Training loss: 0.0027755126719100657, Validation loss: 0.006542503108479726\n",
      "new best w\n",
      "iterations:  2257\n",
      "Training loss: 0.0027735518052141167, Validation loss: 0.006678804473557407\n",
      "iterations:  2258\n",
      "Training loss: 0.0027714425123698283, Validation loss: 0.0065310723418883524\n",
      "new best w\n",
      "iterations:  2259\n",
      "Training loss: 0.0027695138797053124, Validation loss: 0.00667369253748594\n",
      "iterations:  2260\n",
      "Training loss: 0.0027672619110861237, Validation loss: 0.0065310374490433355\n",
      "new best w\n",
      "iterations:  2261\n",
      "Training loss: 0.0027652401658635143, Validation loss: 0.006658338381222669\n",
      "iterations:  2262\n",
      "Training loss: 0.0027633212440738163, Validation loss: 0.006510390908443649\n",
      "new best w\n",
      "iterations:  2263\n",
      "Training loss: 0.002761486885719876, Validation loss: 0.006657759647939405\n",
      "iterations:  2264\n",
      "Training loss: 0.002759145661255601, Validation loss: 0.0065151077334263995\n",
      "iterations:  2265\n",
      "Training loss: 0.0027570425170871256, Validation loss: 0.00664023815367006\n",
      "iterations:  2266\n",
      "Training loss: 0.0027550780624771253, Validation loss: 0.006502308639833096\n",
      "new best w\n",
      "iterations:  2267\n",
      "Training loss: 0.002753463869777562, Validation loss: 0.006637025586219832\n",
      "iterations:  2268\n",
      "Training loss: 0.0027512835805949885, Validation loss: 0.006483778889515471\n",
      "new best w\n",
      "iterations:  2269\n",
      "Training loss: 0.0027490646242059936, Validation loss: 0.006628887462569748\n",
      "iterations:  2270\n",
      "Training loss: 0.0027470112625613352, Validation loss: 0.006490345045772639\n",
      "iterations:  2271\n",
      "Training loss: 0.002745358470691202, Validation loss: 0.006617330093030839\n",
      "iterations:  2272\n",
      "Training loss: 0.002743384489498917, Validation loss: 0.0064622619122901475\n",
      "new best w\n",
      "iterations:  2273\n",
      "Training loss: 0.002741222661875237, Validation loss: 0.006613308113470532\n",
      "iterations:  2274\n",
      "Training loss: 0.002739056896456268, Validation loss: 0.006474479929797776\n",
      "iterations:  2275\n",
      "Training loss: 0.0027373590360788964, Validation loss: 0.0065994347100749706\n",
      "iterations:  2276\n",
      "Training loss: 0.0027351738222335083, Validation loss: 0.006464614754283368\n",
      "iterations:  2277\n",
      "Training loss: 0.0027332623913518036, Validation loss: 0.006589052502317562\n",
      "iterations:  2278\n",
      "Training loss: 0.0027313175320457196, Validation loss: 0.006441715398872993\n",
      "new best w\n",
      "iterations:  2279\n",
      "Training loss: 0.0027296014556451886, Validation loss: 0.006589449808244956\n",
      "iterations:  2280\n",
      "Training loss: 0.0027273409754727516, Validation loss: 0.006445818882889564\n",
      "iterations:  2281\n",
      "Training loss: 0.0027253603137136003, Validation loss: 0.006572961862780538\n",
      "iterations:  2282\n",
      "Training loss: 0.0027233751629313803, Validation loss: 0.006432095074792699\n",
      "new best w\n",
      "iterations:  2283\n",
      "Training loss: 0.0027218447567718246, Validation loss: 0.006569673104487052\n",
      "iterations:  2284\n",
      "Training loss: 0.002719759093596256, Validation loss: 0.006414288731174107\n",
      "new best w\n",
      "iterations:  2285\n",
      "Training loss: 0.0027176861681592495, Validation loss: 0.0065620455299448665\n",
      "iterations:  2286\n",
      "Training loss: 0.002715486492884266, Validation loss: 0.006427835211328646\n",
      "iterations:  2287\n",
      "Training loss: 0.0027138801530308928, Validation loss: 0.00654686970891605\n",
      "iterations:  2288\n",
      "Training loss: 0.0027118567561925295, Validation loss: 0.006405858325940331\n",
      "new best w\n",
      "iterations:  2289\n",
      "Training loss: 0.002709930422668913, Validation loss: 0.0065417241618672085\n",
      "iterations:  2290\n",
      "Training loss: 0.002707886542886549, Validation loss: 0.006394465257322302\n",
      "new best w\n",
      "iterations:  2291\n",
      "Training loss: 0.002705790575348514, Validation loss: 0.006533744656872185\n",
      "iterations:  2292\n",
      "Training loss: 0.002703772543990255, Validation loss: 0.006408283268742217\n",
      "iterations:  2293\n",
      "Training loss: 0.0027021550728045245, Validation loss: 0.006519555726828831\n",
      "iterations:  2294\n",
      "Training loss: 0.0027001790203724178, Validation loss: 0.006384589409041161\n",
      "new best w\n",
      "iterations:  2295\n",
      "Training loss: 0.002698321545725589, Validation loss: 0.006515333975365444\n",
      "iterations:  2296\n",
      "Training loss: 0.0026963882347903, Validation loss: 0.0063719867988288785\n",
      "new best w\n",
      "iterations:  2297\n",
      "Training loss: 0.0026947057800296302, Validation loss: 0.006511455231454999\n",
      "iterations:  2298\n",
      "Training loss: 0.0026925684486279605, Validation loss: 0.006378167985770735\n",
      "iterations:  2299\n",
      "Training loss: 0.0026907069980558053, Validation loss: 0.006494883409425877\n",
      "iterations:  2300\n",
      "Training loss: 0.0026887753862014185, Validation loss: 0.0063605692680312785\n",
      "new best w\n",
      "iterations:  2301\n",
      "Training loss: 0.0026872546705744666, Validation loss: 0.0064931449164653725\n",
      "iterations:  2302\n",
      "Training loss: 0.002685300584215628, Validation loss: 0.006342705814481394\n",
      "new best w\n",
      "iterations:  2303\n",
      "Training loss: 0.0026832432656800163, Validation loss: 0.0064856994519022675\n",
      "iterations:  2304\n",
      "Training loss: 0.002681151927942953, Validation loss: 0.006355936372775851\n",
      "iterations:  2305\n",
      "Training loss: 0.002679204769797871, Validation loss: 0.00646791968556718\n",
      "iterations:  2306\n",
      "Training loss: 0.0026774263502876133, Validation loss: 0.0063401513952175805\n",
      "new best w\n",
      "iterations:  2307\n",
      "Training loss: 0.002675911609925133, Validation loss: 0.006467032588008829\n",
      "iterations:  2308\n",
      "Training loss: 0.0026739541360469195, Validation loss: 0.006325032773561198\n",
      "new best w\n",
      "iterations:  2309\n",
      "Training loss: 0.0026720611997546606, Validation loss: 0.006458602811811669\n",
      "iterations:  2310\n",
      "Training loss: 0.0026700344251336812, Validation loss: 0.00632952880921632\n",
      "iterations:  2311\n",
      "Training loss: 0.0026684438733880923, Validation loss: 0.006447788727194832\n",
      "iterations:  2312\n",
      "Training loss: 0.0026664867616741617, Validation loss: 0.006319111130450037\n",
      "new best w\n",
      "iterations:  2313\n",
      "Training loss: 0.0026646208401727117, Validation loss: 0.006438618462342391\n",
      "iterations:  2314\n",
      "Training loss: 0.00266275465965357, Validation loss: 0.006303759960242693\n",
      "new best w\n",
      "iterations:  2315\n",
      "Training loss: 0.0026608899014071373, Validation loss: 0.006432977176537303\n",
      "iterations:  2316\n",
      "Training loss: 0.002658999225042872, Validation loss: 0.006301744770389503\n",
      "new best w\n",
      "iterations:  2317\n",
      "Training loss: 0.0026574207891557457, Validation loss: 0.006425450888622676\n",
      "iterations:  2318\n",
      "Training loss: 0.002655417256348559, Validation loss: 0.006302009095869633\n",
      "iterations:  2319\n",
      "Training loss: 0.002653637463799987, Validation loss: 0.006412204804686898\n",
      "iterations:  2320\n",
      "Training loss: 0.0026518090216348183, Validation loss: 0.0062827964372217686\n",
      "new best w\n",
      "iterations:  2321\n",
      "Training loss: 0.0026501150056830874, Validation loss: 0.006409563598561091\n",
      "iterations:  2322\n",
      "Training loss: 0.0026483003480334195, Validation loss: 0.006272526223529068\n",
      "new best w\n",
      "iterations:  2323\n",
      "Training loss: 0.0026465385048393287, Validation loss: 0.006402214505622896\n",
      "iterations:  2324\n",
      "Training loss: 0.0026445758754554817, Validation loss: 0.006274259071961069\n",
      "iterations:  2325\n",
      "Training loss: 0.0026427510718184605, Validation loss: 0.006389221073239843\n",
      "iterations:  2326\n",
      "Training loss: 0.002640832247302858, Validation loss: 0.006276270675236034\n",
      "iterations:  2327\n",
      "Training loss: 0.0026393499964539697, Validation loss: 0.0063803982057574985\n",
      "iterations:  2328\n",
      "Training loss: 0.002637550645763837, Validation loss: 0.006251291805656138\n",
      "new best w\n",
      "iterations:  2329\n",
      "Training loss: 0.0026357602640969284, Validation loss: 0.006377559327897378\n",
      "iterations:  2330\n",
      "Training loss: 0.0026339194600203206, Validation loss: 0.006242597445090148\n",
      "new best w\n",
      "iterations:  2331\n",
      "Training loss: 0.002632089857683837, Validation loss: 0.006368995214070132\n",
      "iterations:  2332\n",
      "Training loss: 0.0026302114221677846, Validation loss: 0.006246710789461507\n",
      "iterations:  2333\n",
      "Training loss: 0.0026287535489858903, Validation loss: 0.006359384696962722\n",
      "iterations:  2334\n",
      "Training loss: 0.0026268284375097284, Validation loss: 0.006242011174680878\n",
      "new best w\n",
      "iterations:  2335\n",
      "Training loss: 0.00262501000720213, Validation loss: 0.006347538211953077\n",
      "iterations:  2336\n",
      "Training loss: 0.0026232572698258482, Validation loss: 0.006223898839875592\n",
      "new best w\n",
      "iterations:  2337\n",
      "Training loss: 0.002621482990801616, Validation loss: 0.006344229050315481\n",
      "iterations:  2338\n",
      "Training loss: 0.002619742100462885, Validation loss: 0.00621534592581483\n",
      "new best w\n",
      "iterations:  2339\n",
      "Training loss: 0.002618243351450106, Validation loss: 0.006339005100179787\n",
      "iterations:  2340\n",
      "Training loss: 0.0026163776643345115, Validation loss: 0.006212879169879231\n",
      "new best w\n",
      "iterations:  2341\n",
      "Training loss: 0.002614591690473638, Validation loss: 0.006326499695522239\n",
      "iterations:  2342\n",
      "Training loss: 0.0026127267577068032, Validation loss: 0.006214899502675808\n",
      "iterations:  2343\n",
      "Training loss: 0.002610970829148349, Validation loss: 0.006314824445310158\n",
      "iterations:  2344\n",
      "Training loss: 0.0026091939764934903, Validation loss: 0.00620646044450626\n",
      "new best w\n",
      "iterations:  2345\n",
      "Training loss: 0.0026077770487229874, Validation loss: 0.006310920313720397\n",
      "iterations:  2346\n",
      "Training loss: 0.0026059888989122494, Validation loss: 0.006189696520227517\n",
      "new best w\n",
      "iterations:  2347\n",
      "Training loss: 0.0026043013097835376, Validation loss: 0.00630497008910674\n",
      "iterations:  2348\n",
      "Training loss: 0.0026025428545935715, Validation loss: 0.006177556401341372\n",
      "new best w\n",
      "iterations:  2349\n",
      "Training loss: 0.0026008105868658552, Validation loss: 0.006298416292691032\n",
      "iterations:  2350\n",
      "Training loss: 0.0025989905504481864, Validation loss: 0.006179781344501904\n",
      "iterations:  2351\n",
      "Training loss: 0.0025973862141784295, Validation loss: 0.006287346883595446\n",
      "iterations:  2352\n",
      "Training loss: 0.0025956175732550486, Validation loss: 0.0061786484839334\n",
      "iterations:  2353\n",
      "Training loss: 0.0025940131176052034, Validation loss: 0.006277729344359629\n",
      "iterations:  2354\n",
      "Training loss: 0.0025922461993978857, Validation loss: 0.006167424340348691\n",
      "new best w\n",
      "iterations:  2355\n",
      "Training loss: 0.002590510819255967, Validation loss: 0.006270246365957517\n",
      "iterations:  2356\n",
      "Training loss: 0.0025887889186798534, Validation loss: 0.006157605329717906\n",
      "new best w\n",
      "iterations:  2357\n",
      "Training loss: 0.002587114081925182, Validation loss: 0.006263915048105759\n",
      "iterations:  2358\n",
      "Training loss: 0.002585464630234697, Validation loss: 0.006145555184513684\n",
      "new best w\n",
      "iterations:  2359\n",
      "Training loss: 0.0025840358568918263, Validation loss: 0.006260639498508311\n",
      "iterations:  2360\n",
      "Training loss: 0.002582278747718401, Validation loss: 0.0061412115493726755\n",
      "new best w\n",
      "iterations:  2361\n",
      "Training loss: 0.002580597723314064, Validation loss: 0.006249319916444961\n",
      "iterations:  2362\n",
      "Training loss: 0.0025788280789196395, Validation loss: 0.006141861757929165\n",
      "iterations:  2363\n",
      "Training loss: 0.0025771658290647762, Validation loss: 0.006238435320149245\n",
      "iterations:  2364\n",
      "Training loss: 0.002575413445272278, Validation loss: 0.006138657562276469\n",
      "new best w\n",
      "iterations:  2365\n",
      "Training loss: 0.002573889368668703, Validation loss: 0.006230944303448469\n",
      "iterations:  2366\n",
      "Training loss: 0.002572207425101328, Validation loss: 0.006126291811945167\n",
      "new best w\n",
      "iterations:  2367\n",
      "Training loss: 0.002570700657704866, Validation loss: 0.006226407586124029\n",
      "iterations:  2368\n",
      "Training loss: 0.0025690114080855913, Validation loss: 0.0061116486564154115\n",
      "new best w\n",
      "iterations:  2369\n",
      "Training loss: 0.00256732033396397, Validation loss: 0.006220118082651953\n",
      "iterations:  2370\n",
      "Training loss: 0.002565698151761675, Validation loss: 0.006100594227461848\n",
      "new best w\n",
      "iterations:  2371\n",
      "Training loss: 0.0025640535551906267, Validation loss: 0.00621397093029215\n",
      "iterations:  2372\n",
      "Training loss: 0.0025623476034721173, Validation loss: 0.006101556533006839\n",
      "iterations:  2373\n",
      "Training loss: 0.0025608425770625924, Validation loss: 0.00620364995843245\n",
      "iterations:  2374\n",
      "Training loss: 0.0025591564671117784, Validation loss: 0.006099644736501706\n",
      "new best w\n",
      "iterations:  2375\n",
      "Training loss: 0.002557655419846136, Validation loss: 0.006194617168758409\n",
      "iterations:  2376\n",
      "Training loss: 0.002555948720081237, Validation loss: 0.006093494585327281\n",
      "new best w\n",
      "iterations:  2377\n",
      "Training loss: 0.002554289409443082, Validation loss: 0.006185574431652435\n",
      "iterations:  2378\n",
      "Training loss: 0.0025526048605973694, Validation loss: 0.00608989211706574\n",
      "new best w\n",
      "iterations:  2379\n",
      "Training loss: 0.002551020390453044, Validation loss: 0.006177621289994753\n",
      "iterations:  2380\n",
      "Training loss: 0.0025493449465791686, Validation loss: 0.006084961837746282\n",
      "new best w\n",
      "iterations:  2381\n",
      "Training loss: 0.0025476896523570236, Validation loss: 0.006168548161348192\n",
      "iterations:  2382\n",
      "Training loss: 0.0025460697004342026, Validation loss: 0.006074953477639351\n",
      "new best w\n",
      "iterations:  2383\n",
      "Training loss: 0.002544613989929459, Validation loss: 0.006164530154295926\n",
      "iterations:  2384\n",
      "Training loss: 0.0025430258455893247, Validation loss: 0.006061869847329022\n",
      "new best w\n",
      "iterations:  2385\n",
      "Training loss: 0.0025415234157762593, Validation loss: 0.006159607812676316\n",
      "iterations:  2386\n",
      "Training loss: 0.0025399184442996296, Validation loss: 0.006052625595969855\n",
      "new best w\n",
      "iterations:  2387\n",
      "Training loss: 0.0025383145137066877, Validation loss: 0.006151390851308227\n",
      "iterations:  2388\n",
      "Training loss: 0.002536679734717574, Validation loss: 0.006047185912209058\n",
      "new best w\n",
      "iterations:  2389\n",
      "Training loss: 0.0025351145590393215, Validation loss: 0.00614320419601377\n",
      "iterations:  2390\n",
      "Training loss: 0.002533480533891539, Validation loss: 0.006042699183466749\n",
      "new best w\n",
      "iterations:  2391\n",
      "Training loss: 0.0025318316808854097, Validation loss: 0.006133393282764498\n",
      "iterations:  2392\n",
      "Training loss: 0.0025302010110706207, Validation loss: 0.0060430199165195705\n",
      "iterations:  2393\n",
      "Training loss: 0.002528744781486447, Validation loss: 0.006125031351654602\n",
      "iterations:  2394\n",
      "Training loss: 0.0025271388404693833, Validation loss: 0.006036032337762307\n",
      "new best w\n",
      "iterations:  2395\n",
      "Training loss: 0.0025256620445066203, Validation loss: 0.006118343054542963\n",
      "iterations:  2396\n",
      "Training loss: 0.002524039540075333, Validation loss: 0.00602832891123036\n",
      "new best w\n",
      "iterations:  2397\n",
      "Training loss: 0.0025224761177552136, Validation loss: 0.006110757657875052\n",
      "iterations:  2398\n",
      "Training loss: 0.0025208613174114037, Validation loss: 0.006023037627071529\n",
      "new best w\n",
      "iterations:  2399\n",
      "Training loss: 0.0025192542210656778, Validation loss: 0.006102296797411368\n",
      "iterations:  2400\n",
      "Training loss: 0.0025176573682595903, Validation loss: 0.0060191004129364085\n",
      "new best w\n",
      "iterations:  2401\n",
      "Training loss: 0.002516141556085172, Validation loss: 0.006094860720090961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2402\n",
      "Training loss: 0.0025145517025113627, Validation loss: 0.0060126005027685975\n",
      "new best w\n",
      "iterations:  2403\n",
      "Training loss: 0.002512967511356856, Validation loss: 0.006086611497345418\n",
      "iterations:  2404\n",
      "Training loss: 0.0025113898203394836, Validation loss: 0.006008461937514765\n",
      "new best w\n",
      "iterations:  2405\n",
      "Training loss: 0.0025098182941512277, Validation loss: 0.006078099738352397\n",
      "iterations:  2406\n",
      "Training loss: 0.0025082599714436843, Validation loss: 0.006004085093634363\n",
      "new best w\n",
      "iterations:  2407\n",
      "Training loss: 0.002506839180578612, Validation loss: 0.006071976644165997\n",
      "iterations:  2408\n",
      "Training loss: 0.0025052942089360043, Validation loss: 0.005995170462770519\n",
      "new best w\n",
      "iterations:  2409\n",
      "Training loss: 0.002503864521867879, Validation loss: 0.006066168796739015\n",
      "iterations:  2410\n",
      "Training loss: 0.0025023076683306435, Validation loss: 0.005986361961462306\n",
      "new best w\n",
      "iterations:  2411\n",
      "Training loss: 0.0025007740594291832, Validation loss: 0.006058646263454421\n",
      "iterations:  2412\n",
      "Training loss: 0.0024992346274094125, Validation loss: 0.005978416975825743\n",
      "new best w\n",
      "iterations:  2413\n",
      "Training loss: 0.002497693548353496, Validation loss: 0.006051564911375408\n",
      "iterations:  2414\n",
      "Training loss: 0.002496170177783273, Validation loss: 0.005971405371282433\n",
      "new best w\n",
      "iterations:  2415\n",
      "Training loss: 0.0024946603607583517, Validation loss: 0.006044525320746977\n",
      "iterations:  2416\n",
      "Training loss: 0.002493130350425733, Validation loss: 0.005968331287699878\n",
      "new best w\n",
      "iterations:  2417\n",
      "Training loss: 0.002491605830506063, Validation loss: 0.006035129325076212\n",
      "iterations:  2418\n",
      "Training loss: 0.0024900875111195805, Validation loss: 0.005964251937506039\n",
      "new best w\n",
      "iterations:  2419\n",
      "Training loss: 0.0024886353277344445, Validation loss: 0.006027769059702938\n",
      "iterations:  2420\n",
      "Training loss: 0.0024871224945697944, Validation loss: 0.005957470619490099\n",
      "new best w\n",
      "iterations:  2421\n",
      "Training loss: 0.0024856144607589194, Validation loss: 0.0060198367286024195\n",
      "iterations:  2422\n",
      "Training loss: 0.0024841111420966003, Validation loss: 0.00595246606535533\n",
      "new best w\n",
      "iterations:  2423\n",
      "Training loss: 0.002482612369357276, Validation loss: 0.006011901492485402\n",
      "iterations:  2424\n",
      "Training loss: 0.002481118074565101, Validation loss: 0.005947355811932706\n",
      "new best w\n",
      "iterations:  2425\n",
      "Training loss: 0.0024796281111302302, Validation loss: 0.006004012930546995\n",
      "iterations:  2426\n",
      "Training loss: 0.002478142423149962, Validation loss: 0.00594212812200334\n",
      "new best w\n",
      "iterations:  2427\n",
      "Training loss: 0.002476660882573669, Validation loss: 0.005996185127841993\n",
      "iterations:  2428\n",
      "Training loss: 0.0024751856529857076, Validation loss: 0.00593678825750487\n",
      "new best w\n",
      "iterations:  2429\n",
      "Training loss: 0.002473820744963584, Validation loss: 0.0059906194102185145\n",
      "iterations:  2430\n",
      "Training loss: 0.002472354166713322, Validation loss: 0.005927172161775867\n",
      "new best w\n",
      "iterations:  2431\n",
      "Training loss: 0.0024709603183925752, Validation loss: 0.005984860683001887\n",
      "iterations:  2432\n",
      "Training loss: 0.0024694933699151555, Validation loss: 0.00591867017363138\n",
      "new best w\n",
      "iterations:  2433\n",
      "Training loss: 0.0024680527790135517, Validation loss: 0.00597808888993598\n",
      "iterations:  2434\n",
      "Training loss: 0.0024665910681964825, Validation loss: 0.005912216577833448\n",
      "new best w\n",
      "iterations:  2435\n",
      "Training loss: 0.002465147479579947, Validation loss: 0.005970934473669703\n",
      "iterations:  2436\n",
      "Training loss: 0.002463692451683179, Validation loss: 0.005906225328852372\n",
      "new best w\n",
      "iterations:  2437\n",
      "Training loss: 0.0024622413911919465, Validation loss: 0.0059634224835770945\n",
      "iterations:  2438\n",
      "Training loss: 0.0024607942631280316, Validation loss: 0.005900774938170457\n",
      "new best w\n",
      "iterations:  2439\n",
      "Training loss: 0.002459350960657172, Validation loss: 0.005955868815053211\n",
      "iterations:  2440\n",
      "Training loss: 0.002457911457629325, Validation loss: 0.0058952781201263025\n",
      "new best w\n",
      "iterations:  2441\n",
      "Training loss: 0.0024564756595423665, Validation loss: 0.005948338465550927\n",
      "iterations:  2442\n",
      "Training loss: 0.002455043590974772, Validation loss: 0.005889709707180215\n",
      "new best w\n",
      "iterations:  2443\n",
      "Training loss: 0.0024536348336081986, Validation loss: 0.0059412766602312955\n",
      "iterations:  2444\n",
      "Training loss: 0.0024522091999196792, Validation loss: 0.0058832605675873005\n",
      "new best w\n",
      "iterations:  2445\n",
      "Training loss: 0.0024507871116236798, Validation loss: 0.005933917093236719\n",
      "iterations:  2446\n",
      "Training loss: 0.002449368553634034, Validation loss: 0.00587754374591405\n",
      "new best w\n",
      "iterations:  2447\n",
      "Training loss: 0.0024479534527751747, Validation loss: 0.005926547865524964\n",
      "iterations:  2448\n",
      "Training loss: 0.0024465417979996403, Validation loss: 0.005871791424501228\n",
      "new best w\n",
      "iterations:  2449\n",
      "Training loss: 0.0024451335231238476, Validation loss: 0.005919208680056345\n",
      "iterations:  2450\n",
      "Training loss: 0.0024437286197391246, Validation loss: 0.005865988139555049\n",
      "new best w\n",
      "iterations:  2451\n",
      "Training loss: 0.002442327027473898, Validation loss: 0.005911911228447371\n",
      "iterations:  2452\n",
      "Training loss: 0.0024409287400268802, Validation loss: 0.005860132950457154\n",
      "new best w\n",
      "iterations:  2453\n",
      "Training loss: 0.002439533702049858, Validation loss: 0.005904658415269595\n",
      "iterations:  2454\n",
      "Training loss: 0.0024381419089769367, Validation loss: 0.005854229007703788\n",
      "new best w\n",
      "iterations:  2455\n",
      "Training loss: 0.002436753309838173, Validation loss: 0.005897450366506567\n",
      "iterations:  2456\n",
      "Training loss: 0.002435367901506156, Validation loss: 0.005848280365422639\n",
      "new best w\n",
      "iterations:  2457\n",
      "Training loss: 0.0024339856368437306, Validation loss: 0.005890286324046133\n",
      "iterations:  2458\n",
      "Training loss: 0.0024326065139199655, Validation loss: 0.005842291036106278\n",
      "new best w\n",
      "iterations:  2459\n",
      "Training loss: 0.0024312304889651266, Validation loss: 0.005883165253866759\n",
      "iterations:  2460\n",
      "Training loss: 0.0024298575610463852, Validation loss: 0.0058362647356049345\n",
      "new best w\n",
      "iterations:  2461\n",
      "Training loss: 0.002428487689362495, Validation loss: 0.005876086048627447\n",
      "iterations:  2462\n",
      "Training loss: 0.0024271208738156124, Validation loss: 0.005830204837031101\n",
      "new best w\n",
      "iterations:  2463\n",
      "Training loss: 0.0024257570762295563, Validation loss: 0.005869047600118857\n",
      "iterations:  2464\n",
      "Training loss: 0.002424396297207142, Validation loss: 0.0058241143847820535\n",
      "new best w\n",
      "iterations:  2465\n",
      "Training loss: 0.002423038500900539, Validation loss: 0.005862048828398145\n",
      "iterations:  2466\n",
      "Training loss: 0.0024216836885024738, Validation loss: 0.005817996123188967\n",
      "new best w\n",
      "iterations:  2467\n",
      "Training loss: 0.0024203318262362833, Validation loss: 0.005855088695408851\n",
      "iterations:  2468\n",
      "Training loss: 0.0024189829157933466, Validation loss: 0.0058118525263159615\n",
      "new best w\n",
      "iterations:  2469\n",
      "Training loss: 0.002417636925244441, Validation loss: 0.005848166212290538\n",
      "iterations:  2470\n",
      "Training loss: 0.002416293856704544, Validation loss: 0.005805685825276361\n",
      "new best w\n",
      "iterations:  2471\n",
      "Training loss: 0.0024149536798967015, Validation loss: 0.005841280443640004\n",
      "iterations:  2472\n",
      "Training loss: 0.002413616397297883, Validation loss: 0.005799498032390723\n",
      "new best w\n",
      "iterations:  2473\n",
      "Training loss: 0.002412281980112643, Validation loss: 0.005834430510030883\n",
      "iterations:  2474\n",
      "Training loss: 0.0024109504311296587, Validation loss: 0.005793290962344984\n",
      "new best w\n",
      "iterations:  2475\n",
      "Training loss: 0.00240962172288513, Validation loss: 0.0058276155894144924\n",
      "iterations:  2476\n",
      "Training loss: 0.0024082958584386678, Validation loss: 0.005787066250696618\n",
      "new best w\n",
      "iterations:  2477\n",
      "Training loss: 0.0024069728115262874, Validation loss: 0.005820834917758389\n",
      "iterations:  2478\n",
      "Training loss: 0.002405652585445777, Validation loss: 0.005780825370080721\n",
      "new best w\n",
      "iterations:  2479\n",
      "Training loss: 0.0024043351550167, Validation loss: 0.005814087789156429\n",
      "iterations:  2480\n",
      "Training loss: 0.0024030205237491214, Validation loss: 0.005774569644425532\n",
      "new best w\n",
      "iterations:  2481\n",
      "Training loss: 0.0024017086674433426, Validation loss: 0.005807373555576194\n",
      "iterations:  2482\n",
      "Training loss: 0.0024003995898016664, Validation loss: 0.005768300261438117\n",
      "new best w\n",
      "iterations:  2483\n",
      "Training loss: 0.0023990932675140193, Validation loss: 0.005800691626367459\n",
      "iterations:  2484\n",
      "Training loss: 0.002397789704460005, Validation loss: 0.005762018283580557\n",
      "new best w\n",
      "iterations:  2485\n",
      "Training loss: 0.0023964888781382457, Validation loss: 0.005794041467625057\n",
      "iterations:  2486\n",
      "Training loss: 0.0023951907925950787, Validation loss: 0.005755724657719437\n",
      "new best w\n",
      "iterations:  2487\n",
      "Training loss: 0.002393895426065847, Validation loss: 0.0057874226014782565\n",
      "iterations:  2488\n",
      "Training loss: 0.0023926027827569055, Validation loss: 0.005749420223603483\n",
      "new best w\n",
      "iterations:  2489\n",
      "Training loss: 0.0023913128415761903, Validation loss: 0.0057808346053624645\n",
      "iterations:  2490\n",
      "Training loss: 0.0023900256068868234, Validation loss: 0.005743105721300257\n",
      "new best w\n",
      "iterations:  2491\n",
      "Training loss: 0.002388741058212081, Validation loss: 0.005774277111317418\n",
      "iterations:  2492\n",
      "Training loss: 0.0023874592000716864, Validation loss: 0.005736781797702999\n",
      "new best w\n",
      "iterations:  2493\n",
      "Training loss: 0.0023861800125531034, Validation loss: 0.00576774980534693\n",
      "iterations:  2494\n",
      "Training loss: 0.0023849035003353316, Validation loss: 0.005730449012201975\n",
      "new best w\n",
      "iterations:  2495\n",
      "Training loss: 0.0023836296440243325, Validation loss: 0.005761252426868057\n",
      "iterations:  2496\n",
      "Training loss: 0.002382358448463607, Validation loss: 0.005724107841601711\n",
      "new best w\n",
      "iterations:  2497\n",
      "Training loss: 0.0023810898947368105, Validation loss: 0.00575478476827338\n",
      "iterations:  2498\n",
      "Training loss: 0.002379823987859679, Validation loss: 0.005717758684352774\n",
      "new best w\n",
      "iterations:  2499\n",
      "Training loss: 0.002378560709357052, Validation loss: 0.005748346674625499\n",
      "iterations:  2500\n",
      "Training loss: 0.0023773000644269964, Validation loss: 0.005711401864157598\n",
      "new best w\n",
      "iterations:  2501\n",
      "Training loss: 0.0023760420350030266, Validation loss: 0.0057419380435005125\n",
      "iterations:  2502\n",
      "Training loss: 0.0023747866264779366, Validation loss: 0.005705037633000481\n",
      "new best w\n",
      "iterations:  2503\n",
      "Training loss: 0.002373533821164972, Validation loss: 0.0057355588249953455\n",
      "iterations:  2504\n",
      "Training loss: 0.002372283624666365, Validation loss: 0.005698666173644942\n",
      "new best w\n",
      "iterations:  2505\n",
      "Training loss: 0.0023710360196494825, Validation loss: 0.005729209021912411\n",
      "iterations:  2506\n",
      "Training loss: 0.0023697910119430273, Validation loss: 0.005692287601634248\n",
      "new best w\n",
      "iterations:  2507\n",
      "Training loss: 0.0023685485845459115, Validation loss: 0.005722888690134251\n",
      "iterations:  2508\n",
      "Training loss: 0.002367308743532788, Validation loss: 0.005685901966825806\n",
      "new best w\n",
      "iterations:  2509\n",
      "Training loss: 0.0023660714722144056, Validation loss: 0.005716597939200598\n",
      "iterations:  2510\n",
      "Training loss: 0.0023648367769333396, Validation loss: 0.005679509254483763\n",
      "new best w\n",
      "iterations:  2511\n",
      "Training loss: 0.002363604641295215, Validation loss: 0.005710336933100314\n",
      "iterations:  2512\n",
      "Training loss: 0.002362375071935183, Validation loss: 0.005673109385950439\n",
      "new best w\n",
      "iterations:  2513\n",
      "Training loss: 0.002361148052739382, Validation loss: 0.005704105891290865\n",
      "iterations:  2514\n",
      "Training loss: 0.002359923590663094, Validation loss: 0.00566670221891181\n",
      "new best w\n",
      "iterations:  2515\n",
      "Training loss: 0.0023587016698611257, Validation loss: 0.005697905089958815\n",
      "iterations:  2516\n",
      "Training loss: 0.002357482297639532, Validation loss: 0.005660287547268935\n",
      "new best w\n",
      "iterations:  2517\n",
      "Training loss: 0.002356265458412515, Validation loss: 0.005691734863535385\n",
      "iterations:  2518\n",
      "Training loss: 0.0023550511598709207, Validation loss: 0.005653865100622769\n",
      "new best w\n",
      "iterations:  2519\n",
      "Training loss: 0.0023538393866815856, Validation loss: 0.005685595606482576\n",
      "iterations:  2520\n",
      "Training loss: 0.0023526301469579385, Validation loss: 0.005647434543376848\n",
      "new best w\n",
      "iterations:  2521\n",
      "Training loss: 0.0023514234256152354, Validation loss: 0.005679487775366496\n",
      "iterations:  2522\n",
      "Training loss: 0.0023502192312314053, Validation loss: 0.005640995473458038\n",
      "new best w\n",
      "iterations:  2523\n",
      "Training loss: 0.002349017548968591, Validation loss: 0.005673411891236141\n",
      "iterations:  2524\n",
      "Training loss: 0.0023478183879157284, Validation loss: 0.005634547420653232\n",
      "new best w\n",
      "iterations:  2525\n",
      "Training loss: 0.002346621733483097, Validation loss: 0.005667368542327715\n",
      "iterations:  2526\n",
      "Training loss: 0.0023454275953222873, Validation loss: 0.005628089844555504\n",
      "new best w\n",
      "iterations:  2527\n",
      "Training loss: 0.0023442359590959165, Validation loss: 0.005661358387116564\n",
      "iterations:  2528\n",
      "Training loss: 0.00234304683507562, Validation loss: 0.005621622132111051\n",
      "new best w\n",
      "iterations:  2529\n",
      "Training loss: 0.002341860209183739, Validation loss: 0.005655382157741259\n",
      "iterations:  2530\n",
      "Training loss: 0.002340676092375798, Validation loss: 0.00561514359475404\n",
      "new best w\n",
      "iterations:  2531\n",
      "Training loss: 0.0023394944708446976, Validation loss: 0.005649440663826811\n",
      "iterations:  2532\n",
      "Training loss: 0.002338315356300977, Validation loss: 0.0056086534651140375\n",
      "new best w\n",
      "iterations:  2533\n",
      "Training loss: 0.0023371387352226786, Validation loss: 0.005643534796736784\n",
      "iterations:  2534\n",
      "Training loss: 0.0023359646201547017, Validation loss: 0.005602150893276805\n",
      "new best w\n",
      "iterations:  2535\n",
      "Training loss: 0.002334792997879002, Validation loss: 0.00563766553428762\n",
      "iterations:  2536\n",
      "Training loss: 0.0023336238818634507, Validation loss: 0.005595634942575915\n",
      "new best w\n",
      "iterations:  2537\n",
      "Training loss: 0.002332457259217323, Validation loss: 0.00563183394596161\n",
      "iterations:  2538\n",
      "Training loss: 0.0023312931444305555, Validation loss: 0.005589104584889238\n",
      "new best w\n",
      "iterations:  2539\n",
      "Training loss: 0.002330131524968374, Validation loss: 0.005626041198659252\n",
      "iterations:  2540\n",
      "Training loss: 0.0023289724164538292, Validation loss: 0.005582558695410345\n",
      "new best w\n",
      "iterations:  2541\n",
      "Training loss: 0.002327815806742477, Validation loss: 0.005620288563035852\n",
      "iterations:  2542\n",
      "Training loss: 0.002326661712715114, Validation loss: 0.005575996046861048\n",
      "new best w\n",
      "iterations:  2543\n",
      "Training loss: 0.002325510122658601, Validation loss: 0.005614577420472024\n",
      "iterations:  2544\n",
      "Training loss: 0.002324361054851559, Validation loss: 0.005569415303107157\n",
      "new best w\n",
      "iterations:  2545\n",
      "Training loss: 0.0023232144980605317, Validation loss: 0.005608909270733554\n",
      "iterations:  2546\n",
      "Training loss: 0.0023220704721196193, Validation loss: 0.005562815012134812\n",
      "new best w\n",
      "iterations:  2547\n",
      "Training loss: 0.0023209289663318894, Validation loss: 0.005603285740381187\n",
      "iterations:  2548\n",
      "Training loss: 0.002319790002264692, Validation loss: 0.005556193598340338\n",
      "new best w\n",
      "iterations:  2549\n",
      "Training loss: 0.002318653569824023, Validation loss: 0.005597708591998573\n",
      "iterations:  2550\n",
      "Training loss: 0.002317519692511223, Validation loss: 0.005549549354081108\n",
      "new best w\n",
      "iterations:  2551\n",
      "Training loss: 0.0023163883609125715, Validation loss: 0.005592179734312976\n",
      "iterations:  2552\n",
      "Training loss: 0.00231525960069046, Validation loss: 0.0055428804304295965\n",
      "new best w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2553\n",
      "Training loss: 0.0023141334032012665, Validation loss: 0.005586701233292021\n",
      "iterations:  2554\n",
      "Training loss: 0.0023130097965256527, Validation loss: 0.00553618482706692\n",
      "new best w\n",
      "iterations:  2555\n",
      "Training loss: 0.0023118887728941803, Validation loss: 0.005581275324308939\n",
      "iterations:  2556\n",
      "Training loss: 0.002310770363097677, Validation loss: 0.005529460381245484\n",
      "new best w\n",
      "iterations:  2557\n",
      "Training loss: 0.002309654560361256, Validation loss: 0.005575904425478464\n",
      "iterations:  2558\n",
      "Training loss: 0.0023085413985176125, Validation loss: 0.005522704755743791\n",
      "new best w\n",
      "iterations:  2559\n",
      "Training loss: 0.0023074308719255618, Validation loss: 0.005570591152277019\n",
      "iterations:  2560\n",
      "Training loss: 0.0023063230178372047, Validation loss: 0.005515915425729004\n",
      "new best w\n",
      "iterations:  2561\n",
      "Training loss: 0.0023052178319056043, Validation loss: 0.005565338333573907\n",
      "iterations:  2562\n",
      "Training loss: 0.0023041153552327024, Validation loss: 0.005509089664434119\n",
      "new best w\n",
      "iterations:  2563\n",
      "Training loss: 0.0023030155849510427, Validation loss: 0.005560149029213215\n",
      "iterations:  2564\n",
      "Training loss: 0.0023019185665038317, Validation loss: 0.005502224527549192\n",
      "new best w\n",
      "iterations:  2565\n",
      "Training loss: 0.0023008225132789147, Validation loss: 0.005555026549303388\n",
      "iterations:  2566\n",
      "Training loss: 0.0022997152218795377, Validation loss: 0.005495793137576398\n",
      "new best w\n",
      "iterations:  2567\n",
      "Training loss: 0.0022986127781924656, Validation loss: 0.005549514375950322\n",
      "iterations:  2568\n",
      "Training loss: 0.002297513692852429, Validation loss: 0.0054895004666789\n",
      "new best w\n",
      "iterations:  2569\n",
      "Training loss: 0.0022964171251758916, Validation loss: 0.005544354038611569\n",
      "iterations:  2570\n",
      "Training loss: 0.0022953231119903494, Validation loss: 0.005482898917993987\n",
      "new best w\n",
      "iterations:  2571\n",
      "Training loss: 0.0022942316410356598, Validation loss: 0.005539309781527553\n",
      "iterations:  2572\n",
      "Training loss: 0.002293142759329832, Validation loss: 0.0054761924236860025\n",
      "new best w\n",
      "iterations:  2573\n",
      "Training loss: 0.002292056459074396, Validation loss: 0.005534341522488255\n",
      "iterations:  2574\n",
      "Training loss: 0.002290972794788789, Validation loss: 0.005469412504963311\n",
      "new best w\n",
      "iterations:  2575\n",
      "Training loss: 0.002289891761662155, Validation loss: 0.0055294406282510285\n",
      "iterations:  2576\n",
      "Training loss: 0.0022888134212694354, Validation loss: 0.005462569844308946\n",
      "new best w\n",
      "iterations:  2577\n",
      "Training loss: 0.0022877377713672632, Validation loss: 0.0055246086778457226\n",
      "iterations:  2578\n",
      "Training loss: 0.0022866648808295303, Validation loss: 0.005455667239323181\n",
      "new best w\n",
      "iterations:  2579\n",
      "Training loss: 0.0022855947499462524, Validation loss: 0.005519850434835298\n",
      "iterations:  2580\n",
      "Training loss: 0.002284527455494289, Validation loss: 0.005448704168101927\n",
      "new best w\n",
      "iterations:  2581\n",
      "Training loss: 0.0022834630004649743, Validation loss: 0.0055151716071194755\n",
      "iterations:  2582\n",
      "Training loss: 0.00228240147039564, Validation loss: 0.005441678449932057\n",
      "new best w\n",
      "iterations:  2583\n",
      "Training loss: 0.0022813428712752317, Validation loss: 0.00551057820311941\n",
      "iterations:  2584\n",
      "Training loss: 0.0022802872984738153, Validation loss: 0.0054345868813754986\n",
      "new best w\n",
      "iterations:  2585\n",
      "Training loss: 0.002279234761370434, Validation loss: 0.005506076407411005\n",
      "iterations:  2586\n",
      "Training loss: 0.0022781853664486052, Validation loss: 0.005427425497589152\n",
      "new best w\n",
      "iterations:  2587\n",
      "Training loss: 0.002277139126954312, Validation loss: 0.0055016726160660705\n",
      "iterations:  2588\n",
      "Training loss: 0.0022760961619857573, Validation loss: 0.005420189687024974\n",
      "new best w\n",
      "iterations:  2589\n",
      "Training loss: 0.0022750564892162605, Validation loss: 0.005497373514510564\n",
      "iterations:  2590\n",
      "Training loss: 0.0022740202421051387, Validation loss: 0.005412874241996912\n",
      "new best w\n",
      "iterations:  2591\n",
      "Training loss: 0.002272987443402605, Validation loss: 0.005493186162065332\n",
      "iterations:  2592\n",
      "Training loss: 0.0022719582429553582, Validation loss: 0.0054054733765548525\n",
      "new best w\n",
      "iterations:  2593\n",
      "Training loss: 0.002270932669338952, Validation loss: 0.005489118074093505\n",
      "iterations:  2594\n",
      "Training loss: 0.002269910891137216, Validation loss: 0.005397980724714531\n",
      "new best w\n",
      "iterations:  2595\n",
      "Training loss: 0.002268892943611351, Validation loss: 0.005485177301081664\n",
      "iterations:  2596\n",
      "Training loss: 0.002267879016808794, Validation loss: 0.005390389325114554\n",
      "new best w\n",
      "iterations:  2597\n",
      "Training loss: 0.0022668691536642015, Validation loss: 0.005481372506330242\n",
      "iterations:  2598\n",
      "Training loss: 0.002265863568855593, Validation loss: 0.005382691595286353\n",
      "new best w\n",
      "iterations:  2599\n",
      "Training loss: 0.002264862314124651, Validation loss: 0.005477713044361855\n",
      "iterations:  2600\n",
      "Training loss: 0.002263865632463561, Validation loss: 0.005374879297370909\n",
      "new best w\n",
      "iterations:  2601\n",
      "Training loss: 0.002262873585721381, Validation loss: 0.0054742090420584555\n",
      "iterations:  2602\n",
      "Training loss: 0.0022618864494951983, Validation loss: 0.00536694349638776\n",
      "new best w\n",
      "iterations:  2603\n",
      "Training loss: 0.0022609042972325786, Validation loss: 0.005470871484344844\n",
      "iterations:  2604\n",
      "Training loss: 0.002259927442141136, Validation loss: 0.005358874511717279\n",
      "new best w\n",
      "iterations:  2605\n",
      "Training loss: 0.002258955970976173, Validation loss: 0.005467712306081924\n",
      "iterations:  2606\n",
      "Training loss: 0.0022579902404045904, Validation loss: 0.005350661862162176\n",
      "new best w\n",
      "iterations:  2607\n",
      "Training loss: 0.0022570303524476532, Validation loss: 0.005464744491744605\n",
      "iterations:  2608\n",
      "Training loss: 0.0022560767140765176, Validation loss: 0.005342294204747226\n",
      "new best w\n",
      "iterations:  2609\n",
      "Training loss: 0.002255129444820377, Validation loss: 0.00546198218443778\n",
      "iterations:  2610\n",
      "Training loss: 0.0022541890099785395, Validation loss: 0.005333759267270819\n",
      "new best w\n",
      "iterations:  2611\n",
      "Training loss: 0.0022532555491529406, Validation loss: 0.0054594408058421685\n",
      "iterations:  2612\n",
      "Training loss: 0.0022523295953924016, Validation loss: 0.005325043774516451\n",
      "new best w\n",
      "iterations:  2613\n",
      "Training loss: 0.00225141131130278, Validation loss: 0.005457137188771873\n",
      "iterations:  2614\n",
      "Training loss: 0.0022505013087630817, Validation loss: 0.005316133367961522\n",
      "new best w\n",
      "iterations:  2615\n",
      "Training loss: 0.0022495997767289803, Validation loss: 0.005455089724167848\n",
      "iterations:  2616\n",
      "Training loss: 0.0022487074189637686, Validation loss: 0.005307012518775952\n",
      "new best w\n",
      "iterations:  2617\n",
      "Training loss: 0.002247824454586664, Validation loss: 0.005453318524536358\n",
      "iterations:  2618\n",
      "Training loss: 0.0022469516946503555, Validation loss: 0.005297664433881856\n",
      "new best w\n",
      "iterations:  2619\n",
      "Training loss: 0.002246089392776887, Validation loss: 0.005451845606079521\n",
      "iterations:  2620\n",
      "Training loss: 0.0022452384855191298, Validation loss: 0.0052880709548474005\n",
      "new best w\n",
      "iterations:  2621\n",
      "Training loss: 0.0022443992659286574, Validation loss: 0.005450695092049831\n",
      "iterations:  2622\n",
      "Training loss: 0.0022435728176233203, Validation loss: 0.005278212449413329\n",
      "new best w\n",
      "iterations:  2623\n",
      "Training loss: 0.0022427594786635593, Validation loss: 0.005449893440203076\n",
      "iterations:  2624\n",
      "Training loss: 0.0022419605053133327, Validation loss: 0.005268067695502547\n",
      "new best w\n",
      "iterations:  2625\n",
      "Training loss: 0.002241176286940773, Validation loss: 0.005449469697628481\n",
      "iterations:  2626\n",
      "Training loss: 0.002240408282855192, Validation loss: 0.005257613757645699\n",
      "new best w\n",
      "iterations:  2627\n",
      "Training loss: 0.0022396569408166546, Validation loss: 0.005449455786710283\n",
      "iterations:  2628\n",
      "Training loss: 0.0022389239593692237, Validation loss: 0.005246825855875625\n",
      "new best w\n",
      "iterations:  2629\n",
      "Training loss: 0.0022382098525957684, Validation loss: 0.005449886826532702\n",
      "iterations:  2630\n",
      "Training loss: 0.0022375166014353834, Validation loss: 0.005235677227307689\n",
      "new best w\n",
      "iterations:  2631\n",
      "Training loss: 0.0022368447951221903, Validation loss: 0.0054508014946942265\n",
      "iterations:  2632\n",
      "Training loss: 0.0022361967485580924, Validation loss: 0.005224138980845753\n",
      "new best w\n",
      "iterations:  2633\n",
      "Training loss: 0.0022355731358868454, Validation loss: 0.005452242435262615\n",
      "iterations:  2634\n",
      "Training loss: 0.002234976667699973, Validation loss: 0.005212179945741981\n",
      "new best w\n",
      "iterations:  2635\n",
      "Training loss: 0.002234408113741455, Validation loss: 0.005454256719500514\n",
      "iterations:  2636\n",
      "Training loss: 0.0022338706543174884, Validation loss: 0.0051997665151177356\n",
      "new best w\n",
      "iterations:  2637\n",
      "Training loss: 0.0022333651663509735, Validation loss: 0.005456896367044373\n",
      "iterations:  2638\n",
      "Training loss: 0.0022328953888036372, Validation loss: 0.005186862486037644\n",
      "new best w\n",
      "iterations:  2639\n",
      "Training loss: 0.002232462318131279, Validation loss: 0.0054602189364579565\n",
      "iterations:  2640\n",
      "Training loss: 0.002232070359016902, Validation loss: 0.005173428898348462\n",
      "new best w\n",
      "iterations:  2641\n",
      "Training loss: 0.002231720640365344, Validation loss: 0.00546428819553598\n",
      "iterations:  2642\n",
      "Training loss: 0.002231418361713148, Validation loss: 0.005159423875281252\n",
      "new best w\n",
      "iterations:  2643\n",
      "Training loss: 0.0022311647975368602, Validation loss: 0.005469174883447191\n",
      "iterations:  2644\n",
      "Training loss: 0.002230966098276049, Validation loss: 0.005144802469810009\n",
      "new best w\n",
      "iterations:  2645\n",
      "Training loss: 0.0022308236967510083, Validation loss: 0.005474957578824678\n",
      "iterations:  2646\n",
      "Training loss: 0.0022307448832521197, Validation loss: 0.005129516522012068\n",
      "new best w\n",
      "iterations:  2647\n",
      "Training loss: 0.002230731260526994, Validation loss: 0.0054817236902922565\n",
      "iterations:  2648\n",
      "Training loss: 0.0022307914879516905, Validation loss: 0.005113514534246431\n",
      "new best w\n",
      "iterations:  2649\n",
      "Training loss: 0.002230927347369666, Validation loss: 0.005489570588726454\n",
      "iterations:  2650\n",
      "Training loss: 0.0022311491459101177, Validation loss: 0.005096741572932287\n",
      "new best w\n",
      "iterations:  2651\n",
      "Training loss: 0.002231458849504197, Validation loss: 0.0054986069038733985\n",
      "iterations:  2652\n",
      "Training loss: 0.0022318687524763427, Validation loss: 0.00507913920816388\n",
      "new best w\n",
      "iterations:  2653\n",
      "Training loss: 0.002232381003166111, Validation loss: 0.005508954011862221\n",
      "iterations:  2654\n",
      "Training loss: 0.0022330102974026547, Validation loss: 0.005060645505452947\n",
      "new best w\n",
      "iterations:  2655\n",
      "Training loss: 0.002233758954089623, Validation loss: 0.005520747744791136\n",
      "iterations:  2656\n",
      "Training loss: 0.00223464457728204, Validation loss: 0.005041195087686118\n",
      "new best w\n",
      "iterations:  2657\n",
      "Training loss: 0.002235669629584718, Validation loss: 0.0055341403590377855\n",
      "iterations:  2658\n",
      "Training loss: 0.0022368552442941004, Validation loss: 0.005020719290090474\n",
      "new best w\n",
      "iterations:  2659\n",
      "Training loss: 0.0022382039791367546, Validation loss: 0.005549302805407709\n",
      "iterations:  2660\n",
      "Training loss: 0.0022397412593052122, Validation loss: 0.004999146436822069\n",
      "new best w\n",
      "iterations:  2661\n",
      "Training loss: 0.0022414696581568054, Validation loss: 0.005566427351855867\n",
      "iterations:  2662\n",
      "Training loss: 0.0022434222907949035, Validation loss: 0.004976402274977554\n",
      "new best w\n",
      "iterations:  2663\n",
      "Training loss: 0.0022458994388475516, Validation loss: 0.005586500943001829\n",
      "iterations:  2664\n",
      "Training loss: 0.0022483588258200397, Validation loss: 0.004951389207908818\n",
      "new best w\n",
      "iterations:  2665\n",
      "Training loss: 0.002251759089875358, Validation loss: 0.00561010040224552\n",
      "iterations:  2666\n",
      "Training loss: 0.0022548610618764557, Validation loss: 0.004923905680558235\n",
      "new best w\n",
      "iterations:  2667\n",
      "Training loss: 0.0022593652803672743, Validation loss: 0.005637672542693921\n",
      "iterations:  2668\n",
      "Training loss: 0.002263236651340997, Validation loss: 0.004893870770553706\n",
      "new best w\n",
      "iterations:  2669\n",
      "Training loss: 0.0022689297389843184, Validation loss: 0.005669523734999423\n",
      "iterations:  2670\n",
      "Training loss: 0.0022737436182536795, Validation loss: 0.004861571524816017\n",
      "new best w\n",
      "iterations:  2671\n",
      "Training loss: 0.0022806511484851637, Validation loss: 0.005705666976811596\n",
      "iterations:  2672\n",
      "Training loss: 0.002286602666815163, Validation loss: 0.004827459409684738\n",
      "new best w\n",
      "iterations:  2673\n",
      "Training loss: 0.0022946576193578674, Validation loss: 0.00574597719072549\n",
      "iterations:  2674\n",
      "Training loss: 0.0023019622498831925, Validation loss: 0.004792106863870068\n",
      "new best w\n",
      "iterations:  2675\n",
      "Training loss: 0.0023123566211955995, Validation loss: 0.005792538345098016\n",
      "iterations:  2676\n",
      "Training loss: 0.002321316089340676, Validation loss: 0.004753707226666718\n",
      "new best w\n",
      "iterations:  2677\n",
      "Training loss: 0.0023336251554858987, Validation loss: 0.0058450338628937915\n",
      "iterations:  2678\n",
      "Training loss: 0.0023445657496594103, Validation loss: 0.004713683185008369\n",
      "new best w\n",
      "iterations:  2679\n",
      "Training loss: 0.0023591124846214016, Validation loss: 0.005903945695262994\n",
      "iterations:  2680\n",
      "Training loss: 0.002372419674086333, Validation loss: 0.004672181852351154\n",
      "new best w\n",
      "iterations:  2681\n",
      "Training loss: 0.0023895883692489016, Validation loss: 0.005969972956087713\n",
      "iterations:  2682\n",
      "Training loss: 0.002405717830509548, Validation loss: 0.004629337997312079\n",
      "new best w\n",
      "iterations:  2683\n",
      "Training loss: 0.002425965070109531, Validation loss: 0.006043971206585107\n",
      "iterations:  2684\n",
      "Training loss: 0.002445455636022276, Validation loss: 0.004585273141767299\n",
      "new best w\n",
      "iterations:  2685\n",
      "Training loss: 0.002469323219840896, Validation loss: 0.006126941072885081\n",
      "iterations:  2686\n",
      "Training loss: 0.0024928120078222616, Validation loss: 0.004540196391776701\n",
      "new best w\n",
      "iterations:  2687\n",
      "Training loss: 0.002520942007763564, Validation loss: 0.00622003651714644\n",
      "iterations:  2688\n",
      "Training loss: 0.0025491819932741823, Validation loss: 0.0044943968633452324\n",
      "new best w\n",
      "iterations:  2689\n",
      "Training loss: 0.002582334136893355, Validation loss: 0.006324581931916052\n",
      "iterations:  2690\n",
      "Training loss: 0.002616214412121212, Validation loss: 0.0044482646684239685\n",
      "new best w\n",
      "iterations:  2691\n",
      "Training loss: 0.0026552859434753407, Validation loss: 0.006442094061554342\n",
      "iterations:  2692\n",
      "Training loss: 0.0026958548364851083, Validation loss: 0.004402314329906636\n",
      "new best w\n",
      "iterations:  2693\n",
      "Training loss: 0.002741902907540531, Validation loss: 0.0065743071505182105\n",
      "iterations:  2694\n",
      "Training loss: 0.0027903940137815582, Validation loss: 0.004357211728688458\n",
      "new best w\n",
      "iterations:  2695\n",
      "Training loss: 0.0028446604869616244, Validation loss: 0.0067232004184590325\n",
      "iterations:  2696\n",
      "Training loss: 0.0029025214533757314, Validation loss: 0.004313805097728152\n",
      "new best w\n",
      "iterations:  2697\n",
      "Training loss: 0.0029664597152209205, Validation loss: 0.006891129844143011\n",
      "iterations:  2698\n",
      "Training loss: 0.0030353832769225773, Validation loss: 0.004273160189890351\n",
      "new best w\n",
      "iterations:  2699\n",
      "Training loss: 0.003110686229537286, Validation loss: 0.007081457404378447\n",
      "iterations:  2700\n",
      "Training loss: 0.0031926424764631683, Validation loss: 0.0042365993143743605\n",
      "new best w\n",
      "iterations:  2701\n",
      "Training loss: 0.003281270228876914, Validation loss: 0.0072975564789573445\n",
      "iterations:  2702\n",
      "Training loss: 0.003378538305858579, Validation loss: 0.004205743323515519\n",
      "new best w\n",
      "iterations:  2703\n",
      "Training loss: 0.0034827431586592038, Validation loss: 0.007541794254728297\n",
      "iterations:  2704\n",
      "Training loss: 0.0035979394925049037, Validation loss: 0.004182554726321749\n",
      "new best w\n",
      "iterations:  2705\n",
      "Training loss: 0.0037202845110772416, Validation loss: 0.007818981182629577\n",
      "iterations:  2706\n",
      "Training loss: 0.003856146419462503, Validation loss: 0.004169378794218407\n",
      "new best w\n",
      "iterations:  2707\n",
      "Training loss: 0.003991815162992426, Validation loss: 0.008120962246181264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2708\n",
      "Training loss: 0.0041491932204045815, Validation loss: 0.004169769727607487\n",
      "iterations:  2709\n",
      "Training loss: 0.004271632672738041, Validation loss: 0.008408854759572076\n",
      "iterations:  2710\n",
      "Training loss: 0.004408681243070566, Validation loss: 0.004181039187154885\n",
      "iterations:  2711\n",
      "Training loss: 0.004537675318506124, Validation loss: 0.008698207920896702\n",
      "iterations:  2712\n",
      "Training loss: 0.0046413221230354445, Validation loss: 0.0041926027452755575\n",
      "iterations:  2713\n",
      "Training loss: 0.0047711925384328014, Validation loss: 0.008960327009241379\n",
      "iterations:  2714\n",
      "Training loss: 0.004833435984275745, Validation loss: 0.00420407376440155\n",
      "iterations:  2715\n",
      "Training loss: 0.004908688769909433, Validation loss: 0.00910621092899459\n",
      "iterations:  2716\n",
      "Training loss: 0.004913356532821351, Validation loss: 0.004217749686883408\n",
      "iterations:  2717\n",
      "Training loss: 0.004940241714797564, Validation loss: 0.00913532632157831\n",
      "iterations:  2718\n",
      "Training loss: 0.00492092224881105, Validation loss: 0.004220235880132386\n",
      "iterations:  2719\n",
      "Training loss: 0.004919594370879186, Validation loss: 0.009108260112032836\n",
      "iterations:  2720\n",
      "Training loss: 0.004826808742970593, Validation loss: 0.0042214962110940744\n",
      "iterations:  2721\n",
      "Training loss: 0.004824972820473421, Validation loss: 0.008989482324895093\n",
      "iterations:  2722\n",
      "Training loss: 0.004771388935567414, Validation loss: 0.004205849239434752\n",
      "iterations:  2723\n",
      "Training loss: 0.004760588864631586, Validation loss: 0.00892706791345887\n",
      "iterations:  2724\n",
      "Training loss: 0.004665380751841931, Validation loss: 0.004199035850924416\n",
      "iterations:  2725\n",
      "Training loss: 0.004647632363448525, Validation loss: 0.0088028828490804\n",
      "iterations:  2726\n",
      "Training loss: 0.004573097956715444, Validation loss: 0.0041817506725965155\n",
      "iterations:  2727\n",
      "Training loss: 0.004548876002100172, Validation loss: 0.008708201205563315\n",
      "iterations:  2728\n",
      "Training loss: 0.004452770951958855, Validation loss: 0.0041710474604759375\n",
      "iterations:  2729\n",
      "Training loss: 0.0044328558408576685, Validation loss: 0.008583152856324485\n",
      "iterations:  2730\n",
      "Training loss: 0.004373968283236409, Validation loss: 0.004149530983865516\n",
      "new best w\n",
      "iterations:  2731\n",
      "Training loss: 0.004334227170041678, Validation loss: 0.008490646423944167\n",
      "iterations:  2732\n",
      "Training loss: 0.004232165712507312, Validation loss: 0.0041454495955442604\n",
      "new best w\n",
      "iterations:  2733\n",
      "Training loss: 0.004196104755985229, Validation loss: 0.008330491001657447\n",
      "iterations:  2734\n",
      "Training loss: 0.004130522276519938, Validation loss: 0.0041260644135971926\n",
      "new best w\n",
      "iterations:  2735\n",
      "Training loss: 0.004084453306580723, Validation loss: 0.008219226296519933\n",
      "iterations:  2736\n",
      "Training loss: 0.003987499980809421, Validation loss: 0.00412436096977138\n",
      "new best w\n",
      "iterations:  2737\n",
      "Training loss: 0.0039466592247398785, Validation loss: 0.00805713062035109\n",
      "iterations:  2738\n",
      "Training loss: 0.003877215647350975, Validation loss: 0.004110134997645434\n",
      "new best w\n",
      "iterations:  2739\n",
      "Training loss: 0.003832369715315698, Validation loss: 0.007935453716305673\n",
      "iterations:  2740\n",
      "Training loss: 0.00374146342990445, Validation loss: 0.0041130561680019296\n",
      "iterations:  2741\n",
      "Training loss: 0.0037029474295589217, Validation loss: 0.007780539059825647\n",
      "iterations:  2742\n",
      "Training loss: 0.003644113070357099, Validation loss: 0.0040988389375691365\n",
      "new best w\n",
      "iterations:  2743\n",
      "Training loss: 0.003601012874540562, Validation loss: 0.007671870614204148\n",
      "iterations:  2744\n",
      "Training loss: 0.0035194819776147076, Validation loss: 0.004107566359924585\n",
      "iterations:  2745\n",
      "Training loss: 0.0034834188777406448, Validation loss: 0.007524001898607619\n",
      "iterations:  2746\n",
      "Training loss: 0.003431184121654546, Validation loss: 0.004097175742261897\n",
      "new best w\n",
      "iterations:  2747\n",
      "Training loss: 0.0033905853302229265, Validation loss: 0.007421764010659482\n",
      "iterations:  2748\n",
      "Training loss: 0.003318287301031159, Validation loss: 0.004110829909579896\n",
      "iterations:  2749\n",
      "Training loss: 0.0032857983304102802, Validation loss: 0.007282812037588529\n",
      "iterations:  2750\n",
      "Training loss: 0.003240185671826068, Validation loss: 0.00410378498325826\n",
      "iterations:  2751\n",
      "Training loss: 0.0032032185906247174, Validation loss: 0.007188746968540015\n",
      "iterations:  2752\n",
      "Training loss: 0.0031512254003355864, Validation loss: 0.004111310620785308\n",
      "iterations:  2753\n",
      "Training loss: 0.0031155032794571657, Validation loss: 0.007073072164468462\n",
      "iterations:  2754\n",
      "Training loss: 0.003067147984180239, Validation loss: 0.004119768810330405\n",
      "iterations:  2755\n",
      "Training loss: 0.003032980820192991, Validation loss: 0.006962616691398945\n",
      "iterations:  2756\n",
      "Training loss: 0.00298815371677076, Validation loss: 0.0041295136483630954\n",
      "iterations:  2757\n",
      "Training loss: 0.0029545670207940774, Validation loss: 0.006856328826094641\n",
      "iterations:  2758\n",
      "Training loss: 0.0029131973166207324, Validation loss: 0.0041410163711134195\n",
      "iterations:  2759\n",
      "Training loss: 0.002880223686727771, Validation loss: 0.006752675577994409\n",
      "iterations:  2760\n",
      "Training loss: 0.002842206143516204, Validation loss: 0.004154383214943401\n",
      "iterations:  2761\n",
      "Training loss: 0.0028098939968608256, Validation loss: 0.0066516545889794805\n",
      "iterations:  2762\n",
      "Training loss: 0.0027752152689123656, Validation loss: 0.004169563699317131\n",
      "iterations:  2763\n",
      "Training loss: 0.002741356102261424, Validation loss: 0.006551046158856005\n",
      "iterations:  2764\n",
      "Training loss: 0.0027100328884143198, Validation loss: 0.004186679087537429\n",
      "iterations:  2765\n",
      "Training loss: 0.0026790092387540005, Validation loss: 0.006455567958048509\n",
      "iterations:  2766\n",
      "Training loss: 0.002655606302644228, Validation loss: 0.004197681706845232\n",
      "iterations:  2767\n",
      "Training loss: 0.0026225841650608676, Validation loss: 0.006368913389030918\n",
      "iterations:  2768\n",
      "Training loss: 0.002590213583063107, Validation loss: 0.00423289982081977\n",
      "iterations:  2769\n",
      "Training loss: 0.0025652323150267397, Validation loss: 0.006266321616921246\n",
      "iterations:  2770\n",
      "Training loss: 0.0025471432570982866, Validation loss: 0.004241521468592666\n",
      "iterations:  2771\n",
      "Training loss: 0.002522381029998122, Validation loss: 0.006199964922463971\n",
      "iterations:  2772\n",
      "Training loss: 0.002506032600400096, Validation loss: 0.004255702709302787\n",
      "iterations:  2773\n",
      "Training loss: 0.002481977442692327, Validation loss: 0.006130755160935263\n",
      "iterations:  2774\n",
      "Training loss: 0.002464834357789344, Validation loss: 0.004277776534806722\n",
      "iterations:  2775\n",
      "Training loss: 0.0024424194172530966, Validation loss: 0.006057061341282658\n",
      "iterations:  2776\n",
      "Training loss: 0.002429676716375607, Validation loss: 0.004293456981362365\n",
      "iterations:  2777\n",
      "Training loss: 0.0024086610921099693, Validation loss: 0.005994550207539334\n",
      "iterations:  2778\n",
      "Training loss: 0.0023952240456499786, Validation loss: 0.004315864003436015\n",
      "iterations:  2779\n",
      "Training loss: 0.002376833307967329, Validation loss: 0.0059307364221020285\n",
      "iterations:  2780\n",
      "Training loss: 0.0023669661905156305, Validation loss: 0.004331061980005315\n",
      "iterations:  2781\n",
      "Training loss: 0.002349842333207426, Validation loss: 0.005877235641767071\n",
      "iterations:  2782\n",
      "Training loss: 0.002339222299012474, Validation loss: 0.004352253567822917\n",
      "iterations:  2783\n",
      "Training loss: 0.0023244268828058216, Validation loss: 0.005822661136581678\n",
      "iterations:  2784\n",
      "Training loss: 0.0023167172778670252, Validation loss: 0.00436604578554419\n",
      "iterations:  2785\n",
      "Training loss: 0.002302962150857436, Validation loss: 0.005777556103560343\n"
     ]
    }
   ],
   "source": [
    "mlp_sinc = NNregressor_onelayer(activation_function = 'relu')\n",
    "mlp_sinc.estimate_weights(trainx, trainy, valx, valy, n_hidden=100, \n",
    "                              iterations=10000, patience=20, rate=0.05, \n",
    "                              verbose=True, weight_initialization_factors=[0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some magic to list object variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weights', 'activation_function', 'training_loss', 'validation_loss', 'gradient_norm', 'iterations'])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_sinc.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the gradient norm as a function of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_magnitude = mlp_sinc.gradient_norm\n",
    "iteration_vector = np.arange(mlp_sinc.iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(iteration_vector, gradient_magnitude, s=1)\n",
    "plt.title('Gradient magnitude over training iterations.')\n",
    "plt.ylabel('Gradient norm')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "training_loss = mlp_sinc.training_loss\n",
    "validation_loss = mlp_sinc.validation_loss\n",
    "plt.scatter(iteration_vector, training_loss, s=3, c='lime', label='Train')\n",
    "plt.scatter(iteration_vector, validation_loss, s=3, c='fuchsia', label='Validation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see learnt function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(-15,15, 0.05)\n",
    "sinc_out = np.sin(x_range)/x_range\n",
    "neuralnetwork_out = mlp_sinc.predict(x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_range, sinc_out, s=3, c='lime', label='Sinc(x)')\n",
    "plt.scatter(x_range, neuralnetwork_out, s=3, c='fuchsia', label='NeuralNetwork(x)')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
